import type { ModelInfo } from "../model.js"

/**
 * Azure AI Foundry model metadata.
 *
 * NOTE:
 * - Azure AI Foundry uses *deployment names* at runtime, but Roo still needs underlying model
 *   capabilities (maxTokens/contextWindow/etc.) for validation and parameter shaping.
 * - This list is derived from https://models.dev/api.json (provider: "azure") and includes
 *   all models with tool calling support available through Azure AI Foundry deployments.
 */
export const azureModels = {
	"claude-haiku-4-5": {
		maxTokens: 64_000,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 1,
		outputPrice: 5,
		cacheWritesPrice: 1.25,
		cacheReadsPrice: 0.1,
		supportsTemperature: true,
		description: "Claude Haiku 4.5",
	},
	"claude-opus-4-1": {
		maxTokens: 32_000,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 15,
		outputPrice: 75,
		cacheWritesPrice: 18.75,
		cacheReadsPrice: 1.5,
		supportsTemperature: true,
		description: "Claude Opus 4.1",
	},
	"claude-opus-4-5": {
		maxTokens: 64_000,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 5,
		outputPrice: 25,
		cacheWritesPrice: 6.25,
		cacheReadsPrice: 0.5,
		supportsTemperature: true,
		description: "Claude Opus 4.5",
	},
	"claude-sonnet-4-5": {
		maxTokens: 64_000,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 3,
		outputPrice: 15,
		cacheWritesPrice: 3.75,
		cacheReadsPrice: 0.3,
		supportsTemperature: true,
		description: "Claude Sonnet 4.5",
	},
	"codestral-2501": {
		maxTokens: 256_000,
		contextWindow: 256_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.3,
		outputPrice: 0.9,
		supportsTemperature: true,
		description: "Codestral 25.01",
	},
	"codex-mini": {
		maxTokens: 100_000,
		contextWindow: 200_000,
		supportsImages: false,
		supportsPromptCache: true,
		inputPrice: 1.5,
		outputPrice: 6,
		cacheReadsPrice: 0.375,
		supportsTemperature: false,
		description: "Codex Mini",
	},
	"cohere-command-a": {
		maxTokens: 8_000,
		contextWindow: 256_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 2.5,
		outputPrice: 10,
		supportsTemperature: true,
		description: "Command A",
	},
	"cohere-command-r-08-2024": {
		maxTokens: 4_000,
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.15,
		outputPrice: 0.6,
		supportsTemperature: true,
		description: "Command R",
	},
	"cohere-command-r-plus-08-2024": {
		maxTokens: 4_000,
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 2.5,
		outputPrice: 10,
		supportsTemperature: true,
		description: "Command R+",
	},
	"deepseek-r1-0528": {
		maxTokens: 163_840,
		contextWindow: 163_840,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 1.35,
		outputPrice: 5.4,
		supportsTemperature: true,
		description: "DeepSeek-R1-0528",
	},
	"deepseek-v3-0324": {
		maxTokens: 131_072,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 1.14,
		outputPrice: 4.56,
		supportsTemperature: true,
		description: "DeepSeek-V3-0324",
	},
	"deepseek-v3.1": {
		maxTokens: 131_072,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.56,
		outputPrice: 1.68,
		supportsTemperature: true,
		description: "DeepSeek-V3.1",
	},
	"deepseek-v3.2": {
		maxTokens: 128_000,
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.58,
		outputPrice: 1.68,
		supportsTemperature: true,
		description: "DeepSeek-V3.2",
	},
	"gpt-4-turbo": {
		maxTokens: 4_096,
		contextWindow: 128_000,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 10,
		outputPrice: 30,
		supportsTemperature: true,
		description: "GPT-4 Turbo",
	},
	"gpt-4-turbo-vision": {
		maxTokens: 4_096,
		contextWindow: 128_000,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 10,
		outputPrice: 30,
		supportsTemperature: true,
		description: "GPT-4 Turbo Vision",
	},
	"gpt-4.1": {
		maxTokens: 32_768,
		contextWindow: 1_047_576,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 2,
		outputPrice: 8,
		cacheReadsPrice: 0.5,
		supportsTemperature: true,
		description: "GPT-4.1",
	},
	"gpt-4.1-mini": {
		maxTokens: 32_768,
		contextWindow: 1_047_576,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 0.4,
		outputPrice: 1.6,
		cacheReadsPrice: 0.1,
		supportsTemperature: true,
		description: "GPT-4.1 mini",
	},
	"gpt-4.1-nano": {
		maxTokens: 32_768,
		contextWindow: 1_047_576,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 0.1,
		outputPrice: 0.4,
		cacheReadsPrice: 0.03,
		supportsTemperature: true,
		description: "GPT-4.1 nano",
	},
	"gpt-4o": {
		maxTokens: 16_384,
		contextWindow: 128_000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 2.5,
		outputPrice: 10,
		cacheReadsPrice: 1.25,
		supportsTemperature: true,
		description: "GPT-4o",
	},
	"gpt-4o-mini": {
		maxTokens: 16_384,
		contextWindow: 128_000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 0.15,
		outputPrice: 0.6,
		cacheReadsPrice: 0.08,
		supportsTemperature: true,
		description: "GPT-4o mini",
	},
	"gpt-5": {
		maxTokens: 128_000,
		contextWindow: 272_000,
		includedTools: ["apply_patch"],
		excludedTools: ["apply_diff", "write_to_file"],
		supportsImages: true,
		supportsPromptCache: true,
		supportsReasoningEffort: ["minimal", "low", "medium", "high"],
		reasoningEffort: "medium",
		inputPrice: 1.25,
		outputPrice: 10,
		cacheReadsPrice: 0.13,
		supportsVerbosity: true,
		supportsTemperature: false,
		description: "GPT-5",
	},
	"gpt-5-codex": {
		maxTokens: 128_000,
		contextWindow: 400_000,
		includedTools: ["apply_patch"],
		excludedTools: ["apply_diff", "write_to_file"],
		supportsImages: true,
		supportsPromptCache: true,
		supportsReasoningEffort: ["low", "medium", "high"],
		reasoningEffort: "medium",
		inputPrice: 1.25,
		outputPrice: 10,
		cacheReadsPrice: 0.13,
		supportsTemperature: false,
		description: "GPT-5-Codex",
	},
	"gpt-5-mini": {
		maxTokens: 128_000,
		contextWindow: 272_000,
		includedTools: ["apply_patch"],
		excludedTools: ["apply_diff", "write_to_file"],
		supportsImages: true,
		supportsPromptCache: true,
		supportsReasoningEffort: ["minimal", "low", "medium", "high"],
		reasoningEffort: "medium",
		inputPrice: 0.25,
		outputPrice: 2,
		cacheReadsPrice: 0.03,
		supportsVerbosity: true,
		supportsTemperature: false,
		description: "GPT-5 Mini",
	},
	"gpt-5-nano": {
		maxTokens: 128_000,
		contextWindow: 272_000,
		includedTools: ["apply_patch"],
		excludedTools: ["apply_diff", "write_to_file"],
		supportsImages: true,
		supportsPromptCache: true,
		supportsReasoningEffort: ["minimal", "low", "medium", "high"],
		reasoningEffort: "medium",
		inputPrice: 0.05,
		outputPrice: 0.4,
		cacheReadsPrice: 0.01,
		supportsVerbosity: true,
		supportsTemperature: false,
		description: "GPT-5 Nano",
	},
	"gpt-5-pro": {
		maxTokens: 272_000,
		contextWindow: 400_000,
		includedTools: ["apply_patch"],
		excludedTools: ["apply_diff", "write_to_file"],
		supportsImages: true,
		supportsPromptCache: false,
		supportsReasoningEffort: ["minimal", "low", "medium", "high"],
		reasoningEffort: "medium",
		inputPrice: 15,
		outputPrice: 120,
		supportsVerbosity: true,
		supportsTemperature: false,
		description: "GPT-5 Pro",
	},
	"gpt-5.1": {
		maxTokens: 128_000,
		contextWindow: 272_000,
		includedTools: ["apply_patch"],
		excludedTools: ["apply_diff", "write_to_file"],
		supportsImages: true,
		supportsPromptCache: true,
		promptCacheRetention: "24h",
		supportsReasoningEffort: ["none", "low", "medium", "high"],
		reasoningEffort: "medium",
		inputPrice: 1.25,
		outputPrice: 10,
		cacheReadsPrice: 0.125,
		supportsVerbosity: true,
		supportsTemperature: false,
		description: "GPT-5.1",
	},
	"gpt-5.1-chat": {
		maxTokens: 16_384,
		contextWindow: 128_000,
		includedTools: ["apply_patch"],
		excludedTools: ["apply_diff", "write_to_file"],
		supportsImages: true,
		supportsPromptCache: true,
		promptCacheRetention: "24h",
		inputPrice: 1.25,
		outputPrice: 10,
		cacheReadsPrice: 0.125,
		supportsTemperature: false,
		description: "GPT-5.1 Chat",
	},
	"gpt-5.1-codex": {
		maxTokens: 128_000,
		contextWindow: 400_000,
		includedTools: ["apply_patch"],
		excludedTools: ["apply_diff", "write_to_file"],
		supportsImages: true,
		supportsPromptCache: true,
		promptCacheRetention: "24h",
		supportsReasoningEffort: ["low", "medium", "high"],
		reasoningEffort: "medium",
		inputPrice: 1.25,
		outputPrice: 10,
		cacheReadsPrice: 0.125,
		supportsTemperature: false,
		description: "GPT-5.1 Codex",
	},
	"gpt-5.1-codex-max": {
		maxTokens: 128_000,
		contextWindow: 400_000,
		includedTools: ["apply_patch"],
		excludedTools: ["apply_diff", "write_to_file"],
		supportsImages: true,
		supportsPromptCache: true,
		promptCacheRetention: "24h",
		supportsReasoningEffort: ["low", "medium", "high", "xhigh"],
		reasoningEffort: "medium",
		inputPrice: 1.25,
		outputPrice: 10,
		cacheReadsPrice: 0.125,
		supportsTemperature: false,
		description: "GPT-5.1 Codex Max",
	},
	"gpt-5.1-codex-mini": {
		maxTokens: 128_000,
		contextWindow: 400_000,
		includedTools: ["apply_patch"],
		excludedTools: ["apply_diff", "write_to_file"],
		supportsImages: true,
		supportsPromptCache: true,
		promptCacheRetention: "24h",
		supportsReasoningEffort: ["low", "medium", "high"],
		reasoningEffort: "medium",
		inputPrice: 0.25,
		outputPrice: 2,
		cacheReadsPrice: 0.025,
		supportsTemperature: false,
		description: "GPT-5.1 Codex Mini",
	},
	"gpt-5.2": {
		maxTokens: 128_000,
		contextWindow: 400_000,
		includedTools: ["apply_patch"],
		excludedTools: ["apply_diff", "write_to_file"],
		supportsImages: true,
		supportsPromptCache: true,
		promptCacheRetention: "24h",
		supportsReasoningEffort: ["none", "low", "medium", "high", "xhigh"],
		reasoningEffort: "medium",
		inputPrice: 1.75,
		outputPrice: 14,
		cacheReadsPrice: 0.125,
		supportsVerbosity: true,
		supportsTemperature: false,
		description: "GPT-5.2",
	},
	"gpt-5.2-chat": {
		maxTokens: 16_384,
		contextWindow: 128_000,
		includedTools: ["apply_patch"],
		excludedTools: ["apply_diff", "write_to_file"],
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 1.75,
		outputPrice: 14,
		cacheReadsPrice: 0.175,
		supportsTemperature: false,
		description: "GPT-5.2 Chat",
	},
	"gpt-5.2-codex": {
		maxTokens: 128_000,
		contextWindow: 400_000,
		includedTools: ["apply_patch"],
		excludedTools: ["apply_diff", "write_to_file"],
		supportsImages: true,
		supportsPromptCache: true,
		promptCacheRetention: "24h",
		supportsReasoningEffort: ["low", "medium", "high", "xhigh"],
		reasoningEffort: "medium",
		inputPrice: 1.75,
		outputPrice: 14,
		cacheReadsPrice: 0.175,
		supportsTemperature: false,
		description: "GPT-5.2 Codex",
	},
	"grok-3": {
		maxTokens: 8_192,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: true,
		inputPrice: 3,
		outputPrice: 15,
		cacheReadsPrice: 0.75,
		supportsTemperature: true,
		description: "Grok 3",
	},
	"grok-3-mini": {
		maxTokens: 8_192,
		contextWindow: 131_072,
		supportsImages: false,
		supportsPromptCache: true,
		inputPrice: 0.3,
		outputPrice: 0.5,
		cacheReadsPrice: 0.075,
		supportsTemperature: true,
		description: "Grok 3 Mini",
	},
	"grok-4": {
		maxTokens: 64_000,
		contextWindow: 256_000,
		supportsImages: false,
		supportsPromptCache: true,
		inputPrice: 3,
		outputPrice: 15,
		cacheReadsPrice: 0.75,
		supportsTemperature: true,
		description: "Grok 4",
	},
	"grok-4-fast-non-reasoning": {
		maxTokens: 30_000,
		contextWindow: 2_000_000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 0.2,
		outputPrice: 0.5,
		cacheReadsPrice: 0.05,
		supportsTemperature: true,
		description: "Grok 4 Fast (Non-Reasoning)",
	},
	"grok-4-fast-reasoning": {
		maxTokens: 30_000,
		contextWindow: 2_000_000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 0.2,
		outputPrice: 0.5,
		cacheReadsPrice: 0.05,
		supportsTemperature: true,
		description: "Grok 4 Fast (Reasoning)",
	},
	"grok-code-fast-1": {
		maxTokens: 10_000,
		contextWindow: 256_000,
		supportsImages: false,
		supportsPromptCache: true,
		inputPrice: 0.2,
		outputPrice: 1.5,
		cacheReadsPrice: 0.02,
		supportsTemperature: true,
		description: "Grok Code Fast 1",
	},
	"kimi-k2-thinking": {
		maxTokens: 262_144,
		contextWindow: 262_144,
		supportsImages: false,
		supportsPromptCache: true,
		inputPrice: 0.6,
		outputPrice: 2.5,
		cacheReadsPrice: 0.15,
		supportsTemperature: true,
		description: "Kimi K2 Thinking",
	},
	"llama-3.2-11b-vision-instruct": {
		maxTokens: 8_192,
		contextWindow: 128_000,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 0.37,
		outputPrice: 0.37,
		supportsTemperature: true,
		description: "Llama-3.2-11B-Vision-Instruct",
	},
	"llama-3.2-90b-vision-instruct": {
		maxTokens: 8_192,
		contextWindow: 128_000,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 2.04,
		outputPrice: 2.04,
		supportsTemperature: true,
		description: "Llama-3.2-90B-Vision-Instruct",
	},
	"llama-3.3-70b-instruct": {
		maxTokens: 32_768,
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.71,
		outputPrice: 0.71,
		supportsTemperature: true,
		description: "Llama-3.3-70B-Instruct",
	},
	"llama-4-maverick-17b-128e-instruct-fp8": {
		maxTokens: 8_192,
		contextWindow: 128_000,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 0.25,
		outputPrice: 1,
		supportsTemperature: true,
		description: "Llama 4 Maverick 17B 128E Instruct FP8",
	},
	"llama-4-scout-17b-16e-instruct": {
		maxTokens: 8_192,
		contextWindow: 128_000,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 0.2,
		outputPrice: 0.78,
		supportsTemperature: true,
		description: "Llama 4 Scout 17B 16E Instruct",
	},
	"meta-llama-3.1-405b-instruct": {
		maxTokens: 32_768,
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 5.33,
		outputPrice: 16,
		supportsTemperature: true,
		description: "Meta-Llama-3.1-405B-Instruct",
	},
	"meta-llama-3.1-70b-instruct": {
		maxTokens: 32_768,
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 2.68,
		outputPrice: 3.54,
		supportsTemperature: true,
		description: "Meta-Llama-3.1-70B-Instruct",
	},
	"meta-llama-3.1-8b-instruct": {
		maxTokens: 32_768,
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.3,
		outputPrice: 0.61,
		supportsTemperature: true,
		description: "Meta-Llama-3.1-8B-Instruct",
	},
	"ministral-3b": {
		maxTokens: 8_192,
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.04,
		outputPrice: 0.04,
		supportsTemperature: true,
		description: "Ministral 3B",
	},
	"mistral-large-2411": {
		maxTokens: 32_768,
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 2,
		outputPrice: 6,
		supportsTemperature: true,
		description: "Mistral Large 24.11",
	},
	"mistral-medium-2505": {
		maxTokens: 128_000,
		contextWindow: 128_000,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 0.4,
		outputPrice: 2,
		supportsTemperature: true,
		description: "Mistral Medium 3",
	},
	"mistral-nemo": {
		maxTokens: 128_000,
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.15,
		outputPrice: 0.15,
		supportsTemperature: true,
		description: "Mistral Nemo",
	},
	"mistral-small-2503": {
		maxTokens: 32_768,
		contextWindow: 128_000,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 0.1,
		outputPrice: 0.3,
		supportsTemperature: true,
		description: "Mistral Small 3.1",
	},
	"model-router": {
		maxTokens: 16_384,
		contextWindow: 128_000,
		supportsImages: true,
		supportsPromptCache: false,
		inputPrice: 0.14,
		outputPrice: 0,
		description: "Model Router",
	},
	"o1": {
		maxTokens: 100_000,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		inputPrice: 15,
		outputPrice: 60,
		cacheReadsPrice: 7.5,
		supportsTemperature: false,
		description: "o1",
	},
	"o1-mini": {
		maxTokens: 65_536,
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: true,
		inputPrice: 1.1,
		outputPrice: 4.4,
		cacheReadsPrice: 0.55,
		supportsTemperature: false,
		description: "o1-mini",
	},
	"o1-preview": {
		maxTokens: 32_768,
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: true,
		inputPrice: 16.5,
		outputPrice: 66,
		cacheReadsPrice: 8.25,
		supportsTemperature: false,
		description: "o1-preview",
	},
	"o3": {
		maxTokens: 100_000,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		supportsReasoningEffort: ["low", "medium", "high"],
		reasoningEffort: "medium",
		inputPrice: 2,
		outputPrice: 8,
		cacheReadsPrice: 0.5,
		supportsTemperature: false,
		description: "o3",
	},
	"o3-mini": {
		maxTokens: 100_000,
		contextWindow: 200_000,
		supportsImages: false,
		supportsPromptCache: true,
		supportsReasoningEffort: ["low", "medium", "high"],
		reasoningEffort: "medium",
		inputPrice: 1.1,
		outputPrice: 4.4,
		cacheReadsPrice: 0.55,
		supportsTemperature: false,
		description: "o3-mini",
	},
	"o4-mini": {
		maxTokens: 100_000,
		contextWindow: 200_000,
		supportsImages: true,
		supportsPromptCache: true,
		supportsReasoningEffort: ["low", "medium", "high"],
		reasoningEffort: "medium",
		inputPrice: 1.1,
		outputPrice: 4.4,
		cacheReadsPrice: 0.28,
		supportsTemperature: false,
		description: "o4-mini",
	},
	"phi-4-mini": {
		maxTokens: 4_096,
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.075,
		outputPrice: 0.3,
		supportsTemperature: true,
		description: "Phi-4-mini",
	},
	"phi-4-mini-reasoning": {
		maxTokens: 4_096,
		contextWindow: 128_000,
		supportsImages: false,
		supportsPromptCache: false,
		inputPrice: 0.075,
		outputPrice: 0.3,
		supportsTemperature: true,
		description: "Phi-4-mini-reasoning",
	},
} as const satisfies Record<string, ModelInfo>

export type AzureModelId = keyof typeof azureModels

export const azureDefaultModelId: AzureModelId = "gpt-4o"

export const azureDefaultModelInfo: ModelInfo = azureModels[azureDefaultModelId]
