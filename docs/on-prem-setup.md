# 🏢 Roo Code On-Premises Setup Guide\n\n## 개요\n\nRoo Code 온프레미스 에디션은 기업 환경에서 100% 오프라인으로 작동하도록 설계된 VS Code 확장입니다. 외부 API 호출 차단, 텔레메트리 비활성화, 로컬 LLM 지원을 통해 완전한 격리 환경에서 AI 코딩 어시스턴트를 사용할 수 있습니다.\n\n## 🔑 주요 기능\n\n### ✅ 완전한 오프라인 지원\n- **외부 API 차단**: 모든 클라우드 API 호출 자동 차단\n- **텔레메트리 비활성화**: 사용 데이터 수집 완전 비활성화 \n- **로컬 LLM 통합**: vLLM, Ollama 지원\n- **기업 네트워크 호환**: 방화벽 환경에서 완전 작동\n\n### 🛡️ 보안 기능\n- HTTP/HTTPS 요청 차단 및 모니터링\n- 온프레미스 모드 자동 감지 (`ON_PREM=true`)\n- 로컬 LLM 서버 연결 검증\n- 네트워크 격리 테스트 포함\n\n## 📋 시스템 요구사항\n\n### 최소 요구사항\n- **VS Code**: 1.84.0 이상\n- **Node.js**: 20.19.2\n- **메모리**: 4GB RAM 이상\n- **디스크**: 200MB 여유 공간\n\n### 로컬 LLM 서버 (선택사항)\n- **vLLM 서버**: GPU 지원 권장\n- **Ollama**: CPU/GPU 모두 지원\n- **네트워크**: 내부 네트워크 접근 필요\n\n## 🚀 설치 방법\n\n### 1. VSIX 파일 다운로드\n\n최신 온프레미스 에디션을 다운로드하세요:\n\n`bash\n# GitHub Releases에서 다운로드\nwget https://github.com/RooCodeInc/Roo-Code/releases/latest/download/roo-cline-onprem-latest.vsix\n`\n\n### 2. VS Code에 설치\n\n#### 방법 1: 명령줄 설치\n`bash\ncode --install-extension roo-cline-onprem-3.22.6-onprem.1.vsix\n`\n\n#### 방법 2: VS Code UI 설치\n1. VS Code 열기\n2. `Ctrl+Shift+P` (또는 `Cmd+Shift+P`)\n3. \"Extensions: Install from VSIX...\" 선택\n4. VSIX 파일 선택\n\n### 3. 환경 변수 설정\n\n온프레미스 모드를 활성화하려면:\n\n`bash\n# Linux/macOS\nexport ON_PREM=true\n\n# Windows\nset ON_PREM=true\n\n# PowerShell\n$env:ON_PREM=\"true\"\n`\n\n또는 VS Code 설정에서:\n\n`json\n{\n  \"terminal.integrated.env.linux\": {\n    \"ON_PREM\": \"true\"\n  },\n  \"terminal.integrated.env.osx\": {\n    \"ON_PREM\": \"true\"\n  },\n  \"terminal.integrated.env.windows\": {\n    \"ON_PREM\": \"true\"\n  }\n}\n`\n\n## 🤖 로컬 LLM 설정\n\n### vLLM 서버 설정\n\n#### 1. vLLM 설치\n`bash\npip install vllm\n`\n\n#### 2. 서버 시작\n`bash\npython -m vllm.entrypoints.openai.api_server \\\n  --model microsoft/DialoGPT-medium \\\n  --host 0.0.0.0 \\\n  --port 8000\n`\n\n#### 3. VS Code 설정\n`json\n{\n  \"rooCode.localLLM.enabled\": true,\n  \"rooCode.localLLM.type\": \"vllm\",\n  \"rooCode.localLLM.vllm.endpoint\": \"http://localhost:8000\",\n  \"rooCode.localLLM.vllm.modelName\": \"microsoft/DialoGPT-medium\",\n  \"rooCode.localLLM.fallbackToCloud\": false\n}\n`\n\n### Ollama 서버 설정\n\n#### 1. Ollama 설치\n`bash\n# Linux\ncurl -fsSL https://ollama.ai/install.sh | sh\n\n# macOS\nbrew install ollama\n\n# Windows\nwinget install Ollama.Ollama\n`\n\n#### 2. 모델 다운로드 및 실행\n`bash\n# 모델 다운로드\nollama pull llama2\n\n# 서버 시작\nollama serve\n`\n\n#### 3. VS Code 설정\n`json\n{\n  \"rooCode.localLLM.enabled\": true,\n  \"rooCode.localLLM.type\": \"ollama\",\n  \"rooCode.localLLM.ollama.endpoint\": \"http://localhost:11434\",\n  \"rooCode.localLLM.ollama.modelName\": \"llama2\",\n  \"rooCode.localLLM.fallbackToCloud\": false\n}\n`\n\n## ⚙️ 고급 설정\n\n### 네트워크 보안 설정\n\n`json\n{\n  \"rooCode.onPrem.blockExternalRequests\": true,\n  \"rooCode.onPrem.allowedDomains\": [\n    \"localhost\",\n    \"127.0.0.1\",\n    \"internal-llm-server.company.com\"\n  ],\n  \"rooCode.onPrem.logBlockedRequests\": true\n}\n`\n\n### 텔레메트리 완전 비활성화\n\n`json\n{\n  \"rooCode.telemetry.enabled\": false,\n  \"rooCode.onPrem.blockTelemetry\": true,\n  \"telemetry.telemetryLevel\": \"off\"\n}\n`\n\n### 로컬 LLM 성능 조정\n\n`json\n{\n  \"rooCode.localLLM.maxTokens\": 2048,\n  \"rooCode.localLLM.temperature\": 0.7,\n  \"rooCode.localLLM.timeout\": 30,\n  \"rooCode.localLLM.requestQueue.maxConcurrent\": 3,\n  \"rooCode.localLLM.requestQueue.retryAttempts\": 2\n}\n`\n\n## 🔍 문제 해결\n\n### 일반적인 문제\n\n#### 1. 온프레미스 모드가 활성화되지 않음\n\n**증상**: 클라우드 API 호출이 계속 발생\n\n**해결책**:\n`bash\n# 환경 변수 확인\necho $ON_PREM  # Linux/macOS\necho %ON_PREM%  # Windows CMD\necho $env:ON_PREM  # PowerShell\n\n# VS Code 재시작\ncode --reload-extensions\n`\n\n#### 2. 로컬 LLM 연결 실패\n\n**증상**: \"Local LLM server connection failed\"\n\n**해결책**:\n`bash\n# 서버 상태 확인\ncurl http://localhost:8000/health  # vLLM\ncurl http://localhost:11434/api/tags  # Ollama\n\n# 방화벽 규칙 확인\nsudo ufw allow 8000  # vLLM\nsudo ufw allow 11434  # Ollama\n`\n\n#### 3. 성능 문제\n\n**증상**: 응답 속도 느림\n\n**해결책**:\n`json\n{\n  \"rooCode.localLLM.timeout\": 60,\n  \"rooCode.localLLM.maxTokens\": 1024,\n  \"rooCode.localLLM.requestQueue.maxConcurrent\": 1\n}\n`\n\n### 로그 확인\n\n#### 1. 확장 로그\n- `Ctrl+Shift+P` → \"Developer: Show Logs\" → \"Extension Host\"\n\n#### 2. 네트워크 차단 로그\n`bash\n# 로그 파일 위치\n# macOS: ~/Library/Application Support/Code/logs/\n# Linux: ~/.config/Code/logs/\n# Windows: %APPDATA%\\Code\\logs\\\n\ntail -f \"$HOME/.config/Code/logs/renderer1.log\" | grep \"BLOCKED\"\n`\n\n## 🧪 검증 테스트\n\n### 1. 온프레미스 모드 확인\n\n`javascript\n// VS Code 개발자 콘솔에서 실행\nconsole.log('ON_PREM mode:', process.env.ON_PREM);\n`\n\n### 2. 외부 요청 차단 테스트\n\n`bash\n# 개발자 도구 > 네트워크 탭에서 확인\n# api.openai.com, api.anthropic.com 등으로의 요청이 차단되는지 확인\n`\n\n### 3. 로컬 LLM 연결 테스트\n\n`bash\n# VS Code 명령 팔레트에서\n# \"Roo Code: Test Local LLM Connection\" 실행\n`\n\n## 📚 추가 리소스\n\n- [vLLM 문서](https://vllm.readthedocs.io/)\n- [Ollama 문서](https://ollama.ai/docs)\n- [Roo Code GitHub](https://github.com/RooCodeInc/Roo-Code)\n- [이슈 신고](https://github.com/RooCodeInc/Roo-Code/issues)\n\n## 📞 지원\n\n기업 지원이 필요하시면:\n- 📧 Email: enterprise@roocode.io\n- 💬 Slack: [Roo Code Enterprise](https://roocode.slack.com)\n- 📞 Phone: +1-555-ROO-CODE\n\n---\n\n**⚠️ 중요**: 온프레미스 모드에서는 클라우드 기반 기능(Claude, ChatGPT 등)이 비활성화됩니다. 모든 AI 기능은 로컬 LLM 서버를 통해서만 작동합니다.
