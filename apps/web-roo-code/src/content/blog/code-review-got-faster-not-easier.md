---
title: Code Review Got Faster, Not Easier
slug: code-review-got-faster-not-easier
description: AI is generating over half of Google's production code. The bottleneck didn't disappear, it moved. Here's how teams are adapting review workflows to handle the volume.
primary_schema:
    - Article
    - FAQPage
tags:
    - code-review
    - ai-development
    - engineering-practices
    - productivity
status: published
publish_date: "2025-10-16"
publish_time_pt: "9:00am"
source: "Roo Cast"
---

Over fifty percent.

That is how much code checked into production at Google each week is generated by AI, gets through code review, is accepted, and isn't rolled back.

The bottleneck did not disappear. It moved.

## The shift

Review cycles that used to take days or weeks can now move much faster. That sounds like progress until you realize what it means for the reviewer: more code to review, at a faster cadence.

> "There's certainly a lot more code to review now than there was historically. And so I think that we're writing more code than we have ever before. But also the rate at which code reviews are happening is happening at a much more rapid clip."
>
> Paige Bailey, [Roo Cast S01E14](https://www.youtube.com/watch?v=k3od6FjUfK0&t=2583)

More code. Faster turnaround. Higher review load. The math gets tight unless something changes in how review happens.

## The intermediate check

The pattern emerging at scale: use AI as an intermediate check before human review, not as a replacement for it.

The model flags priority issues. It recommends fixes. It catches the structural problems that waste reviewer attention. Then the engineer gives final approval.

This is not "AI does the review." This is "AI does the triage so the human review can focus on what matters."

The distinction matters because the failure mode is different. If you treat AI review as a replacement, you end up with confident suggestions that miss context. If you treat it as triage, you end up with a smaller set of issues for a human to validate.

> "I remember when I first started at Google, if you were trying to shepherd a CL through into production, it would sometimes be like a days- or weeks-long process. But now it's much much faster."
>
> Paige Bailey, [Roo Cast S01E14](https://www.youtube.com/watch?v=k3od6FjUfK0&t=2617)

In practice, part of the speedup comes from shifting the bottleneck from "waiting for a reviewer to have time" to "reviewer receives pre-triaged issues."

## The constraint for smaller teams

For larger organizations, the intermediate check is already infrastructure. For Series A through C teams, the question is different: how do you get the same leverage without the same headcount?

The answer is the same pattern, scaled down. Use an AI agent that can check out the branch, run your linters, and flag issues before the PR lands in someone's queue. The human still owns the stamp. The human still makes the call on anything ambiguous. But the human is not spending their first pass catching formatting issues and obvious bugs.

Code review load scales with AI-generated volume. Headcount doesn't have to scale linearly.

## How Roo Code closes the loop on review

Roo Code can act as the intermediate check before human review. When you use Roo Code's PR Reviewer, it checks out the branch, runs your test suite, analyzes the changes, and flags issues with priority levels-all before the code lands in a human reviewer's queue.

The key difference from a standalone linter: Roo Code closes the loop. It doesn't just flag problems-it proposes fixes and can iterate on them until the tests pass. The human reviewer sees a cleaner diff with fewer obvious issues to catch.

## Manual review vs. AI-assisted review

|                    | Manual Review Only               | AI-Assisted Review (Intermediate Check)          |
| ------------------ | -------------------------------- | ------------------------------------------------ |
| **First pass**     | Human catches all issues         | AI flags priority issues + recommends fixes      |
| **Time per PR**    | Proportional to code volume      | Reduced by pre-triage                            |
| **Reviewer focus** | Everything, including formatting | Ambiguous decisions and context-sensitive issues |
| **Scale**          | Linear with headcount            | Sublinear-review capacity grows faster than team |
| **Ownership**      | Human owns final approval        | Human still owns final approval                  |

## The tradeoff

The tradeoff is ownership. Someone still needs to own the stamp.

If you automate triage but no one owns final approval, you end up with a review process that feels fast but catches less. The speed is real. The confidence is borrowed.

The pattern that works: clear ownership of final approval, with AI handling the first pass. The reviewer's job shifts from "find all the problems" to "validate the flagged problems and catch what the model missed."

## Why this matters for your team

For a small engineering team shipping regularly, the intermediate check pattern changes the math on review load. Instead of spending reviewer time on low-signal issues, you spend it validating flagged issues and catching what the model missed.

The hours reclaimed go somewhere. They go into shipping, not into waiting for review cycles to complete.

If your team is generating more code with AI assistance, the review queue is already growing. The question is whether you scale review with headcount or with an intermediate check that preserves human ownership.

The first step: audit how much time your reviewers spend on issues that an automated check could have caught. That number is your leverage.

## Frequently asked questions

### Can AI replace human code reviewers?

No-and that's the point. The intermediate check pattern uses AI to triage, not to replace. The model flags priority issues and recommends fixes. The human reviewer still owns final approval and catches what the model missed. The goal is to shift human attention from "find all the problems" to "validate flagged problems and catch context-sensitive issues."

### How do I handle increased code review volume without adding headcount?

Use an AI agent as the first pass before human review. The agent runs linters, executes tests, and flags issues with priority levels. The human reviewer sees a pre-triaged PR with fewer obvious problems to catch. Review load scales with AI-generated code volume; headcount doesn't have to scale linearly.

### What's the difference between a linter and an AI code reviewer?

A linter catches static issues and stops. An AI code reviewer like Roo Code closes the loop: it flags issues, proposes fixes, runs tests to verify the fixes, and iterates until the problems are resolved. The human reviewer sees a cleaner diff, not just a list of warnings.

### How does Roo Code help with code review?

Roo Code's PR Reviewer acts as the intermediate check. It checks out the branch, runs your test suite, analyzes changes, and flags issues with priority levels-before the code reaches a human reviewer. Unlike a standalone linter, Roo Code proposes fixes and can iterate on them automatically. The human reviewer focuses on ambiguous decisions and context-sensitive issues.

### What happens if the AI misses something?

The human reviewer is still the final checkpoint. The intermediate check pattern is designed for this: AI handles triage and obvious issues; humans catch what the model missed and make judgment calls on anything ambiguous. The risk is lower than "no review at all" and the coverage is higher than "human reviews everything cold."
