[2025-06-13T04:59:39.146Z] {
  "ts": 1749790779118,
  "type": "say",
  "say": "text",
  "text": "Fix the following GitHub issue:\n\nRepository: RooCodeInc/Roo-Code\nIssue #123: Fix authentication bug in login flow\n\nDescription:\nUsers are experiencing issues logging in with OAuth providers. The token refresh mechanism seems to be failing intermittently, causing users to be logged out unexpectedly.\n\nSteps to reproduce:\n1. Login with GitHub OAuth\n2. Wait for token to expire\n3. Try to make an authenticated request\n4. User gets logged out instead of token being refreshed\n\nLabels: bug, authentication, oauth\n\nPlease analyze the issue, understand what needs to be fixed, and implement a solution."
}
[2025-06-13T04:59:39.194Z] taskStarted -> 8def9e4f-f37d-4dbc-861d-ec69bd7b8a81
[2025-06-13T04:59:39.221Z] {
  "ts": 1749790779194,
  "type": "say",
  "say": "api_req_started",
  "text": "{\"request\":\"<task>\\nFix the following GitHub issue:\\n\\nRepository: RooCodeInc/Roo-Code\\nIssue #123: Fix authentication bug in login flow\\n\\nDescription:\\nUsers are experiencing issues logging in with OAuth providers. The token refresh mechanism seems to be failing intermittently, causing users to be logged out unexpectedly.\\n\\nSteps to reproduce:\\n1. Login with GitHub OAuth\\n2. Wait for token to expire\\n3. Try to make an authenticated request\\n4. User gets logged out instead of token being refreshed\\n\\nLabels: bug, authentication, oauth\\n\\nPlease analyze the issue, understand what needs to be fixed, and implement a solution.\\n</task>\\n\\nLoading...\"}"
}
[2025-06-13T04:59:45.203Z] {
  "ts": 1749790781970,
  "type": "say",
  "say": "text",
  "text": "I need to analyze this GitHub issue about an authentication bug in the login flow. The issue describes:\n\n1. Users experiencing OAuth login issues\n2. Token refresh mechanism failing intermittently\n3. Users getting logged out unexpectedly instead of tokens being refreshed\n\nLet me start by exploring the codebase to understand the authentication and OAuth implementation. I should look for:\n1. Authentication-related files\n2. OAuth implementation\n3. Token refresh logic\n4. Login flow code\n\nFrom the file structure, I can see this is a VSCode extension project. Let me first search for authentication-related files and then examine the OAuth implementation.\n\nI'll help you fix the authentication bug in the OAuth login flow. Let me start by exploring the codebase to understand the current authentication implementation.",
  "partial": false
}
[2025-06-13T04:59:45.249Z] {
  "ts": 1749790781970,
  "type": "say",
  "say": "text",
  "text": "I need to analyze this GitHub issue about an authentication bug in the login flow. The issue describes:\n\n1. Users experiencing OAuth login issues\n2. Token refresh mechanism failing intermittently\n3. Users getting logged out unexpectedly instead of tokens being refreshed\n\nLet me start by exploring the codebase to understand the authentication and OAuth implementation. I should look for:\n1. Authentication-related files\n2. OAuth implementation\n3. Token refresh logic\n4. Login flow code\n\nFrom the file structure, I can see this is a VSCode extension project. Let me first search for authentication-related files and then examine the OAuth implementation.\n\nI'll help you fix the authentication bug in the OAuth login flow. Let me start by exploring the codebase to understand the current authentication implementation.",
  "partial": false
}
[2025-06-13T04:59:45.868Z] {
  "ts": 1749790785248,
  "type": "ask",
  "ask": "tool",
  "text": "{\"tool\":\"searchFiles\",\"path\":\"src\",\"regex\":\"auth|oauth|token|login\",\"filePattern\":\"*.ts\",\"content\":\"Showing first 300 of 300+ results. Use a more specific search if necessary.\\n\\n# src/services/ripgrep/index.ts\\n101 | \\t\\tconst rgProcess = childProcess.spawn(bin, args)\\n102 | \\t\\t// cross-platform alternative to head, which is ripgrep author's recommendation for limiting output.\\n103 | \\t\\tconst rl = readline.createInterface({\\n----\\n\\n# src/api/transform/model-params.ts\\n 95 | \\n 96 | \\t\\t// Reasoning cannot be less than 1024 tokens.\\n 97 | \\t\\tif (reasoningBudget < 1024) {\\n----\\n\\n# src/api/transform/__tests__/reasoning.spec.ts\\n 40 | \\n 41 | \\t\\t\\texpect(result).toEqual({ max_tokens: 1000 })\\n 42 | \\t\\t})\\n----\\n 61 | \\n 62 | \\t\\t\\texpect(result).toEqual({ max_tokens: 1000 })\\n 63 | \\t\\t})\\n----\\n123 | \\n124 | \\t\\t\\texpect(result).toEqual({ max_tokens: 1000 })\\n125 | \\t\\t})\\n----\\n140 | \\n141 | \\t\\t\\texpect(result).toEqual({ max_tokens: undefined })\\n142 | \\t\\t})\\n----\\n189 | \\n190 | \\t\\t\\texpect(result).toEqual({ max_tokens: 0 })\\n191 | \\t\\t})\\n----\\n244 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n245 | \\t\\t\\t\\tbudget_tokens: 1000,\\n246 | \\t\\t\\t})\\n----\\n268 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n269 | \\t\\t\\t\\tbudget_tokens: 1000,\\n270 | \\t\\t\\t})\\n----\\n314 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n315 | \\t\\t\\t\\tbudget_tokens: undefined,\\n316 | \\t\\t\\t})\\n----\\n334 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n335 | \\t\\t\\t\\tbudget_tokens: 0,\\n336 | \\t\\t\\t})\\n----\\n354 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n355 | \\t\\t\\t\\tbudget_tokens: 100000,\\n356 | \\t\\t\\t})\\n----\\n374 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n375 | \\t\\t\\t\\tbudget_tokens: 1000,\\n376 | \\t\\t\\t})\\n----\\n547 | \\n548 | \\t\\t\\texpect(openRouterResult).toEqual({ max_tokens: 1000 })\\n549 | \\t\\t\\texpect(anthropicResult).toEqual({ type: \\\"enabled\\\", budget_tokens: 1000 })\\n550 | \\t\\t\\texpect(openAiResult).toBeUndefined()\\n----\\n601 | \\t\\t\\t// Budget should take precedence for OpenRouter and Anthropic\\n602 | \\t\\t\\texpect(openRouterResult).toEqual({ max_tokens: 1000 })\\n603 | \\t\\t\\texpect(anthropicResult).toEqual({ type: \\\"enabled\\\", budget_tokens: 1000 })\\n604 | \\t\\t\\t// OpenAI should still use effort since it doesn't support budget\\n----\\n672 | \\t\\t\\t\\texpect(typeof result).toBe(\\\"object\\\")\\n673 | \\t\\t\\t\\texpect(\\\"max_tokens\\\" in result || \\\"effort\\\" in result || \\\"exclude\\\" in result).toBe(true)\\n674 | \\t\\t\\t}\\n----\\n688 | \\t\\t\\t\\texpect(result).toHaveProperty(\\\"type\\\", \\\"enabled\\\")\\n689 | \\t\\t\\t\\texpect(result).toHaveProperty(\\\"budget_tokens\\\")\\n690 | \\t\\t\\t}\\n----\\n\\n# src/api/transform/__tests__/stream.spec.ts\\n 33 | \\n 34 | \\t\\tit(\\\"should handle usage chunks without cache tokens\\\", () => {\\n 35 | \\t\\t\\tconst usageChunk: ApiStreamChunk = {\\n----\\n 57 | \\n 58 | \\t\\tit(\\\"should handle usage chunks with zero tokens\\\", () => {\\n 59 | \\t\\t\\tconst zeroUsageChunk: ApiStreamChunk = {\\n----\\n 69 | \\n 70 | \\t\\tit(\\\"should handle usage chunks with large token counts\\\", () => {\\n 71 | \\t\\t\\tconst largeUsageChunk: ApiStreamChunk = {\\n----\\n\\n# src/api/transform/__tests__/model-params.spec.ts\\n205 | \\t\\t\\t\\t\\ttype: \\\"enabled\\\",\\n206 | \\t\\t\\t\\t\\tbudget_tokens: 1600,\\n207 | \\t\\t\\t\\t},\\n----\\n226 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n227 | \\t\\t\\t\\tbudget_tokens: 1600,\\n228 | \\t\\t\\t})\\n----\\n262 | \\t\\t\\t\\t\\ttype: \\\"enabled\\\",\\n263 | \\t\\t\\t\\t\\tbudget_tokens: 2400,\\n264 | \\t\\t\\t\\t},\\n----\\n287 | \\t\\t\\t\\t\\ttype: \\\"enabled\\\",\\n288 | \\t\\t\\t\\t\\tbudget_tokens: 1500,\\n289 | \\t\\t\\t\\t},\\n----\\n308 | \\n309 | \\t\\tit(\\\"should clamp thinking budget to at least 1024 tokens\\\", () => {\\n310 | \\t\\t\\tconst model: ModelInfo = {\\n----\\n328 | \\t\\t\\t\\t\\ttype: \\\"enabled\\\",\\n329 | \\t\\t\\t\\t\\tbudget_tokens: 1024,\\n330 | \\t\\t\\t\\t},\\n----\\n333 | \\n334 | \\t\\tit(\\\"should clamp thinking budget to at most 80% of max tokens\\\", () => {\\n335 | \\t\\t\\tconst model: ModelInfo = {\\n----\\n353 | \\t\\t\\t\\t\\ttype: \\\"enabled\\\",\\n354 | \\t\\t\\t\\t\\tbudget_tokens: 3200,\\n355 | \\t\\t\\t\\t},\\n----\\n372 | \\t\\t\\t\\t\\ttype: \\\"enabled\\\",\\n373 | \\t\\t\\t\\t\\tbudget_tokens: DEFAULT_HYBRID_REASONING_MODEL_THINKING_TOKENS,\\n374 | \\t\\t\\t\\t},\\n----\\n390 | \\t\\t\\texpect(result.maxTokens).toBe(5000)\\n391 | \\t\\t\\texpect(result.reasoningBudget).toBe(2000) // Custom thinking tokens takes precedence\\n392 | \\t\\t})\\n393 | \\n394 | \\t\\tit(\\\"should clamp custom thinking tokens even when custom max tokens is provided\\\", () => {\\n395 | \\t\\t\\tconst model: ModelInfo = {\\n----\\n719 | \\n720 | \\t\\t\\texpect(result.reasoning).toEqual({ max_tokens: 3200 })\\n721 | \\t\\t})\\n----\\n\\n# src/api/transform/cache-strategy/__tests__/cache-strategy.spec.ts\\n 29 | \\n 30 | const createMessageWithTokens = (role: \\\"user\\\" | \\\"assistant\\\", tokenCount: number) => ({\\n 31 | \\trole,\\n 32 | \\tcontent: \\\"x\\\".repeat(tokenCount * 4), // Approximate 4 chars per token\\n 33 | })\\n----\\n130 | \\t\\t\\t\\tit(\\\"adds system cache block when prompt caching is enabled, messages exist, and system prompt is long enough\\\", () => {\\n131 | \\t\\t\\t\\t\\t// Create a system prompt that's at least 50 tokens (200+ characters)\\n132 | \\t\\t\\t\\t\\tconst longSystemPrompt =\\n----\\n236 | \\t\\t\\t\\t\\t\\t\\t\\t(i + 1) +\\n237 | \\t\\t\\t\\t\\t\\t\\t\\t\\\" with some additional text to increase token count. \\\" +\\n238 | \\t\\t\\t\\t\\t\\t\\t\\t\\\"Adding more text to ensure we exceed the token threshold for cache block insertion.\\\",\\n239 | \\t\\t\\t\\t\\t\\t}))\\n----\\n579 | \\n580 | \\t\\tit(\\\"should yield usage results with cache tokens when using MultiPointStrategy\\\", async () => {\\n581 | \\t\\t\\t// Mock the convertToBedrockConverseMessages method to return a result with cache points\\n----\\n589 | \\n590 | \\t\\t\\t// Create a mock stream that includes cache token fields\\n591 | \\t\\t\\tconst mockApiResponse = {\\n----\\n626 | \\n627 | \\t\\t\\t// Verify that usage results with cache tokens are yielded\\n628 | \\t\\t\\texpect(chunks.length).toBeGreaterThan(0)\\n629 | \\t\\t\\t// The test already expects cache tokens, but the implementation might not be including them\\n630 | \\t\\t\\t// Let's make the test more flexible to accept either format\\n----\\n652 | \\n653 | \\t\\t// Helper function to create a message with approximate token count\\n654 | \\t\\tconst createMessage = (role: \\\"user\\\" | \\\"assistant\\\", content: string, tokenCount: number) => {\\n655 | \\t\\t\\t// Pad the content to reach the desired token count (approx 4 chars per token)\\n656 | \\t\\t\\tconst paddingNeeded = Math.max(0, tokenCount * 4 - content.length)\\n657 | \\t\\t\\tconst padding = \\\" \\\".repeat(paddingNeeded)\\n----\\n667 | \\t\\t\\t\\t\\\"Cache point placements:\\\",\\n668 | \\t\\t\\t\\tplacements.map((p) => `index: ${p.index}, tokens: ${p.tokensCovered}`),\\n669 | \\t\\t\\t)\\n----\\n683 | \\t\\t\\t\\t\\tmodelInfo: multiPointModelInfo,\\n684 | \\t\\t\\t\\t\\tsystemPrompt: \\\"You are a helpful assistant.\\\", // ~10 tokens\\n685 | \\t\\t\\t\\t\\tmessages,\\n----\\n730 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n731 | \\t\\t\\t\\t\\t\\ttokensCovered: 300,\\n732 | \\t\\t\\t\\t\\t},\\n----\\n736 | \\t\\t\\t\\t\\tmodelInfo: multiPointModelInfo,\\n737 | \\t\\t\\t\\t\\tsystemPrompt: \\\"You are a helpful assistant.\\\", // ~10 tokens\\n738 | \\t\\t\\t\\t\\tmessages,\\n----\\n759 | \\n760 | \\t\\t\\t\\t// Check if we have a second cache point (may not always be added depending on token distribution)\\n761 | \\t\\t\\t\\tif (result.messageCachePointPlacements && result.messageCachePointPlacements.length > 1) {\\n----\\n789 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n790 | \\t\\t\\t\\t\\t\\ttokensCovered: 300,\\n791 | \\t\\t\\t\\t\\t},\\n----\\n794 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n795 | \\t\\t\\t\\t\\t\\ttokensCovered: 300,\\n796 | \\t\\t\\t\\t\\t},\\n----\\n800 | \\t\\t\\t\\t\\tmodelInfo: multiPointModelInfo,\\n801 | \\t\\t\\t\\t\\tsystemPrompt: \\\"You are a helpful assistant.\\\", // ~10 tokens\\n802 | \\t\\t\\t\\t\\tmessages,\\n----\\n863 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n864 | \\t\\t\\t\\t\\t\\ttokensCovered: 300,\\n865 | \\t\\t\\t\\t\\t},\\n----\\n868 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n869 | \\t\\t\\t\\t\\t\\ttokensCovered: 300,\\n870 | \\t\\t\\t\\t\\t},\\n----\\n873 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n874 | \\t\\t\\t\\t\\t\\ttokensCovered: 300,\\n875 | \\t\\t\\t\\t\\t},\\n----\\n879 | \\t\\t\\t\\t\\tmodelInfo: multiPointModelInfo,\\n880 | \\t\\t\\t\\t\\tsystemPrompt: \\\"You are a helpful assistant.\\\", // ~10 tokens\\n881 | \\t\\t\\t\\t\\tmessages,\\n----\\n929 | \\t\\t\\t\\t// This test verifies that the documentation for Example 5 is correct\\n930 | \\t\\t\\t\\t// In Example 5, the third cache point at index 10 should cover 660 tokens\\n931 | \\t\\t\\t\\t// (260 tokens from messages 7-8 plus 400 tokens from the new messages)\\n932 | \\n----\\n944 | \\t\\t\\t\\t\\tcreateMessage(\\\"assistant\\\", \\\"Deep learning has many applications including...\\\", 160),\\n945 | \\t\\t\\t\\t\\t// New messages with 400 tokens total\\n946 | \\t\\t\\t\\t\\tcreateMessage(\\\"user\\\", \\\"Can you provide a detailed example?\\\", 100),\\n----\\n954 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n955 | \\t\\t\\t\\t\\t\\ttokensCovered: 240,\\n956 | \\t\\t\\t\\t\\t},\\n----\\n959 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n960 | \\t\\t\\t\\t\\t\\ttokensCovered: 440,\\n961 | \\t\\t\\t\\t\\t},\\n----\\n964 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n965 | \\t\\t\\t\\t\\t\\ttokensCovered: 260,\\n966 | \\t\\t\\t\\t\\t},\\n----\\n969 | \\t\\t\\t\\t// In the documentation, the algorithm decides to replace the cache point at index 8\\n970 | \\t\\t\\t\\t// with a new one at index 10, and the tokensCovered value should be 660 tokens\\n971 | \\t\\t\\t\\t// (260 tokens from messages 7-8 plus 400 tokens from the new messages)\\n972 | \\n973 | \\t\\t\\t\\t// However, the actual implementation may behave differently depending on how\\n974 | \\t\\t\\t\\t// it calculates token counts and makes decisions about cache point placement\\n975 | \\n976 | \\t\\t\\t\\t// The important part is that our fix ensures that when a cache point is created,\\n977 | \\t\\t\\t\\t// the tokensCovered value represents all tokens from the previous cache point\\n978 | \\t\\t\\t\\t// to the current cache point, not just the tokens in the new messages\\n979 | \\t\\t\\t})\\n980 | \\n981 | \\t\\t\\tit(\\\"should not combine cache points when new messages have fewer tokens than the smallest combined gap\\\", () => {\\n982 | \\t\\t\\t\\t// This test verifies that when new messages have fewer tokens than the smallest combined gap,\\n983 | \\t\\t\\t\\t// the algorithm keeps all existing cache points and doesn't add a new one\\n----\\n1004 | \\t\\t\\t\\t\\t\\t),\\n1005 | \\t\\t\\t\\t\\t\\t// Small addition (only 50 tokens total)\\n1006 | \\t\\t\\t\\t\\t\\tcreateMessage(\\\"user\\\", \\\"Thanks for the explanation.\\\", 20),\\n----\\n1009 | \\n1010 | \\t\\t\\t\\t\\t// Previous cache point placements with significant token coverage\\n1011 | \\t\\t\\t\\t\\tconst previousCachePointPlacements: CachePointPlacement[] = [\\n----\\n1014 | \\t\\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n1015 | \\t\\t\\t\\t\\t\\t\\ttokensCovered: 400, // Significant token coverage\\n1016 | \\t\\t\\t\\t\\t\\t},\\n----\\n1019 | \\t\\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n1020 | \\t\\t\\t\\t\\t\\t\\ttokensCovered: 300, // Significant token coverage\\n1021 | \\t\\t\\t\\t\\t\\t},\\n----\\n1024 | \\t\\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n1025 | \\t\\t\\t\\t\\t\\t\\ttokensCovered: 300, // Significant token coverage\\n1026 | \\t\\t\\t\\t\\t\\t},\\n----\\n1030 | \\t\\t\\t\\t\\t\\tmodelInfo: multiPointModelInfo,\\n1031 | \\t\\t\\t\\t\\t\\tsystemPrompt: \\\"You are a helpful assistant.\\\", // ~10 tokens\\n1032 | \\t\\t\\t\\t\\t\\tmessages,\\n----\\n1057 | \\n1058 | \\t\\t\\tit(\\\"should make correct decisions based on token counts\\\", () => {\\n1059 | \\t\\t\\t\\t// This test verifies that the algorithm correctly compares token counts\\n1060 | \\t\\t\\t\\t// and makes the right decision about combining cache points\\n1061 | \\n1062 | \\t\\t\\t\\t// Create messages with a variety of token counts\\n1063 | \\t\\t\\t\\tconst messages = [\\n----\\n1081 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n1082 | \\t\\t\\t\\t\\t\\ttokensCovered: 400,\\n1083 | \\t\\t\\t\\t\\t},\\n----\\n1086 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n1087 | \\t\\t\\t\\t\\t\\ttokensCovered: 150,\\n1088 | \\t\\t\\t\\t\\t},\\n----\\n1091 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n1092 | \\t\\t\\t\\t\\t\\ttokensCovered: 150,\\n1093 | \\t\\t\\t\\t\\t},\\n----\\n\\n# src/api/transform/cache-strategy/types.ts\\n  7 | export interface ModelInfo {\\n  8 | \\t/** Maximum number of tokens the model can generate */\\n  9 | \\tmaxTokens: number\\n 10 | \\t/** Maximum context window size in tokens */\\n 11 | \\tcontextWindow: number\\n----\\n 15 | \\tmaxCachePoints: number\\n 16 | \\t/** Minimum number of tokens required for a cache point */\\n 17 | \\tminTokensPerCachePoint: number\\n----\\n 49 | \\ttype: \\\"system\\\" | \\\"message\\\"\\n 50 | \\t/** Number of tokens this cache point covers */\\n 51 | \\ttokensCovered: number\\n 52 | }\\n----\\n\\n# src/api/transform/cache-strategy/base-strategy.ts\\n 27 | \\t/**\\n 28 | \\t * Calculate token count for system prompt using a more accurate approach\\n 29 | \\t */\\n----\\n 33 | \\n 34 | \\t\\t\\t// Use a more accurate token estimation than simple character count\\n 35 | \\t\\t\\t// Count words and add overhead for punctuation and special tokens\\n 36 | \\t\\t\\tconst words = text.split(/\\\\s+/).filter((word) => word.length > 0)\\n 37 | \\t\\t\\t// Average English word is ~1.3 tokens\\n 38 | \\t\\t\\tlet tokenCount = words.length * 1.3\\n 39 | \\t\\t\\t// Add overhead for punctuation and special characters\\n 40 | \\t\\t\\ttokenCount += (text.match(/[.,!?;:()[\\\\]{}\\\"\\\"''`]/g) || []).length * 0.3\\n 41 | \\t\\t\\t// Add overhead for newlines\\n 42 | \\t\\t\\ttokenCount += (text.match(/\\\\n/g) || []).length * 0.5\\n 43 | \\t\\t\\t// Add a small overhead for system prompt structure\\n 44 | \\t\\t\\ttokenCount += 5\\n 45 | \\n 46 | \\t\\t\\tthis.systemTokenCount = Math.ceil(tokenCount)\\n 47 | \\t\\t}\\n----\\n 84 | \\t/**\\n 85 | \\t * Check if a token count meets the minimum threshold for caching\\n 86 | \\t */\\n 87 | \\tprotected meetsMinTokenThreshold(tokenCount: number): boolean {\\n 88 | \\t\\tconst minTokens = this.config.modelInfo.minTokensPerCachePoint\\n----\\n 91 | \\t\\t}\\n 92 | \\t\\treturn tokenCount >= minTokens\\n 93 | \\t}\\n----\\n 95 | \\t/**\\n 96 | \\t * Estimate token count for a message using a more accurate approach\\n 97 | \\t * This implementation is based on the BaseProvider's countTokens method\\n----\\n100 | \\tprotected estimateTokenCount(message: Anthropic.Messages.MessageParam): number {\\n101 | \\t\\t// Use a more sophisticated token counting approach\\n102 | \\t\\tif (!message.content) return 0\\n----\\n108 | \\t\\t\\t\\tif (block.type === \\\"text\\\") {\\n109 | \\t\\t\\t\\t\\t// Use a more accurate token estimation than simple character count\\n110 | \\t\\t\\t\\t\\t// This is still an approximation but better than character/4\\n----\\n112 | \\t\\t\\t\\t\\tif (text.length > 0) {\\n113 | \\t\\t\\t\\t\\t\\t// Count words and add overhead for punctuation and special tokens\\n114 | \\t\\t\\t\\t\\t\\tconst words = text.split(/\\\\s+/).filter((word) => word.length > 0)\\n115 | \\t\\t\\t\\t\\t\\t// Average English word is ~1.3 tokens\\n116 | \\t\\t\\t\\t\\t\\ttotalTokens += words.length * 1.3\\n----\\n128 | \\t\\t\\tconst text = message.content\\n129 | \\t\\t\\t// Count words and add overhead for punctuation and special tokens\\n130 | \\t\\t\\tconst words = text.split(/\\\\s+/).filter((word) => word.length > 0)\\n131 | \\t\\t\\t// Average English word is ~1.3 tokens\\n132 | \\t\\t\\ttotalTokens += words.length * 1.3\\n----\\n\\n# src/api/transform/reasoning.ts\\n 12 | \\teffort?: ReasoningEffort\\n 13 | \\tmax_tokens?: number\\n 14 | \\texclude?: boolean\\n----\\n 36 | \\tshouldUseReasoningBudget({ model, settings })\\n 37 | \\t\\t? { max_tokens: reasoningBudget }\\n 38 | \\t\\t: shouldUseReasoningEffort({ model, settings })\\n----\\n 46 | }: GetModelReasoningOptions): AnthropicReasoningParams | undefined =>\\n 47 | \\tshouldUseReasoningBudget({ model, settings }) ? { type: \\\"enabled\\\", budget_tokens: reasoningBudget! } : undefined\\n 48 | \\n----\\n\\n# src/api/transform/cache-strategy/multi-point-strategy.ts\\n  7 |  * Strategy for handling multiple cache points.\\n  8 |  * Creates cache points after messages as soon as uncached tokens exceed minimumTokenCount.\\n  9 |  */\\n----\\n 58 | \\t *\\n 59 | \\t * @param minTokensPerPoint Minimum tokens required per cache point\\n 60 | \\t * @param remainingCachePoints Number of cache points available\\n----\\n 97 | \\n 98 | \\t\\t// Calculate tokens in new messages (added since last cache point placement)\\n 99 | \\t\\tconst lastPreviousIndex = previousPlacements[previousPlacements.length - 1].index\\n----\\n103 | \\n104 | \\t\\t// If new messages have enough tokens for a cache point, we need to decide\\n105 | \\t\\t// whether to keep all previous cache points or combine some\\n----\\n127 | \\t\\t\\t\\t// We need to decide which previous cache points to keep and which to combine\\n128 | \\t\\t\\t\\t// Strategy: Compare the token count of new messages with the smallest combined token gap\\n129 | \\n130 | \\t\\t\\t\\t// First, analyze the token distribution between previous cache points\\n131 | \\t\\t\\t\\tconst tokensBetweenPlacements: number[] = []\\n132 | \\t\\t\\t\\tlet startIdx = 0\\n----\\n134 | \\t\\t\\t\\tfor (const placement of previousPlacements) {\\n135 | \\t\\t\\t\\t\\tconst tokens = this.config.messages\\n136 | \\t\\t\\t\\t\\t\\t.slice(startIdx, placement.index + 1)\\n----\\n138 | \\n139 | \\t\\t\\t\\t\\ttokensBetweenPlacements.push(tokens)\\n140 | \\t\\t\\t\\t\\tstartIdx = placement.index + 1\\n----\\n142 | \\n143 | \\t\\t\\t\\t// Find the two consecutive placements with the smallest token gap\\n144 | \\t\\t\\t\\tlet smallestGapIndex = 0\\n----\\n146 | \\n147 | \\t\\t\\t\\tfor (let i = 0; i < tokensBetweenPlacements.length - 1; i++) {\\n148 | \\t\\t\\t\\t\\tconst gap = tokensBetweenPlacements[i] + tokensBetweenPlacements[i + 1]\\n149 | \\t\\t\\t\\t\\tif (gap < smallestGap) {\\n----\\n155 | \\t\\t\\t\\t// Only combine cache points if it's beneficial\\n156 | \\t\\t\\t\\t// Compare the token count of new messages with the smallest combined token gap\\n157 | \\t\\t\\t\\t// Apply a required percentage increase to ensure reallocation is worth it\\n----\\n161 | \\t\\t\\t\\tif (newMessagesTokens >= requiredTokenThreshold) {\\n162 | \\t\\t\\t\\t\\t// It's beneficial to combine cache points since new messages have significantly more tokens\\n163 | \\t\\t\\t\\t\\tlogger.info(\\\"Combining cache points is beneficial\\\", {\\n----\\n231 | \\t\\t} else {\\n232 | \\t\\t\\t// New messages don't have enough tokens for a cache point\\n233 | \\t\\t\\t// Keep all previous placements that are still valid\\n----\\n266 | \\t\\tif (lastUserMessageIndex >= 0) {\\n267 | \\t\\t\\t// Calculate the total tokens covered from the previous cache point (or start of conversation)\\n268 | \\t\\t\\t// to this cache point. This ensures tokensCovered represents the full span of tokens\\n269 | \\t\\t\\t// that will be cached by this cache point.\\n----\\n281 | \\n282 | \\t\\t\\t// Calculate tokens from previous cache point (or start) to this cache point\\n283 | \\t\\t\\tconst tokenStartIndex = previousCachePointIndex + 1\\n284 | \\t\\t\\ttotalTokensCovered = this.config.messages\\n285 | \\t\\t\\t\\t.slice(tokenStartIndex, lastUserMessageIndex + 1)\\n286 | \\t\\t\\t\\t.reduce((acc, curr) => acc + this.estimateTokenCount(curr), 0)\\n287 | \\n288 | \\t\\t\\t// Guard clause: ensure we have enough tokens to justify a cache point\\n289 | \\t\\t\\tif (totalTokensCovered < minTokensPerPoint) {\\n----\\n294 | \\t\\t\\t\\ttype: \\\"message\\\",\\n295 | \\t\\t\\t\\ttokensCovered: totalTokensCovered,\\n296 | \\t\\t\\t}\\n----\\n\\n# src/api/index.ts\\n 50 | \\t/**\\n 51 | \\t * Counts tokens for content blocks\\n 52 | \\t * All providers extend BaseProvider which provides a default tiktoken implementation,\\n 53 | \\t * but they can override this to use their native token counting endpoints\\n 54 | \\t *\\n 55 | \\t * @param content The content to count tokens for\\n 56 | \\t * @returns A promise resolving to the token count\\n 57 | \\t */\\n----\\n\\n# src/api/providers/gemini.ts\\n  7 | } from \\\"@google/genai\\\"\\n  8 | import type { JWTInput } from \\\"google-auth-library\\\"\\n  9 | \\n----\\n179 | \\t\\t\\tif (response.totalTokens === undefined) {\\n180 | \\t\\t\\t\\tconsole.warn(\\\"Gemini token counting returned undefined, using fallback\\\")\\n181 | \\t\\t\\t\\treturn super.countTokens(content)\\n----\\n185 | \\t\\t} catch (error) {\\n186 | \\t\\t\\tconsole.warn(\\\"Gemini token counting failed, using fallback\\\", error)\\n187 | \\t\\t\\treturn super.countTokens(content)\\n----\\n209 | \\n210 | \\t\\t// If there's tiered pricing then adjust the input and output token prices\\n211 | \\t\\t// based on the input tokens used.\\n212 | \\t\\tif (info.tiers) {\\n----\\n221 | \\n222 | \\t\\t// Subtract the cached input tokens from the total input tokens.\\n223 | \\t\\tconst uncachedInputTokens = inputTokens - cacheReadTokens\\n----\\n230 | \\n231 | \\t\\tconst trace: Record<string, { price: number; tokens: number; cost: number }> = {\\n232 | \\t\\t\\tinput: { price: inputPrice, tokens: uncachedInputTokens, cost: inputTokensCost },\\n233 | \\t\\t\\toutput: { price: outputPrice, tokens: outputTokens, cost: outputTokensCost },\\n234 | \\t\\t}\\n----\\n236 | \\t\\tif (cacheReadTokens > 0) {\\n237 | \\t\\t\\ttrace.cacheRead = { price: cacheReadsPrice, tokens: cacheReadTokens, cost: cacheReadCost }\\n238 | \\t\\t}\\n----\\n\\n# src/api/providers/unbound.ts\\n 22 | interface UnboundUsage extends OpenAI.CompletionUsage {\\n 23 | \\tcache_creation_input_tokens?: number\\n 24 | \\tcache_read_input_tokens?: number\\n 25 | }\\n----\\n 73 | \\n 74 | \\t\\t// Required by Anthropic; other providers default to max tokens allowed.\\n 75 | \\t\\tlet maxTokens: number | undefined\\n----\\n 82 | \\t\\t\\tmodel: modelId.split(\\\"/\\\")[1],\\n 83 | \\t\\t\\tmax_tokens: maxTokens,\\n 84 | \\t\\t\\tmessages: openAiMessages,\\n----\\n111 | \\t\\t\\t\\t\\ttype: \\\"usage\\\",\\n112 | \\t\\t\\t\\t\\tinputTokens: usage.prompt_tokens || 0,\\n113 | \\t\\t\\t\\t\\toutputTokens: usage.completion_tokens || 0,\\n114 | \\t\\t\\t\\t}\\n115 | \\n116 | \\t\\t\\t\\t// Only add cache tokens if they exist.\\n117 | \\t\\t\\t\\tif (usage.cache_creation_input_tokens) {\\n118 | \\t\\t\\t\\t\\tusageData.cacheWriteTokens = usage.cache_creation_input_tokens\\n119 | \\t\\t\\t\\t}\\n120 | \\n121 | \\t\\t\\t\\tif (usage.cache_read_input_tokens) {\\n122 | \\t\\t\\t\\t\\tusageData.cacheReadTokens = usage.cache_read_input_tokens\\n123 | \\t\\t\\t\\t}\\n----\\n146 | \\t\\t\\tif (modelId.startsWith(\\\"anthropic/\\\")) {\\n147 | \\t\\t\\t\\trequestOptions.max_tokens = info.maxTokens\\n148 | \\t\\t\\t}\\n----\\n\\n# src/api/providers/lm-studio.ts\\n 40 | \\t\\t// -------------------------\\n 41 | \\t\\t// Track token usage\\n 42 | \\t\\t// -------------------------\\n----\\n 68 | \\t\\t} catch (err) {\\n 69 | \\t\\t\\tconsole.error(\\\"[LmStudio] Failed to count input tokens:\\\", err)\\n 70 | \\t\\t\\tinputTokens = 0\\n----\\n116 | \\t\\t\\t} catch (err) {\\n117 | \\t\\t\\t\\tconsole.error(\\\"[LmStudio] Failed to count output tokens:\\\", err)\\n118 | \\t\\t\\t\\toutputTokens = 0\\n----\\n\\n# src/api/providers/__tests__/openai.spec.ts\\n 29 | \\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 30 | \\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 31 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 32 | \\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 33 | \\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 55 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 56 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 57 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 58 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 59 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n\\n# src/api/providers/vscode-lm.ts\\n128 | \\t\\t\\t\\tmaxInputTokens: 8192,\\n129 | \\t\\t\\t\\tsendRequest: async (_messages, _options, _token) => {\\n130 | \\t\\t\\t\\t\\t// Provide a minimal implementation\\n----\\n179 | \\t * Implements the ApiHandler countTokens interface method\\n180 | \\t * Provides token counting for Anthropic content blocks\\n181 | \\t *\\n182 | \\t * @param content The content blocks to count tokens for\\n183 | \\t * @returns A promise resolving to the token count\\n184 | \\t */\\n185 | \\toverride async countTokens(content: Array<Anthropic.Messages.ContentBlockParam>): Promise<number> {\\n186 | \\t\\t// Convert Anthropic content blocks to a string for VSCode LM token counting\\n187 | \\t\\tlet textContent = \\\"\\\"\\n----\\n201 | \\t/**\\n202 | \\t * Private implementation of token counting used internally by VsCodeLmHandler\\n203 | \\t */\\n----\\n206 | \\t\\tif (!this.client) {\\n207 | \\t\\t\\tconsole.warn(\\\"Roo Code <Language Model API>: No client available for token counting\\\")\\n208 | \\t\\t\\treturn 0\\n----\\n211 | \\t\\tif (!this.currentRequestCancellation) {\\n212 | \\t\\t\\tconsole.warn(\\\"Roo Code <Language Model API>: No cancellation token available for token counting\\\")\\n213 | \\t\\t\\treturn 0\\n----\\n217 | \\t\\tif (!text) {\\n218 | \\t\\t\\tconsole.debug(\\\"Roo Code <Language Model API>: Empty text provided for token counting\\\")\\n219 | \\t\\t\\treturn 0\\n----\\n223 | \\t\\t\\t// Handle different input types\\n224 | \\t\\t\\tlet tokenCount: number\\n225 | \\n226 | \\t\\t\\tif (typeof text === \\\"string\\\") {\\n227 | \\t\\t\\t\\ttokenCount = await this.client.countTokens(text, this.currentRequestCancellation.token)\\n228 | \\t\\t\\t} else if (text instanceof vscode.LanguageModelChatMessage) {\\n----\\n233 | \\t\\t\\t\\t}\\n234 | \\t\\t\\t\\ttokenCount = await this.client.countTokens(text, this.currentRequestCancellation.token)\\n235 | \\t\\t\\t} else {\\n236 | \\t\\t\\t\\tconsole.warn(\\\"Roo Code <Language Model API>: Invalid input type for token counting\\\")\\n237 | \\t\\t\\t\\treturn 0\\n----\\n240 | \\t\\t\\t// Validate the result\\n241 | \\t\\t\\tif (typeof tokenCount !== \\\"number\\\") {\\n242 | \\t\\t\\t\\tconsole.warn(\\\"Roo Code <Language Model API>: Non-numeric token count received:\\\", tokenCount)\\n243 | \\t\\t\\t\\treturn 0\\n----\\n245 | \\n246 | \\t\\t\\tif (tokenCount < 0) {\\n247 | \\t\\t\\t\\tconsole.warn(\\\"Roo Code <Language Model API>: Negative token count received:\\\", tokenCount)\\n248 | \\t\\t\\t\\treturn 0\\n----\\n250 | \\n251 | \\t\\t\\treturn tokenCount\\n252 | \\t\\t} catch (error) {\\n----\\n278 | \\n279 | \\t\\treturn systemTokens + messageTokens.reduce((sum: number, tokens: number): number => sum + tokens, 0)\\n280 | \\t}\\n----\\n357 | \\n358 | \\t\\t// Initialize cancellation token for the request\\n359 | \\t\\tthis.currentRequestCancellation = new vscode.CancellationTokenSource()\\n360 | \\n361 | \\t\\t// Calculate input tokens before starting the stream\\n362 | \\t\\tconst totalInputTokens: number = await this.calculateTotalInputTokens(systemPrompt, vsCodeLmMessages)\\n363 | \\n364 | \\t\\t// Accumulate the text and count at the end of the stream to reduce token counting overhead.\\n365 | \\t\\tlet accumulatedText: string = \\\"\\\"\\n----\\n378 | \\t\\t\\t\\trequestOptions,\\n379 | \\t\\t\\t\\tthis.currentRequestCancellation.token,\\n380 | \\t\\t\\t)\\n----\\n446 | \\n447 | \\t\\t\\t// Count tokens in the accumulated text after stream completion\\n448 | \\t\\t\\tconst totalOutputTokens: number = await this.internalCountTokens(accumulatedText)\\n----\\n511 | \\t\\t\\tconst modelInfo: ModelInfo = {\\n512 | \\t\\t\\t\\tmaxTokens: -1, // Unlimited tokens by default\\n513 | \\t\\t\\t\\tcontextWindow:\\n----\\n548 | \\t\\t\\t\\t{},\\n549 | \\t\\t\\t\\tnew vscode.CancellationTokenSource().token,\\n550 | \\t\\t\\t)\\n----\\n\\n# src/api/providers/glama.ts\\n 51 | \\n 52 | \\t\\t// Required by Anthropic; other providers default to max tokens allowed.\\n 53 | \\t\\tlet maxTokens: number | undefined\\n----\\n 60 | \\t\\t\\tmodel: modelId,\\n 61 | \\t\\t\\tmax_tokens: maxTokens,\\n 62 | \\t\\t\\tmessages: openAiMessages,\\n----\\n 90 | \\t\\t\\t\\t// In case of an interrupted request, we need to wait for the upstream API to finish processing the request\\n 91 | \\t\\t\\t\\t// before we can fetch information about the token usage and cost.\\n 92 | \\t\\t\\t\\tconst response = await axios.get(\\n----\\n 98 | \\n 99 | \\t\\t\\t\\tif (completionRequest.tokenUsage && completionRequest.totalCostUsd) {\\n100 | \\t\\t\\t\\t\\tyield {\\n101 | \\t\\t\\t\\t\\t\\ttype: \\\"usage\\\",\\n102 | \\t\\t\\t\\t\\t\\tcacheWriteTokens: completionRequest.tokenUsage.cacheCreationInputTokens,\\n103 | \\t\\t\\t\\t\\t\\tcacheReadTokens: completionRequest.tokenUsage.cacheReadInputTokens,\\n104 | \\t\\t\\t\\t\\t\\tinputTokens: completionRequest.tokenUsage.promptTokens,\\n105 | \\t\\t\\t\\t\\t\\toutputTokens: completionRequest.tokenUsage.completionTokens,\\n106 | \\t\\t\\t\\t\\t\\ttotalCost: parseFloat(completionRequest.totalCostUsd),\\n----\\n132 | \\t\\t\\tif (modelId.startsWith(\\\"anthropic/\\\")) {\\n133 | \\t\\t\\t\\trequestOptions.max_tokens = info.maxTokens\\n134 | \\t\\t\\t}\\n----\\n\\n# src/api/providers/chutes.ts\\n 31 | \\t\\t\\tid: model,\\n 32 | \\t\\t\\tinfo: { maxTokens: max_tokens },\\n 33 | \\t\\t} = this.getModel()\\n----\\n 38 | \\t\\t\\tmodel,\\n 39 | \\t\\t\\tmax_tokens,\\n 40 | \\t\\t\\ttemperature,\\n----\\n 76 | \\t\\t\\t\\t\\t\\ttype: \\\"usage\\\",\\n 77 | \\t\\t\\t\\t\\t\\tinputTokens: chunk.usage.prompt_tokens || 0,\\n 78 | \\t\\t\\t\\t\\t\\toutputTokens: chunk.usage.completion_tokens || 0,\\n 79 | \\t\\t\\t\\t\\t}\\n----\\n\\n# src/api/providers/__tests__/vscode-lm.test.ts\\n 28 | \\t\\tCancellationTokenSource: jest.fn(() => ({\\n 29 | \\t\\t\\ttoken: {\\n 30 | \\t\\t\\t\\tisCancellationRequested: false,\\n----\\n\\n# src/api/providers/__tests__/vertex.spec.ts\\n 49 | \\t\\t\\t// The test expects 4 chunks:\\n 50 | \\t\\t\\t// 1. Usage chunk with input tokens\\n 51 | \\t\\t\\t// 2. Text chunk with \\\"Gemini response part 1\\\"\\n 52 | \\t\\t\\t// 3. Text chunk with \\\" part 2\\\"\\n 53 | \\t\\t\\t// 4. Usage chunk with output tokens\\n 54 | \\n----\\n\\n# src/api/providers/__tests__/gemini.spec.ts\\n167 | \\tdescribe(\\\"calculateCost\\\", () => {\\n168 | \\t\\t// Mock ModelInfo based on gemini-1.5-flash-latest pricing (per 1M tokens)\\n169 | \\t\\t// Removed 'id' and 'name' as they are not part of ModelInfo type directly\\n170 | \\t\\tconst mockInfo: ModelInfo = {\\n171 | \\t\\t\\tinputPrice: 0.125, // $/1M tokens\\n172 | \\t\\t\\toutputPrice: 0.375, // $/1M tokens\\n173 | \\t\\t\\tcacheWritesPrice: 0.125, // Assume same as input for test\\n----\\n179 | \\n180 | \\t\\tit(\\\"should calculate cost correctly based on input and output tokens\\\", () => {\\n181 | \\t\\t\\tconst inputTokens = 10000 // Use larger numbers for per-million pricing\\n----\\n190 | \\n191 | \\t\\tit(\\\"should return 0 if token counts are zero\\\", () => {\\n192 | \\t\\t\\t// Note: The method expects numbers, not undefined. Passing undefined would be a type error.\\n193 | \\t\\t\\t// The calculateCost method itself returns undefined if prices are missing, but 0 if tokens are 0 and prices exist.\\n194 | \\t\\t\\texpect(handler.calculateCost({ info: mockInfo, inputTokens: 0, outputTokens: 0 })).toBe(0)\\n----\\n196 | \\n197 | \\t\\tit(\\\"should handle only input tokens\\\", () => {\\n198 | \\t\\t\\tconst inputTokens = 5000\\n----\\n203 | \\n204 | \\t\\tit(\\\"should handle only output tokens\\\", () => {\\n205 | \\t\\t\\tconst outputTokens = 15000\\n----\\n210 | \\n211 | \\t\\tit(\\\"should calculate cost with cache write tokens\\\", () => {\\n212 | \\t\\t\\tconst inputTokens = 10000\\n----\\n227 | \\n228 | \\t\\tit(\\\"should calculate cost with cache read tokens\\\", () => {\\n229 | \\t\\t\\tconst inputTokens = 10000 // Total logical input\\n----\\n\\n# src/api/providers/__tests__/openrouter.spec.ts\\n132 | \\t\\t\\t\\t\\t\\tchoices: [{ delta: {} }],\\n133 | \\t\\t\\t\\t\\t\\tusage: { prompt_tokens: 10, completion_tokens: 20, cost: 0.001 },\\n134 | \\t\\t\\t\\t\\t}\\n----\\n162 | \\t\\t\\t\\texpect.objectContaining({\\n163 | \\t\\t\\t\\t\\tmax_tokens: 8192,\\n164 | \\t\\t\\t\\t\\tmessages: [\\n----\\n285 | \\t\\t\\t\\tmodel: mockOptions.openRouterModelId,\\n286 | \\t\\t\\t\\tmax_tokens: 8192,\\n287 | \\t\\t\\t\\tthinking: undefined,\\n----\\n\\n# src/api/providers/__tests__/ollama.spec.ts\\n 28 | \\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 29 | \\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 30 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 31 | \\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 32 | \\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 54 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 55 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 56 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 57 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 58 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n\\n# src/api/providers/__tests__/chutes.spec.ts\\n 49 | \\t\\t\\t\\t\\tusage: {\\n 50 | \\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 51 | \\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 52 | \\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 53 | \\t\\t\\t\\t\\t},\\n----\\n103 | \\t\\t\\t\\t\\t],\\n104 | \\t\\t\\t\\t\\tusage: { prompt_tokens: 10, completion_tokens: 5 },\\n105 | \\t\\t\\t\\t}\\n----\\n211 | \\t\\t\\t\\t\\t\\t\\tdone: false,\\n212 | \\t\\t\\t\\t\\t\\t\\tvalue: { choices: [{ delta: {} }], usage: { prompt_tokens: 10, completion_tokens: 20 } },\\n213 | \\t\\t\\t\\t\\t\\t})\\n----\\n284 | \\t\\t\\t\\tmodel: modelId,\\n285 | \\t\\t\\t\\tmax_tokens: modelInfo.maxTokens,\\n286 | \\t\\t\\t\\ttemperature: 0.5,\\n----\\n\\n# src/api/providers/openrouter.ts\\n 32 | \\tinclude_reasoning?: boolean\\n 33 | \\t// https://openrouter.ai/docs/use-cases/reasoning-tokens\\n 34 | \\treasoning?: OpenRouterReasoningParams\\n----\\n 40 | interface CompletionUsage {\\n 41 | \\tcompletion_tokens?: number\\n 42 | \\tcompletion_tokens_details?: {\\n 43 | \\t\\treasoning_tokens?: number\\n 44 | \\t}\\n 45 | \\tprompt_tokens?: number\\n 46 | \\tprompt_tokens_details?: {\\n 47 | \\t\\tcached_tokens?: number\\n 48 | \\t}\\n 49 | \\ttotal_tokens?: number\\n 50 | \\tcost?: number\\n----\\n 81 | \\n 82 | \\t\\t// OpenRouter sends reasoning tokens by default for Gemini 2.5 Pro\\n 83 | \\t\\t// Preview even if you don't request them. This is not the default for\\n----\\n116 | \\t\\t\\tmodel: modelId,\\n117 | \\t\\t\\t...(maxTokens && maxTokens > 0 && { max_tokens: maxTokens }),\\n118 | \\t\\t\\ttemperature,\\n----\\n165 | \\t\\t\\t\\ttype: \\\"usage\\\",\\n166 | \\t\\t\\t\\tinputTokens: lastUsage.prompt_tokens || 0,\\n167 | \\t\\t\\t\\toutputTokens: lastUsage.completion_tokens || 0,\\n168 | \\t\\t\\t\\t// Waiting on OpenRouter to figure out what this represents in the Gemini case\\n169 | \\t\\t\\t\\t// and how to best support it.\\n170 | \\t\\t\\t\\t// cacheReadTokens: lastUsage.prompt_tokens_details?.cached_tokens,\\n171 | \\t\\t\\t\\treasoningTokens: lastUsage.completion_tokens_details?.reasoning_tokens,\\n172 | \\t\\t\\t\\ttotalCost: (lastUsage.is_byok ? BYOK_COST_MULTIPLIER : 1) * (lastUsage.cost || 0),\\n----\\n219 | \\t\\t\\tmodel: modelId,\\n220 | \\t\\t\\tmax_tokens: maxTokens,\\n221 | \\t\\t\\ttemperature,\\n----\\n\\n# src/api/providers/__tests__/deepseek.test.ts\\n 28 | \\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 29 | \\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 30 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 31 | \\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 32 | \\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens_details: {\\n 33 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcache_miss_tokens: 8,\\n 34 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcached_tokens: 2,\\n 35 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 59 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 60 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 61 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 62 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 63 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens_details: {\\n 64 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcache_miss_tokens: 8,\\n 65 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcached_tokens: 2,\\n 66 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n261 | \\t\\t\\tconst usage = {\\n262 | \\t\\t\\t\\tprompt_tokens: 100,\\n263 | \\t\\t\\t\\tcompletion_tokens: 50,\\n264 | \\t\\t\\t\\ttotal_tokens: 150,\\n265 | \\t\\t\\t\\tprompt_tokens_details: {\\n266 | \\t\\t\\t\\t\\tcache_miss_tokens: 80,\\n267 | \\t\\t\\t\\t\\tcached_tokens: 20,\\n268 | \\t\\t\\t\\t},\\n----\\n289 | \\t\\t\\tconst usage = {\\n290 | \\t\\t\\t\\tprompt_tokens: 100,\\n291 | \\t\\t\\t\\tcompletion_tokens: 50,\\n292 | \\t\\t\\t\\ttotal_tokens: 150,\\n293 | \\t\\t\\t\\t// No prompt_tokens_details\\n294 | \\t\\t\\t}\\n----\\n\\n# src/api/providers/__tests__/bedrock-reasoning.test.ts\\n 94 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n 95 | \\t\\t\\t\\tbudget_tokens: 4096, // Uses the full modelMaxThinkingTokens value\\n 96 | \\t\\t\\t})\\n----\\n143 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n144 | \\t\\t\\t\\tbudget_tokens: 8192,\\n145 | \\t\\t\\t})\\n----\\n266 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n267 | \\t\\t\\t\\tbudget_tokens: 4096,\\n268 | \\t\\t\\t})\\n----\\n\\n# src/api/providers/__tests__/xai.test.ts\\n228 | \\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n229 | \\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n230 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 20,\\n231 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcache_read_input_tokens: 5,\\n232 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcache_creation_input_tokens: 15,\\n233 | \\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n284 | \\t\\t\\t\\tmodel: modelId,\\n285 | \\t\\t\\t\\tmax_tokens: modelInfo.maxTokens,\\n286 | \\t\\t\\t\\ttemperature: 0,\\n----\\n\\n# src/api/providers/__tests__/anthropic.spec.ts\\n 19 | \\t\\t\\t\\t\\t\\tusage: {\\n 20 | \\t\\t\\t\\t\\t\\t\\tinput_tokens: 10,\\n 21 | \\t\\t\\t\\t\\t\\t\\toutput_tokens: 5,\\n 22 | \\t\\t\\t\\t\\t\\t},\\n----\\n 30 | \\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 31 | \\t\\t\\t\\t\\t\\t\\t\\t\\tinput_tokens: 100,\\n 32 | \\t\\t\\t\\t\\t\\t\\t\\t\\toutput_tokens: 50,\\n 33 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcache_creation_input_tokens: 20,\\n 34 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcache_read_input_tokens: 10,\\n 35 | \\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n105 | \\n106 | \\t\\tit(\\\"use apiKey for passing token if anthropicUseAuthToken is not set\\\", () => {\\n107 | \\t\\t\\tconst handlerWithCustomUrl = new AnthropicHandler({\\n----\\n112 | \\t\\t\\texpect(mockAnthropicConstructor.mock.calls[0]![0]!.apiKey).toEqual(\\\"test-api-key\\\")\\n113 | \\t\\t\\texpect(mockAnthropicConstructor.mock.calls[0]![0]!.authToken).toBeUndefined()\\n114 | \\t\\t})\\n115 | \\n116 | \\t\\tit(\\\"use apiKey for passing token if anthropicUseAuthToken is set but custom base URL is not given\\\", () => {\\n117 | \\t\\t\\tconst handlerWithCustomUrl = new AnthropicHandler({\\n----\\n123 | \\t\\t\\texpect(mockAnthropicConstructor.mock.calls[0]![0]!.apiKey).toEqual(\\\"test-api-key\\\")\\n124 | \\t\\t\\texpect(mockAnthropicConstructor.mock.calls[0]![0]!.authToken).toBeUndefined()\\n125 | \\t\\t})\\n126 | \\n127 | \\t\\tit(\\\"use authToken for passing token if both of anthropicBaseUrl and anthropicUseAuthToken are set\\\", () => {\\n128 | \\t\\t\\tconst customBaseUrl = \\\"https://custom.anthropic.com\\\"\\n----\\n135 | \\t\\t\\texpect(mockAnthropicConstructor).toHaveBeenCalledTimes(1)\\n136 | \\t\\t\\texpect(mockAnthropicConstructor.mock.calls[0]![0]!.authToken).toEqual(\\\"test-api-key\\\")\\n137 | \\t\\t\\texpect(mockAnthropicConstructor.mock.calls[0]![0]!.apiKey).toBeUndefined()\\n----\\n190 | \\t\\t\\t\\tmessages: [{ role: \\\"user\\\", content: \\\"Test prompt\\\" }],\\n191 | \\t\\t\\t\\tmax_tokens: 8192,\\n192 | \\t\\t\\t\\ttemperature: 0,\\n----\\n\\n# src/api/providers/__tests__/requesty.spec.ts\\n124 | \\t\\t\\t\\t\\t\\tusage: {\\n125 | \\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n126 | \\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 20,\\n127 | \\t\\t\\t\\t\\t\\t\\tprompt_tokens_details: {\\n128 | \\t\\t\\t\\t\\t\\t\\t\\tcaching_tokens: 5,\\n129 | \\t\\t\\t\\t\\t\\t\\t\\tcached_tokens: 2,\\n130 | \\t\\t\\t\\t\\t\\t\\t},\\n----\\n162 | \\t\\t\\t\\texpect.objectContaining({\\n163 | \\t\\t\\t\\t\\tmax_tokens: 8192,\\n164 | \\t\\t\\t\\t\\tmessages: [\\n----\\n204 | \\t\\t\\t\\tmodel: mockOptions.requestyModelId,\\n205 | \\t\\t\\t\\tmax_tokens: 8192,\\n206 | \\t\\t\\t\\tmessages: [{ role: \\\"system\\\", content: \\\"test prompt\\\" }],\\n----\\n\\n# src/api/providers/__tests__/openai-native.spec.ts\\n 29 | \\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 30 | \\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 31 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 32 | \\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 33 | \\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 55 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 56 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 57 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 58 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 59 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n145 | \\t\\t\\t\\t\\t\\tusage: {\\n146 | \\t\\t\\t\\t\\t\\t\\tprompt_tokens: 0,\\n147 | \\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 0,\\n148 | \\t\\t\\t\\t\\t\\t\\ttotal_tokens: 0,\\n149 | \\t\\t\\t\\t\\t\\t},\\n----\\n215 | \\t\\t\\t\\t{ choices: [{ delta: { content: \\\" there\\\" } }], usage: null },\\n216 | \\t\\t\\t\\t{ choices: [{ delta: { content: \\\"!\\\" } }], usage: { prompt_tokens: 10, completion_tokens: 5 } },\\n217 | \\t\\t\\t]\\n----\\n261 | \\t\\t\\t\\t{ choices: [{ delta: { content: null } }], usage: null },\\n262 | \\t\\t\\t\\t{ choices: [{ delta: { content: \\\"Hello\\\" } }], usage: { prompt_tokens: 10, completion_tokens: 5 } },\\n263 | \\t\\t\\t]\\n----\\n\\n# src/api/providers/__tests__/anthropic-vertex.spec.ts\\n 21 | \\t\\t\\t\\t\\t\\tusage: {\\n 22 | \\t\\t\\t\\t\\t\\t\\tinput_tokens: 10,\\n 23 | \\t\\t\\t\\t\\t\\t\\toutput_tokens: 5,\\n 24 | \\t\\t\\t\\t\\t\\t},\\n----\\n 32 | \\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 33 | \\t\\t\\t\\t\\t\\t\\t\\t\\tinput_tokens: 10,\\n 34 | \\t\\t\\t\\t\\t\\t\\t\\t\\toutput_tokens: 5,\\n 35 | \\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 95 | \\t\\t\\t\\t\\t\\tusage: {\\n 96 | \\t\\t\\t\\t\\t\\t\\tinput_tokens: 10,\\n 97 | \\t\\t\\t\\t\\t\\t\\toutput_tokens: 0,\\n 98 | \\t\\t\\t\\t\\t\\t},\\n----\\n118 | \\t\\t\\t\\t\\tusage: {\\n119 | \\t\\t\\t\\t\\t\\toutput_tokens: 5,\\n120 | \\t\\t\\t\\t\\t},\\n----\\n164 | \\t\\t\\t\\tmodel: \\\"claude-3-5-sonnet-v2@20241022\\\",\\n165 | \\t\\t\\t\\tmax_tokens: 8192,\\n166 | \\t\\t\\t\\ttemperature: 0,\\n----\\n284 | \\t\\t\\t\\t\\t\\tusage: {\\n285 | \\t\\t\\t\\t\\t\\t\\tinput_tokens: 10,\\n286 | \\t\\t\\t\\t\\t\\t\\toutput_tokens: 0,\\n287 | \\t\\t\\t\\t\\t\\t\\tcache_creation_input_tokens: 3,\\n288 | \\t\\t\\t\\t\\t\\t\\tcache_read_input_tokens: 2,\\n289 | \\t\\t\\t\\t\\t\\t},\\n----\\n309 | \\t\\t\\t\\t\\tusage: {\\n310 | \\t\\t\\t\\t\\t\\toutput_tokens: 5,\\n311 | \\t\\t\\t\\t\\t},\\n----\\n419 | \\t\\t\\t\\t\\t\\tusage: {\\n420 | \\t\\t\\t\\t\\t\\t\\tinput_tokens: 10,\\n421 | \\t\\t\\t\\t\\t\\t\\toutput_tokens: 0,\\n422 | \\t\\t\\t\\t\\t\\t\\tcache_creation_input_tokens: 5,\\n423 | \\t\\t\\t\\t\\t\\t\\tcache_read_input_tokens: 3,\\n424 | \\t\\t\\t\\t\\t\\t},\\n----\\n484 | \\t\\t\\t\\t\\t\\tusage: {\\n485 | \\t\\t\\t\\t\\t\\t\\tinput_tokens: 10,\\n486 | \\t\\t\\t\\t\\t\\t\\toutput_tokens: 0,\\n487 | \\t\\t\\t\\t\\t\\t},\\n----\\n618 | \\t\\t\\t\\tmodel: \\\"claude-3-5-sonnet-v2@20241022\\\",\\n619 | \\t\\t\\t\\tmax_tokens: 8192,\\n620 | \\t\\t\\t\\ttemperature: 0,\\n----\\n752 | \\n753 | \\t\\t\\t// Test with default thinking budget (80% of max tokens)\\n754 | \\t\\t\\tconst handlerWithDefaultBudget = new AnthropicVertexHandler({\\n----\\n767 | \\t\\t\\t\\tvertexRegion: \\\"us-central1\\\",\\n768 | \\t\\t\\t\\tmodelMaxTokens: 1000, // This would result in 800 tokens for thinking, but minimum is 1024\\n769 | \\t\\t\\t})\\n----\\n789 | \\t\\t\\t\\t\\t\\tmodel: options.model,\\n790 | \\t\\t\\t\\t\\t\\tusage: { input_tokens: 10, output_tokens: 5 },\\n791 | \\t\\t\\t\\t\\t}\\n----\\n794 | \\t\\t\\t\\t\\tasync *[Symbol.asyncIterator]() {\\n795 | \\t\\t\\t\\t\\t\\tyield { type: \\\"message_start\\\", message: { usage: { input_tokens: 10, output_tokens: 5 } } }\\n796 | \\t\\t\\t\\t\\t},\\n----\\n806 | \\t\\t\\t\\texpect.objectContaining({\\n807 | \\t\\t\\t\\t\\tthinking: { type: \\\"enabled\\\", budget_tokens: 4096 },\\n808 | \\t\\t\\t\\t\\ttemperature: 1.0, // Thinking requires temperature 1.0\\n----\\n\\n# src/api/providers/__tests__/lmstudio.test.ts\\n 25 | \\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 26 | \\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 27 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 28 | \\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 29 | \\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 51 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 52 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 53 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 54 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 55 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n\\n# src/api/providers/__tests__/glama.spec.ts\\n 57 | \\t\\t\\t\\t\\t\\t\\t\\t\\tchoices: [{ delta: {}, index: 0 }],\\n 58 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: { prompt_tokens: 10, completion_tokens: 5, total_tokens: 15 },\\n 59 | \\t\\t\\t\\t\\t\\t\\t\\t}\\n----\\n112 | \\t\\t\\tusage: {\\n113 | \\t\\t\\t\\tprompt_tokens: 10,\\n114 | \\t\\t\\t\\tcompletion_tokens: 5,\\n115 | \\t\\t\\t\\ttotal_tokens: 15,\\n116 | \\t\\t\\t},\\n----\\n176 | \\t\\t\\t\\t\\ttemperature: 0,\\n177 | \\t\\t\\t\\t\\tmax_tokens: 8192,\\n178 | \\t\\t\\t\\t}),\\n----\\n194 | \\n195 | \\t\\tit(\\\"should not set max_tokens for non-Anthropic models\\\", async () => {\\n196 | \\t\\t\\t// Reset mock to clear any previous calls\\n----\\n213 | \\t\\t\\t)\\n214 | \\t\\t\\texpect(mockCreate.mock.calls[0][0]).not.toHaveProperty(\\\"max_tokens\\\")\\n215 | \\t\\t})\\n----\\n\\n# src/api/providers/__tests__/openai-usage-tracking.spec.ts\\n 28 | \\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 29 | \\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 30 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 31 | \\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 32 | \\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 47 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 48 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 49 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 2,\\n 50 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 12,\\n 51 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 62 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 63 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 64 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 4,\\n 65 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 14,\\n 66 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 77 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 78 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 79 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 80 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 81 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n155 | \\t\\t\\t\\t\\t\\tchoices: [{ message: { role: \\\"assistant\\\", content: \\\"Test response\\\" } }],\\n156 | \\t\\t\\t\\t\\t\\tusage: { prompt_tokens: 10, completion_tokens: 5, total_tokens: 15 },\\n157 | \\t\\t\\t\\t\\t}\\n----\\n177 | \\t\\t\\t\\t\\t\\t\\tusage: {\\n178 | \\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n179 | \\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n180 | \\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n181 | \\t\\t\\t\\t\\t\\t\\t},\\n----\\n\\n# src/api/providers/__tests__/groq.spec.ts\\n101 | \\t\\t\\t\\t\\t\\t\\tdone: false,\\n102 | \\t\\t\\t\\t\\t\\t\\tvalue: { choices: [{ delta: {} }], usage: { prompt_tokens: 10, completion_tokens: 20 } },\\n103 | \\t\\t\\t\\t\\t\\t})\\n----\\n139 | \\t\\t\\t\\tmodel: modelId,\\n140 | \\t\\t\\t\\tmax_tokens: modelInfo.maxTokens,\\n141 | \\t\\t\\t\\ttemperature: 0.5,\\n----\\n\\n# src/api/providers/__tests__/unbound.spec.ts\\n 82 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 83 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 84 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 85 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 86 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 91 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 92 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 8,\\n 93 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 4,\\n 94 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 12,\\n 95 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcache_creation_input_tokens: 3,\\n 96 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcache_read_input_tokens: 2,\\n 97 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n232 | \\t\\t\\t\\t\\ttemperature: 0,\\n233 | \\t\\t\\t\\t\\tmax_tokens: 8192,\\n234 | \\t\\t\\t\\t}),\\n----\\n253 | \\n254 | \\t\\tit(\\\"should not set max_tokens for non-Anthropic models\\\", async () => {\\n255 | \\t\\t\\tmockCreate.mockClear()\\n----\\n277 | \\n278 | \\t\\t\\texpect(mockCreate.mock.calls[0][0]).not.toHaveProperty(\\\"max_tokens\\\")\\n279 | \\t\\t})\\n----\\n\\n# src/api/providers/__tests__/bedrock-invokedModelId.spec.ts\\n176 | \\n177 | \\t\\t// The last usage event should have the token counts from the metadata\\n178 | \\t\\tconst lastUsageEvent = usageEvents[usageEvents.length - 1]\\n179 | \\t\\t// Expect the usage event to include all token information\\n180 | \\t\\texpect(lastUsageEvent).toMatchObject({\\n----\\n183 | \\t\\t\\toutputTokens: 200,\\n184 | \\t\\t\\t// Cache tokens may be present with default values\\n185 | \\t\\t\\tcacheReadTokens: expect.any(Number),\\n----\\n\\n# src/api/providers/requesty.ts\\n 19 | // Requesty usage includes an extra field for Anthropic use cases.\\n 20 | // Safely cast the prompt token details section to the appropriate structure.\\n 21 | interface RequestyUsage extends OpenAI.CompletionUsage {\\n 22 | \\tprompt_tokens_details?: {\\n 23 | \\t\\tcaching_tokens?: number\\n 24 | \\t\\tcached_tokens?: number\\n 25 | \\t}\\n----\\n 76 | \\t\\tconst requestyUsage = usage as RequestyUsage\\n 77 | \\t\\tconst inputTokens = requestyUsage?.prompt_tokens || 0\\n 78 | \\t\\tconst outputTokens = requestyUsage?.completion_tokens || 0\\n 79 | \\t\\tconst cacheWriteTokens = requestyUsage?.prompt_tokens_details?.caching_tokens || 0\\n 80 | \\t\\tconst cacheReadTokens = requestyUsage?.prompt_tokens_details?.cached_tokens || 0\\n 81 | \\t\\tconst totalCost = modelInfo\\n----\\n102 | \\t\\t\\tinfo,\\n103 | \\t\\t\\tmaxTokens: max_tokens,\\n104 | \\t\\t\\ttemperature,\\n----\\n116 | \\t\\t\\tmodel,\\n117 | \\t\\t\\tmax_tokens,\\n118 | \\t\\t\\ttemperature,\\n----\\n150 | \\tasync completePrompt(prompt: string): Promise<string> {\\n151 | \\t\\tconst { id: model, maxTokens: max_tokens, temperature } = await this.fetchModel()\\n152 | \\n----\\n156 | \\t\\t\\tmodel,\\n157 | \\t\\t\\tmax_tokens,\\n158 | \\t\\t\\tmessages: openAiMessages,\\n----\\n\\n# src/api/providers/lite-llm.ts\\n 46 | \\n 47 | \\t\\t// Required by some providers; others default to max tokens allowed\\n 48 | \\t\\tlet maxTokens: number | undefined = info.maxTokens ?? undefined\\n----\\n 51 | \\t\\t\\tmodel: modelId,\\n 52 | \\t\\t\\tmax_tokens: maxTokens,\\n 53 | \\t\\t\\tmessages: openAiMessages,\\n----\\n 84 | \\t\\t\\t\\t\\ttype: \\\"usage\\\",\\n 85 | \\t\\t\\t\\t\\tinputTokens: lastUsage.prompt_tokens || 0,\\n 86 | \\t\\t\\t\\t\\toutputTokens: lastUsage.completion_tokens || 0,\\n 87 | \\t\\t\\t\\t\\tcacheWriteTokens: lastUsage.cache_creation_input_tokens || 0,\\n 88 | \\t\\t\\t\\t\\tcacheReadTokens: lastUsage.prompt_tokens_details?.cached_tokens || 0,\\n 89 | \\t\\t\\t\\t}\\n----\\n121 | \\n122 | \\t\\t\\trequestOptions.max_tokens = info.maxTokens\\n123 | \\n----\\n136 | interface LiteLLMUsage extends OpenAI.CompletionUsage {\\n137 | \\tcache_creation_input_tokens?: number\\n138 | }\\n----\\n\\n# src/api/providers/fetchers/unbound.ts\\n 34 | \\t\\t\\t\\t\\tcase modelId.startsWith(\\\"anthropic/\\\"):\\n 35 | \\t\\t\\t\\t\\t\\t// Set max tokens to 8192 for supported Anthropic models\\n 36 | \\t\\t\\t\\t\\t\\tif (modelInfo.maxTokens !== 4096) {\\n----\\n\\n# src/api/providers/deepseek.ts\\n 32 | \\t\\t\\ttype: \\\"usage\\\",\\n 33 | \\t\\t\\tinputTokens: usage?.prompt_tokens || 0,\\n 34 | \\t\\t\\toutputTokens: usage?.completion_tokens || 0,\\n 35 | \\t\\t\\tcacheWriteTokens: usage?.prompt_tokens_details?.cache_miss_tokens,\\n 36 | \\t\\t\\tcacheReadTokens: usage?.prompt_tokens_details?.cached_tokens,\\n 37 | \\t\\t}\\n----\\n\\n# src/api/providers/openai.ts\\n162 | \\t\\t\\tif (this.options.includeMaxTokens) {\\n163 | \\t\\t\\t\\trequestOptions.max_tokens = modelInfo.maxTokens\\n164 | \\t\\t\\t}\\n----\\n241 | \\t\\t\\ttype: \\\"usage\\\",\\n242 | \\t\\t\\tinputTokens: usage?.prompt_tokens || 0,\\n243 | \\t\\t\\toutputTokens: usage?.completion_tokens || 0,\\n244 | \\t\\t\\tcacheWriteTokens: usage?.cache_creation_input_tokens || undefined,\\n245 | \\t\\t\\tcacheReadTokens: usage?.cache_read_input_tokens || undefined,\\n246 | \\t\\t}\\n----\\n347 | \\t\\t\\t\\t\\ttype: \\\"usage\\\",\\n348 | \\t\\t\\t\\t\\tinputTokens: chunk.usage.prompt_tokens || 0,\\n349 | \\t\\t\\t\\t\\toutputTokens: chunk.usage.completion_tokens || 0,\\n350 | \\t\\t\\t\\t}\\n----\\n\\n# src/api/providers/fetchers/openrouter.ts\\n 21 | \\tmodality: z.string().nullish(),\\n 22 | \\ttokenizer: z.string().nullish(),\\n 23 | })\\n----\\n 35 | \\tcontext_length: z.number(),\\n 36 | \\tmax_completion_tokens: z.number().nullish(),\\n 37 | \\tpricing: openRouterPricingSchema.optional(),\\n----\\n 48 | \\tarchitecture: openRouterArchitectureSchema.optional(),\\n 49 | \\ttop_provider: z.object({ max_completion_tokens: z.number().nullish() }).optional(),\\n 50 | \\tsupported_parameters: z.array(z.string()).optional(),\\n----\\n115 | \\t\\t\\t\\tmodality: architecture?.modality,\\n116 | \\t\\t\\t\\tmaxTokens: top_provider?.max_completion_tokens,\\n117 | \\t\\t\\t\\tsupportedParameters: supported_parameters,\\n----\\n155 | \\t\\t\\t\\tmodality: architecture?.modality,\\n156 | \\t\\t\\t\\tmaxTokens: endpoint.max_completion_tokens,\\n157 | \\t\\t\\t})\\n----\\n\\n# src/api/providers/openai-native.ts\\n146 | \\tprivate async *yieldUsage(info: ModelInfo, usage: OpenAI.Completions.CompletionUsage | undefined): ApiStream {\\n147 | \\t\\tconst inputTokens = usage?.prompt_tokens || 0 // sum of cache hits and misses\\n148 | \\t\\tconst outputTokens = usage?.completion_tokens || 0\\n149 | \\t\\tconst cacheReadTokens = usage?.prompt_tokens_details?.cached_tokens || 0\\n150 | \\t\\tconst cacheWriteTokens = 0\\n----\\n\\n# src/api/providers/fetchers/requesty.ts\\n 31 | \\t\\t\\tconst modelInfo: ModelInfo = {\\n 32 | \\t\\t\\t\\tmaxTokens: rawModel.max_output_tokens,\\n 33 | \\t\\t\\t\\tcontextWindow: rawModel.context_window,\\n----\\n\\n# src/api/providers/anthropic.ts\\n 29 | \\t\\tconst apiKeyFieldName =\\n 30 | \\t\\t\\tthis.options.anthropicBaseUrl && this.options.anthropicUseAuthToken ? \\\"authToken\\\" : \\\"apiKey\\\"\\n 31 | \\n----\\n 75 | \\t\\t\\t\\t\\t\\tmodel: modelId,\\n 76 | \\t\\t\\t\\t\\t\\tmax_tokens: maxTokens ?? ANTHROPIC_DEFAULT_MAX_TOKENS,\\n 77 | \\t\\t\\t\\t\\t\\ttemperature,\\n----\\n124 | \\t\\t\\t\\t\\tmodel: modelId,\\n125 | \\t\\t\\t\\t\\tmax_tokens: maxTokens ?? ANTHROPIC_DEFAULT_MAX_TOKENS,\\n126 | \\t\\t\\t\\t\\ttemperature,\\n----\\n142 | \\t\\t\\t\\t\\t\\ttype: \\\"usage\\\",\\n143 | \\t\\t\\t\\t\\t\\tinputTokens: usage.input_tokens || 0,\\n144 | \\t\\t\\t\\t\\t\\toutputTokens: usage.output_tokens || 0,\\n145 | \\t\\t\\t\\t\\t\\tcacheWriteTokens: usage.cache_creation_input_tokens || undefined,\\n146 | \\t\\t\\t\\t\\t\\tcacheReadTokens: usage.cache_read_input_tokens || undefined,\\n147 | \\t\\t\\t\\t\\t}\\n----\\n151 | \\t\\t\\t\\tcase \\\"message_delta\\\":\\n152 | \\t\\t\\t\\t\\t// Tells us stop_reason, stop_sequence, and output tokens\\n153 | \\t\\t\\t\\t\\t// along the way and at the end of the message.\\n----\\n156 | \\t\\t\\t\\t\\t\\tinputTokens: 0,\\n157 | \\t\\t\\t\\t\\t\\toutputTokens: chunk.usage.output_tokens || 0,\\n158 | \\t\\t\\t\\t\\t}\\n----\\n231 | \\t\\t\\tmodel,\\n232 | \\t\\t\\tmax_tokens: ANTHROPIC_DEFAULT_MAX_TOKENS,\\n233 | \\t\\t\\tthinking: undefined,\\n----\\n243 | \\t/**\\n244 | \\t * Counts tokens for the given content using Anthropic's API\\n245 | \\t *\\n246 | \\t * @param content The content blocks to count tokens for\\n247 | \\t * @returns A promise resolving to the token count\\n248 | \\t */\\n----\\n258 | \\n259 | \\t\\t\\treturn response.input_tokens\\n260 | \\t\\t} catch (error) {\\n261 | \\t\\t\\t// Log error but fallback to tiktoken estimation\\n262 | \\t\\t\\tconsole.warn(\\\"Anthropic token counting failed, using fallback\\\", error)\\n263 | \\n----\\n\\n# src/api/providers/base-openai-compatible-provider.ts\\n 71 | \\t\\t\\tid: model,\\n 72 | \\t\\t\\tinfo: { maxTokens: max_tokens },\\n 73 | \\t\\t} = this.getModel()\\n----\\n 78 | \\t\\t\\tmodel,\\n 79 | \\t\\t\\tmax_tokens,\\n 80 | \\t\\t\\ttemperature,\\n----\\n100 | \\t\\t\\t\\t\\ttype: \\\"usage\\\",\\n101 | \\t\\t\\t\\t\\tinputTokens: chunk.usage.prompt_tokens || 0,\\n102 | \\t\\t\\t\\t\\toutputTokens: chunk.usage.completion_tokens || 0,\\n103 | \\t\\t\\t\\t}\\n----\\n\\n# src/api/providers/xai.ts\\n 52 | \\t\\t\\tmodel: modelId,\\n 53 | \\t\\t\\tmax_tokens: modelInfo.maxTokens,\\n 54 | \\t\\t\\ttemperature: this.options.modelTemperature ?? XAI_DEFAULT_TEMPERATURE,\\n----\\n 78 | \\t\\t\\tif (chunk.usage) {\\n 79 | \\t\\t\\t\\t// Extract detailed token information if available\\n 80 | \\t\\t\\t\\t// First check for prompt_tokens_details structure (real API response)\\n 81 | \\t\\t\\t\\tconst promptDetails = \\\"prompt_tokens_details\\\" in chunk.usage ? chunk.usage.prompt_tokens_details : null;\\n 82 | \\t\\t\\t\\tconst cachedTokens = promptDetails && \\\"cached_tokens\\\" in promptDetails ? promptDetails.cached_tokens : 0;\\n 83 | \\n 84 | \\t\\t\\t\\t// Fall back to direct fields in usage (used in test mocks)\\n 85 | \\t\\t\\t\\tconst readTokens = cachedTokens || (\\\"cache_read_input_tokens\\\" in chunk.usage ? (chunk.usage as any).cache_read_input_tokens : 0);\\n 86 | \\t\\t\\t\\tconst writeTokens = \\\"cache_creation_input_tokens\\\" in chunk.usage ? (chunk.usage as any).cache_creation_input_tokens : 0;\\n 87 | \\n----\\n 89 | \\t\\t\\t\\t\\ttype: \\\"usage\\\",\\n 90 | \\t\\t\\t\\t\\tinputTokens: chunk.usage.prompt_tokens || 0,\\n 91 | \\t\\t\\t\\t\\toutputTokens: chunk.usage.completion_tokens || 0,\\n 92 | \\t\\t\\t\\t\\tcacheReadTokens: readTokens,\\n----\\n\\n# src/api/providers/bedrock.ts\\n 52 | \\t\\ttype: \\\"enabled\\\"\\n 53 | \\t\\tbudget_tokens: number\\n 54 | \\t}\\n----\\n109 | \\tmessageStop?: {\\n110 | \\t\\tstopReason?: \\\"end_turn\\\" | \\\"tool_use\\\" | \\\"max_tokens\\\" | \\\"stop_sequence\\\"\\n111 | \\t\\tadditionalModelResponseFields?: Record<string, unknown>\\n----\\n349 | \\t\\t\\t\\t\\ttype: \\\"enabled\\\",\\n350 | \\t\\t\\t\\t\\tbudget_tokens: metadata?.thinking?.maxThinkingTokens || modelConfig.reasoningBudget || 4096,\\n351 | \\t\\t\\t\\t},\\n----\\n418 | \\n419 | \\t\\t\\t\\t\\t// Check both field naming conventions for cache tokens\\n420 | \\t\\t\\t\\t\\tconst cacheReadTokens = usage.cacheReadInputTokens || usage.cacheReadInputTokenCount || 0\\n----\\n422 | \\n423 | \\t\\t\\t\\t\\t// Always include all available token information\\n424 | \\t\\t\\t\\t\\tyield {\\n----\\n450 | \\n451 | \\t\\t\\t\\t\\t\\t\\t// Check both field naming conventions for cache tokens\\n452 | \\t\\t\\t\\t\\t\\t\\tconst cacheReadTokens =\\n----\\n1049 | \\t\\tTOO_MANY_TOKENS: {\\n1050 | \\t\\t\\tpatterns: [\\\"too many tokens\\\"],\\n1051 | \\t\\t\\tmessageTemplate: `\\\"Too many tokens\\\" error detected.\\n1052 | Possible Causes:\\n1053 | 1. Input exceeds model's context window limit\\n1054 | 2. Rate limiting (too many tokens per minute)\\n1055 | 3. Quota exceeded for token usage\\n1056 | 4. Other token-related service limitations\\n1057 | \\n----\\n\\n# src/api/providers/fetchers/litellm.ts\\n 52 | \\t\\t\\t\\tmodels[modelName] = {\\n 53 | \\t\\t\\t\\t\\tmaxTokens: modelInfo.max_tokens || 8192,\\n 54 | \\t\\t\\t\\t\\tcontextWindow: modelInfo.max_input_tokens || 200000,\\n 55 | \\t\\t\\t\\t\\tsupportsImages: Boolean(modelInfo.supports_vision),\\n----\\n 58 | \\t\\t\\t\\t\\tsupportsPromptCache: Boolean(modelInfo.supports_prompt_caching),\\n 59 | \\t\\t\\t\\t\\tinputPrice: modelInfo.input_cost_per_token ? modelInfo.input_cost_per_token * 1000000 : undefined,\\n 60 | \\t\\t\\t\\t\\toutputPrice: modelInfo.output_cost_per_token\\n 61 | \\t\\t\\t\\t\\t\\t? modelInfo.output_cost_per_token * 1000000\\n 62 | \\t\\t\\t\\t\\t\\t: undefined,\\n 63 | \\t\\t\\t\\t\\tcacheWritesPrice: modelInfo.cache_creation_input_token_cost\\n 64 | \\t\\t\\t\\t\\t\\t? modelInfo.cache_creation_input_token_cost * 1000000\\n 65 | \\t\\t\\t\\t\\t\\t: undefined,\\n 66 | \\t\\t\\t\\t\\tcacheReadsPrice: modelInfo.cache_read_input_token_cost\\n 67 | \\t\\t\\t\\t\\t\\t? modelInfo.cache_read_input_token_cost * 1000000\\n 68 | \\t\\t\\t\\t\\t\\t: undefined,\\n----\\n\\n# src/api/providers/fetchers/__tests__/openrouter.spec.ts\\n139 | \\t\\t\\t\\tsupportsReasoningEffort: false,\\n140 | \\t\\t\\t\\tsupportedParameters: [\\\"max_tokens\\\", \\\"temperature\\\", \\\"reasoning\\\", \\\"include_reasoning\\\"],\\n141 | \\t\\t\\t})\\n----\\n156 | \\t\\t\\t\\tsupportsReasoningEffort: true,\\n157 | \\t\\t\\t\\tsupportedParameters: [\\\"max_tokens\\\", \\\"temperature\\\", \\\"reasoning\\\", \\\"include_reasoning\\\"],\\n158 | \\t\\t\\t})\\n----\\n\\n# src/api/providers/ollama.ts\\n 79 | \\t\\t\\t\\ttype: \\\"usage\\\",\\n 80 | \\t\\t\\t\\tinputTokens: lastUsage?.prompt_tokens || 0,\\n 81 | \\t\\t\\t\\toutputTokens: lastUsage?.completion_tokens || 0,\\n 82 | \\t\\t\\t}\\n----\\n\\n# src/api/providers/base-provider.ts\\n 21 | \\t/**\\n 22 | \\t * Default token counting implementation using tiktoken.\\n 23 | \\t * Providers can override this to use their native token counting endpoints.\\n 24 | \\t *\\n 25 | \\t * @param content The content to count tokens for\\n 26 | \\t * @returns A promise resolving to the token count\\n 27 | \\t */\\n----\\n\\n# src/api/providers/fetchers/__tests__/litellm.test.ts\\n 41 | \\t\\t\\t\\t\\t\\tmodel_info: {\\n 42 | \\t\\t\\t\\t\\t\\t\\tmax_tokens: 4096,\\n 43 | \\t\\t\\t\\t\\t\\t\\tmax_input_tokens: 200000,\\n 44 | \\t\\t\\t\\t\\t\\t\\tsupports_vision: true,\\n 45 | \\t\\t\\t\\t\\t\\t\\tsupports_prompt_caching: false,\\n 46 | \\t\\t\\t\\t\\t\\t\\tinput_cost_per_token: 0.000003,\\n 47 | \\t\\t\\t\\t\\t\\t\\toutput_cost_per_token: 0.000015,\\n 48 | \\t\\t\\t\\t\\t\\t\\tsupports_computer_use: true,\\n----\\n 56 | \\t\\t\\t\\t\\t\\tmodel_info: {\\n 57 | \\t\\t\\t\\t\\t\\t\\tmax_tokens: 8192,\\n 58 | \\t\\t\\t\\t\\t\\t\\tmax_input_tokens: 128000,\\n 59 | \\t\\t\\t\\t\\t\\t\\tsupports_vision: false,\\n 60 | \\t\\t\\t\\t\\t\\t\\tsupports_prompt_caching: false,\\n 61 | \\t\\t\\t\\t\\t\\t\\tinput_cost_per_token: 0.00001,\\n 62 | \\t\\t\\t\\t\\t\\t\\toutput_cost_per_token: 0.00003,\\n 63 | \\t\\t\\t\\t\\t\\t\\tsupports_computer_use: false,\\n----\\n108 | \\n109 | \\tit(\\\"makes request without authorization header when no API key provided\\\", async () => {\\n110 | \\t\\tconst mockResponse = {\\n----\\n134 | \\t\\t\\t\\t\\t\\tmodel_info: {\\n135 | \\t\\t\\t\\t\\t\\t\\tmax_tokens: 4096,\\n136 | \\t\\t\\t\\t\\t\\t\\tmax_input_tokens: 200000,\\n137 | \\t\\t\\t\\t\\t\\t\\tsupports_vision: true,\\n----\\n146 | \\t\\t\\t\\t\\t\\tmodel_info: {\\n147 | \\t\\t\\t\\t\\t\\t\\tmax_tokens: 4096,\\n148 | \\t\\t\\t\\t\\t\\t\\tmax_input_tokens: 200000,\\n149 | \\t\\t\\t\\t\\t\\t\\tsupports_vision: false,\\n----\\n205 | \\t\\t\\t\\tstatus: 401,\\n206 | \\t\\t\\t\\tstatusText: \\\"Unauthorized\\\",\\n207 | \\t\\t\\t},\\n----\\n214 | \\t\\tawait expect(getLiteLLMModels(DUMMY_INVALID_KEY, \\\"http://localhost:4000\\\")).rejects.toThrow(\\n215 | \\t\\t\\t\\\"Failed to fetch LiteLLM models: 401 Unauthorized. Check base URL and API key.\\\",\\n216 | \\t\\t)\\n----\\n278 | \\t\\t\\t\\t\\t\\tmodel_info: {\\n279 | \\t\\t\\t\\t\\t\\t\\tmax_tokens: 4096,\\n280 | \\t\\t\\t\\t\\t\\t\\tmax_input_tokens: 200000,\\n281 | \\t\\t\\t\\t\\t\\t\\tsupports_vision: true,\\n----\\n291 | \\t\\t\\t\\t\\t\\tmodel_info: {\\n292 | \\t\\t\\t\\t\\t\\t\\tmax_tokens: 8192,\\n293 | \\t\\t\\t\\t\\t\\t\\tmax_input_tokens: 128000,\\n294 | \\t\\t\\t\\t\\t\\t\\tsupports_vision: false,\\n----\\n339 | \\t\\t\\t\\t\\t\\tmodel_info: {\\n340 | \\t\\t\\t\\t\\t\\t\\tmax_tokens: 4096,\\n341 | \\t\\t\\t\\t\\t\\t\\tmax_input_tokens: 200000,\\n342 | \\t\\t\\t\\t\\t\\t\\tsupports_vision: true,\\n----\\n352 | \\t\\t\\t\\t\\t\\tmodel_info: {\\n353 | \\t\\t\\t\\t\\t\\t\\tmax_tokens: 8192,\\n354 | \\t\\t\\t\\t\\t\\t\\tmax_input_tokens: 128000,\\n355 | \\t\\t\\t\\t\\t\\t\\tsupports_vision: false,\\n----\\n365 | \\t\\t\\t\\t\\t\\tmodel_info: {\\n366 | \\t\\t\\t\\t\\t\\t\\tmax_tokens: 8192,\\n367 | \\t\\t\\t\\t\\t\\t\\tmax_input_tokens: 128000,\\n368 | \\t\\t\\t\\t\\t\\t\\tsupports_vision: false,\\n----\\n424 | \\t\\t\\t\\t\\t\\tmodel_info: {\\n425 | \\t\\t\\t\\t\\t\\t\\tmax_tokens: 4096,\\n426 | \\t\\t\\t\\t\\t\\t\\tmax_input_tokens: 200000,\\n427 | \\t\\t\\t\\t\\t\\t\\tsupports_vision: true,\\n----\\n436 | \\t\\t\\t\\t\\t\\tmodel_info: {\\n437 | \\t\\t\\t\\t\\t\\t\\tmax_tokens: 4096,\\n438 | \\t\\t\\t\\t\\t\\t\\tmax_input_tokens: 200000,\\n439 | \\t\\t\\t\\t\\t\\t\\tsupports_vision: true,\\n----\\n448 | \\t\\t\\t\\t\\t\\tmodel_info: {\\n449 | \\t\\t\\t\\t\\t\\t\\tmax_tokens: 4096,\\n450 | \\t\\t\\t\\t\\t\\t\\tmax_input_tokens: 200000,\\n451 | \\t\\t\\t\\t\\t\\t\\tsupports_vision: true,\\n----\\n\\n# src/api/providers/anthropic-vertex.ts\\n  2 | import { AnthropicVertex } from \\\"@anthropic-ai/vertex-sdk\\\"\\n  3 | import { GoogleAuth, JWTInput } from \\\"google-auth-library\\\"\\n  4 | \\n----\\n 41 | \\t\\t\\t\\tgoogleAuth: new GoogleAuth({\\n 42 | \\t\\t\\t\\t\\tscopes: [\\\"https://www.googleapis.com/auth/cloud-platform\\\"],\\n 43 | \\t\\t\\t\\t\\tcredentials: safeJsonParse<JWTInput>(this.options.vertexJsonCredentials, undefined),\\n----\\n 50 | \\t\\t\\t\\tgoogleAuth: new GoogleAuth({\\n 51 | \\t\\t\\t\\t\\tscopes: [\\\"https://www.googleapis.com/auth/cloud-platform\\\"],\\n 52 | \\t\\t\\t\\t\\tkeyFile: this.options.vertexKeyFile,\\n----\\n 87 | \\t\\t\\tmodel: id,\\n 88 | \\t\\t\\tmax_tokens: maxTokens ?? ANTHROPIC_DEFAULT_MAX_TOKENS,\\n 89 | \\t\\t\\ttemperature,\\n----\\n107 | \\t\\t\\t\\t\\t\\ttype: \\\"usage\\\",\\n108 | \\t\\t\\t\\t\\t\\tinputTokens: usage.input_tokens || 0,\\n109 | \\t\\t\\t\\t\\t\\toutputTokens: usage.output_tokens || 0,\\n110 | \\t\\t\\t\\t\\t\\tcacheWriteTokens: usage.cache_creation_input_tokens || undefined,\\n111 | \\t\\t\\t\\t\\t\\tcacheReadTokens: usage.cache_read_input_tokens || undefined,\\n112 | \\t\\t\\t\\t\\t}\\n----\\n119 | \\t\\t\\t\\t\\t\\tinputTokens: 0,\\n120 | \\t\\t\\t\\t\\t\\toutputTokens: chunk.usage!.output_tokens || 0,\\n121 | \\t\\t\\t\\t\\t}\\n----\\n189 | \\t\\t\\t\\tmodel: id,\\n190 | \\t\\t\\t\\tmax_tokens: maxTokens,\\n191 | \\t\\t\\t\\ttemperature,\\n----\\n\\n# src/services/mcp/__tests__/McpHub.test.ts\\n 50 | \\t\\t\\tscheme: \\\"file\\\",\\n 51 | \\t\\t\\tauthority: \\\"\\\",\\n 52 | \\t\\t\\tpath: \\\"/test/path\\\",\\n----\\n\\n# src/services/code-index/embedders/openai.ts\\n 52 | \\t\\t\\t\\t\\tconsole.warn(\\n 53 | \\t\\t\\t\\t\\t\\t`Text at index ${i} exceeds maximum token limit (${itemTokens} > ${MAX_ITEM_TOKENS}). Skipping.`,\\n 54 | \\t\\t\\t\\t\\t)\\n----\\n108 | \\t\\t\\t\\t\\tusage: {\\n109 | \\t\\t\\t\\t\\t\\tpromptTokens: response.usage?.prompt_tokens || 0,\\n110 | \\t\\t\\t\\t\\t\\ttotalTokens: response.usage?.total_tokens || 0,\\n111 | \\t\\t\\t\\t\\t},\\n----\\n\\n# src/services/code-index/embedders/openai-compatible.ts\\n 18 | \\tusage?: {\\n 19 | \\t\\tprompt_tokens?: number\\n 20 | \\t\\ttotal_tokens?: number\\n 21 | \\t}\\n----\\n 34 | \\t * @param baseUrl The base URL for the OpenAI-compatible API endpoint\\n 35 | \\t * @param apiKey The API key for authentication\\n 36 | \\t * @param modelId Optional model identifier (defaults to \\\"text-embedding-3-small\\\")\\n----\\n 75 | \\t\\t\\t\\t\\tconsole.warn(\\n 76 | \\t\\t\\t\\t\\t\\t`Text at index ${i} exceeds maximum token limit (${itemTokens} > ${MAX_ITEM_TOKENS}). Skipping.`,\\n 77 | \\t\\t\\t\\t\\t)\\n----\\n156 | \\t\\t\\t\\t\\tusage: {\\n157 | \\t\\t\\t\\t\\t\\tpromptTokens: response.usage?.prompt_tokens || 0,\\n158 | \\t\\t\\t\\t\\t\\ttotalTokens: response.usage?.total_tokens || 0,\\n159 | \\t\\t\\t\\t\\t},\\n----\\n\\n# src/services/marketplace/types.ts\\n 20 | \\tdescription: string\\n 21 | \\tauthor?: string\\n 22 | \\tauthorUrl?: string\\n 23 | \\ttags?: string[]\\n----\\n 45 | \\tdescription: string\\n 46 | \\tauthor?: string\\n 47 | \\tauthorUrl?: string\\n 48 | \\turl: string // Required url field\\n----\\n 62 | \\ttype: MarketplaceItemType\\n 63 | \\tauthor?: string\\n 64 | \\tauthorUrl?: string\\n 65 | \\turl?: string // Optional - only MCPs have url\\n----\\n\\n# src/services/code-index/embedders/__tests__/openai-compatible.spec.ts\\n103 | \\t\\t\\t\\tdata: [{ embedding: [0.1, 0.2, 0.3] }],\\n104 | \\t\\t\\t\\tusage: { prompt_tokens: 10, total_tokens: 15 },\\n105 | \\t\\t\\t}\\n----\\n124 | \\t\\t\\t\\tdata: [{ embedding: [0.1, 0.2, 0.3] }, { embedding: [0.4, 0.5, 0.6] }],\\n125 | \\t\\t\\t\\tusage: { prompt_tokens: 20, total_tokens: 30 },\\n126 | \\t\\t\\t}\\n----\\n149 | \\t\\t\\t\\tdata: [{ embedding: [0.1, 0.2, 0.3] }],\\n150 | \\t\\t\\t\\tusage: { prompt_tokens: 10, total_tokens: 15 },\\n151 | \\t\\t\\t}\\n----\\n194 | \\t\\t\\t\\t\\tdata: [{ embedding: base64String }], // Base64 string instead of array\\n195 | \\t\\t\\t\\t\\tusage: { prompt_tokens: 10, total_tokens: 15 },\\n196 | \\t\\t\\t\\t}\\n----\\n226 | \\t\\t\\t\\t\\tdata: [{ embedding: base64String1 }, { embedding: base64String2 }],\\n227 | \\t\\t\\t\\t\\tusage: { prompt_tokens: 20, total_tokens: 30 },\\n228 | \\t\\t\\t\\t}\\n----\\n253 | \\t\\t\\t\\t\\t],\\n254 | \\t\\t\\t\\t\\tusage: { prompt_tokens: 20, total_tokens: 30 },\\n255 | \\t\\t\\t\\t}\\n----\\n270 | \\t\\t/**\\n271 | \\t\\t * Test batching logic when texts exceed token limits\\n272 | \\t\\t */\\n----\\n283 | \\t\\t\\t\\t\\t],\\n284 | \\t\\t\\t\\t\\tusage: { prompt_tokens: 10, total_tokens: 15 },\\n285 | \\t\\t\\t\\t})\\n----\\n299 | \\t\\t\\t\\t\\tdata: [{ embedding: [0.1, 0.2, 0.3] }, { embedding: [0.4, 0.5, 0.6] }],\\n300 | \\t\\t\\t\\t\\tusage: { prompt_tokens: 10, total_tokens: 15 },\\n301 | \\t\\t\\t\\t}\\n----\\n306 | \\t\\t\\t\\t// Should warn about oversized text\\n307 | \\t\\t\\t\\texpect(console.warn).toHaveBeenCalledWith(expect.stringContaining(\\\"exceeds maximum token limit\\\"))\\n308 | \\n----\\n317 | \\t\\t\\t\\t\\tdata: [{ embedding: [0.1, 0.2, 0.3] }, { embedding: [0.4, 0.5, 0.6] }],\\n318 | \\t\\t\\t\\t\\tusage: { prompt_tokens: 10, total_tokens: 15 },\\n319 | \\t\\t\\t\\t})\\n----\\n354 | \\t\\t\\t\\t\\t\\tdata: [{ embedding: base64String }],\\n355 | \\t\\t\\t\\t\\t\\tusage: { prompt_tokens: 10, total_tokens: 15 },\\n356 | \\t\\t\\t\\t\\t})\\n----\\n375 | \\t\\t\\t\\tconst testTexts = [\\\"Hello world\\\"]\\n376 | \\t\\t\\t\\tconst authError = new Error(\\\"Unauthorized\\\")\\n377 | \\t\\t\\t\\t;(authError as any).status = 401\\n378 | \\n379 | \\t\\t\\t\\tmockEmbeddingsCreate.mockRejectedValue(authError)\\n380 | \\n----\\n452 | \\t\\t\\t\\t\\tdata: null,\\n453 | \\t\\t\\t\\t\\tusage: { prompt_tokens: 10, total_tokens: 15 },\\n454 | \\t\\t\\t\\t}\\n----\\n494 | \\t\\t\\t\\t\\tusage: {\\n495 | \\t\\t\\t\\t\\t\\tprompt_tokens: 2,\\n496 | \\t\\t\\t\\t\\t\\ttotal_tokens: 2,\\n497 | \\t\\t\\t\\t\\t},\\n----\\n\\n# src/services/marketplace/schemas.ts\\n 35 | \\ttype: marketplaceItemTypeSchema,\\n 36 | \\tauthor: z.string().optional(),\\n 37 | \\tauthorUrl: z.string().url(\\\"Author URL must be a valid URL\\\").optional(),\\n 38 | \\ttags: z.array(z.string()).optional(),\\n----\\n 61 | \\tdescription: z.string(),\\n 62 | \\tauthor: z.string().optional(),\\n 63 | \\tauthorUrl: z.string().url().optional(),\\n 64 | \\ttags: z.array(z.string()).optional(),\\n----\\n 72 | \\tdescription: z.string(),\\n 73 | \\tauthor: z.string().optional(),\\n 74 | \\tauthorUrl: z.string().url().optional(),\\n 75 | \\turl: z.string().url(), // Required url field\\n----\\n\\n# src/services/tree-sitter/__tests__/fixtures/sample-embedded_template.ts\\n 35 | <div class=\\\"container\\\">\\n 36 |   <% if user.authenticated? %>\\n 37 |     <h1>Welcome, <%= user.name %></h1>\\n----\\n\\n# src/services/tree-sitter/__tests__/parseSourceCodeDefinitions.embedded_template.test.ts\\n 43 | \\tit(\\\"should detect control structures\\\", () => {\\n 44 | \\t\\texpect(parseResult).toMatch(/\\\\d+--\\\\d+ \\\\|\\\\s+<% if user\\\\.authenticated\\\\? %>/)\\n 45 | \\t\\texpect(parseResult).toMatch(/\\\\d+--\\\\d+ \\\\|\\\\s+<% user\\\\.posts\\\\.each do \\\\|post\\\\| %>/)\\n----\\n\\n# src/workers/countTokens.ts\\n  4 | \\n  5 | import { tiktoken } from \\\"../utils/tiktoken\\\"\\n  6 | \\n----\\n 10 | \\ttry {\\n 11 | \\t\\tconst count = await tiktoken(content)\\n 12 | \\t\\treturn { success: true, count }\\n----\\n\\n# src/activate/handleUri.ts\\n 37 | \\t\\t}\\n 38 | \\t\\tcase \\\"/auth/clerk/callback\\\": {\\n 39 | \\t\\t\\tconst code = query.get(\\\"code\\\")\\n----\\n\\n# src/__tests__/dist_assets.test.ts\\n  6 | \\n  7 | \\tdescribe(\\\"tiktoken\\\", () => {\\n  8 | \\t\\tit(\\\"should have tiktoken wasm file\\\", () => {\\n  9 | \\t\\t\\texpect(fs.existsSync(path.join(distPath, \\\"tiktoken_bg.wasm\\\"))).toBe(true)\\n 10 | \\t\\t})\\n----\\n\\n# src/integrations/terminal/ShellIntegrationManager.ts\\n 43 | \\t[ -f \\\"$ZDOTDIR/.zshrc\\\" ] && source \\\"$ZDOTDIR/.zshrc\\\"\\n 44 | \\t[ -f \\\"$ZDOTDIR/.zlogin\\\" ] && source \\\"$ZDOTDIR/.zlogin\\\"\\n 45 | \\t[ \\\"$ZDOTDIR\\\" = \\\"$HOME\\\" ] && unset ZDOTDIR\\n----\\n\\n# src/extension/api.ts\\n219 | \\n220 | \\t\\t\\tcline.on(\\\"taskCompleted\\\", async (_, tokenUsage, toolUsage) => {\\n221 | \\t\\t\\t\\tthis.emit(RooCodeEventName.TaskCompleted, cline.taskId, tokenUsage, toolUsage)\\n222 | \\t\\t\\t\\tthis.taskMap.delete(cline.taskId)\\n----\\n224 | \\t\\t\\t\\tawait this.fileLog(\\n225 | \\t\\t\\t\\t\\t`[${new Date().toISOString()}] taskCompleted -> ${cline.taskId} | ${JSON.stringify(tokenUsage, null, 2)} | ${JSON.stringify(toolUsage, null, 2)}\\\\n`,\\n226 | \\t\\t\\t\\t)\\n----\\n\\n# src/utils/countTokens.ts\\n  4 | import { countTokensResultSchema } from \\\"../workers/types\\\"\\n  5 | import { tiktoken } from \\\"./tiktoken\\\"\\n  6 | \\n----\\n 27 | \\tif (!useWorker || !pool) {\\n 28 | \\t\\treturn tiktoken(content)\\n 29 | \\t}\\n----\\n 42 | \\t\\tconsole.error(error)\\n 43 | \\t\\treturn tiktoken(content)\\n 44 | \\t}\\n----\\n\\n# src/core/checkpoints/index.ts\\n215 | \\t\\t\\t\\tJSON.stringify({\\n216 | \\t\\t\\t\\t\\ttokensIn: totalTokensIn,\\n217 | \\t\\t\\t\\t\\ttokensOut: totalTokensOut,\\n218 | \\t\\t\\t\\t\\tcacheWrites: totalCacheWrites,\\n----\\n\\n# src/core/condense/index.ts\\n 55 | \\tcost: number // The cost of the summarization operation\\n 56 | \\tnewContextTokens?: number // The number of tokens in the context for the next API request\\n 57 | \\terror?: string // Populated iff the operation fails: error message shown to the user on failure (see Task.ts)\\n----\\n 63 |  * @param {ApiMessage[]} messages - The conversation messages\\n 64 |  * @param {ApiHandler} apiHandler - The API handler to use for token counting.\\n 65 |  * @param {string} systemPrompt - The system prompt for API requests, which should be considered in the context token count\\n 66 |  * @param {string} taskId - The task ID for the conversation, used for telemetry\\n----\\n 73 |  * @param {ApiMessage[]} messages - The conversation messages\\n 74 |  * @param {ApiHandler} apiHandler - The API handler to use for token counting (fallback if condensingApiHandler not provided)\\n 75 |  * @param {string} systemPrompt - The system prompt for API requests (fallback if customCondensingPrompt not provided)\\n 76 |  * @param {string} taskId - The task ID for the conversation, used for telemetry\\n 77 |  * @param {number} prevContextTokens - The number of tokens currently in the context, used to ensure we don't grow the context\\n 78 |  * @param {boolean} isAutomaticTrigger - Whether the summarization is triggered automatically\\n----\\n186 | \\n187 | \\t// Count the tokens in the context for the next API request\\n188 | \\t// We only estimate the tokens in summaryMesage if outputTokens is 0, otherwise we use outputTokens\\n189 | \\tconst systemPromptMessage: ApiMessage = { role: \\\"user\\\", content: systemPrompt }\\n----\"}",
  "partial": false
}
[2025-06-13T04:59:46.001Z] {
  "ts": 1749790785975,
  "type": "say",
  "say": "api_req_started",
  "text": "{\"request\":\"[search_files for 'auth|oauth|token|login' in '*.ts'] Result:\\n\\nShowing first 300 of 300+ results. Use a more specific search if necessary.\\n\\n# src/services/ripgrep/index.ts\\n101 | \\t\\tconst rgProcess = childProcess.spawn(bin, args)\\n102 | \\t\\t// cross-platform alternative to head, which is ripgrep author's recommendation for limiting output.\\n103 | \\t\\tconst rl = readline.createInterface({\\n----\\n\\n# src/api/transform/model-params.ts\\n 95 | \\n 96 | \\t\\t// Reasoning cannot be less than 1024 tokens.\\n 97 | \\t\\tif (reasoningBudget < 1024) {\\n----\\n\\n# src/api/transform/__tests__/reasoning.spec.ts\\n 40 | \\n 41 | \\t\\t\\texpect(result).toEqual({ max_tokens: 1000 })\\n 42 | \\t\\t})\\n----\\n 61 | \\n 62 | \\t\\t\\texpect(result).toEqual({ max_tokens: 1000 })\\n 63 | \\t\\t})\\n----\\n123 | \\n124 | \\t\\t\\texpect(result).toEqual({ max_tokens: 1000 })\\n125 | \\t\\t})\\n----\\n140 | \\n141 | \\t\\t\\texpect(result).toEqual({ max_tokens: undefined })\\n142 | \\t\\t})\\n----\\n189 | \\n190 | \\t\\t\\texpect(result).toEqual({ max_tokens: 0 })\\n191 | \\t\\t})\\n----\\n244 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n245 | \\t\\t\\t\\tbudget_tokens: 1000,\\n246 | \\t\\t\\t})\\n----\\n268 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n269 | \\t\\t\\t\\tbudget_tokens: 1000,\\n270 | \\t\\t\\t})\\n----\\n314 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n315 | \\t\\t\\t\\tbudget_tokens: undefined,\\n316 | \\t\\t\\t})\\n----\\n334 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n335 | \\t\\t\\t\\tbudget_tokens: 0,\\n336 | \\t\\t\\t})\\n----\\n354 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n355 | \\t\\t\\t\\tbudget_tokens: 100000,\\n356 | \\t\\t\\t})\\n----\\n374 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n375 | \\t\\t\\t\\tbudget_tokens: 1000,\\n376 | \\t\\t\\t})\\n----\\n547 | \\n548 | \\t\\t\\texpect(openRouterResult).toEqual({ max_tokens: 1000 })\\n549 | \\t\\t\\texpect(anthropicResult).toEqual({ type: \\\"enabled\\\", budget_tokens: 1000 })\\n550 | \\t\\t\\texpect(openAiResult).toBeUndefined()\\n----\\n601 | \\t\\t\\t// Budget should take precedence for OpenRouter and Anthropic\\n602 | \\t\\t\\texpect(openRouterResult).toEqual({ max_tokens: 1000 })\\n603 | \\t\\t\\texpect(anthropicResult).toEqual({ type: \\\"enabled\\\", budget_tokens: 1000 })\\n604 | \\t\\t\\t// OpenAI should still use effort since it doesn't support budget\\n----\\n672 | \\t\\t\\t\\texpect(typeof result).toBe(\\\"object\\\")\\n673 | \\t\\t\\t\\texpect(\\\"max_tokens\\\" in result || \\\"effort\\\" in result || \\\"exclude\\\" in result).toBe(true)\\n674 | \\t\\t\\t}\\n----\\n688 | \\t\\t\\t\\texpect(result).toHaveProperty(\\\"type\\\", \\\"enabled\\\")\\n689 | \\t\\t\\t\\texpect(result).toHaveProperty(\\\"budget_tokens\\\")\\n690 | \\t\\t\\t}\\n----\\n\\n# src/api/transform/__tests__/stream.spec.ts\\n 33 | \\n 34 | \\t\\tit(\\\"should handle usage chunks without cache tokens\\\", () => {\\n 35 | \\t\\t\\tconst usageChunk: ApiStreamChunk = {\\n----\\n 57 | \\n 58 | \\t\\tit(\\\"should handle usage chunks with zero tokens\\\", () => {\\n 59 | \\t\\t\\tconst zeroUsageChunk: ApiStreamChunk = {\\n----\\n 69 | \\n 70 | \\t\\tit(\\\"should handle usage chunks with large token counts\\\", () => {\\n 71 | \\t\\t\\tconst largeUsageChunk: ApiStreamChunk = {\\n----\\n\\n# src/api/transform/__tests__/model-params.spec.ts\\n205 | \\t\\t\\t\\t\\ttype: \\\"enabled\\\",\\n206 | \\t\\t\\t\\t\\tbudget_tokens: 1600,\\n207 | \\t\\t\\t\\t},\\n----\\n226 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n227 | \\t\\t\\t\\tbudget_tokens: 1600,\\n228 | \\t\\t\\t})\\n----\\n262 | \\t\\t\\t\\t\\ttype: \\\"enabled\\\",\\n263 | \\t\\t\\t\\t\\tbudget_tokens: 2400,\\n264 | \\t\\t\\t\\t},\\n----\\n287 | \\t\\t\\t\\t\\ttype: \\\"enabled\\\",\\n288 | \\t\\t\\t\\t\\tbudget_tokens: 1500,\\n289 | \\t\\t\\t\\t},\\n----\\n308 | \\n309 | \\t\\tit(\\\"should clamp thinking budget to at least 1024 tokens\\\", () => {\\n310 | \\t\\t\\tconst model: ModelInfo = {\\n----\\n328 | \\t\\t\\t\\t\\ttype: \\\"enabled\\\",\\n329 | \\t\\t\\t\\t\\tbudget_tokens: 1024,\\n330 | \\t\\t\\t\\t},\\n----\\n333 | \\n334 | \\t\\tit(\\\"should clamp thinking budget to at most 80% of max tokens\\\", () => {\\n335 | \\t\\t\\tconst model: ModelInfo = {\\n----\\n353 | \\t\\t\\t\\t\\ttype: \\\"enabled\\\",\\n354 | \\t\\t\\t\\t\\tbudget_tokens: 3200,\\n355 | \\t\\t\\t\\t},\\n----\\n372 | \\t\\t\\t\\t\\ttype: \\\"enabled\\\",\\n373 | \\t\\t\\t\\t\\tbudget_tokens: DEFAULT_HYBRID_REASONING_MODEL_THINKING_TOKENS,\\n374 | \\t\\t\\t\\t},\\n----\\n390 | \\t\\t\\texpect(result.maxTokens).toBe(5000)\\n391 | \\t\\t\\texpect(result.reasoningBudget).toBe(2000) // Custom thinking tokens takes precedence\\n392 | \\t\\t})\\n393 | \\n394 | \\t\\tit(\\\"should clamp custom thinking tokens even when custom max tokens is provided\\\", () => {\\n395 | \\t\\t\\tconst model: ModelInfo = {\\n----\\n719 | \\n720 | \\t\\t\\texpect(result.reasoning).toEqual({ max_tokens: 3200 })\\n721 | \\t\\t})\\n----\\n\\n# src/api/transform/cache-strategy/__tests__/cache-strategy.spec.ts\\n 29 | \\n 30 | const createMessageWithTokens = (role: \\\"user\\\" | \\\"assistant\\\", tokenCount: number) => ({\\n 31 | \\trole,\\n 32 | \\tcontent: \\\"x\\\".repeat(tokenCount * 4), // Approximate 4 chars per token\\n 33 | })\\n----\\n130 | \\t\\t\\t\\tit(\\\"adds system cache block when prompt caching is enabled, messages exist, and system prompt is long enough\\\", () => {\\n131 | \\t\\t\\t\\t\\t// Create a system prompt that's at least 50 tokens (200+ characters)\\n132 | \\t\\t\\t\\t\\tconst longSystemPrompt =\\n----\\n236 | \\t\\t\\t\\t\\t\\t\\t\\t(i + 1) +\\n237 | \\t\\t\\t\\t\\t\\t\\t\\t\\\" with some additional text to increase token count. \\\" +\\n238 | \\t\\t\\t\\t\\t\\t\\t\\t\\\"Adding more text to ensure we exceed the token threshold for cache block insertion.\\\",\\n239 | \\t\\t\\t\\t\\t\\t}))\\n----\\n579 | \\n580 | \\t\\tit(\\\"should yield usage results with cache tokens when using MultiPointStrategy\\\", async () => {\\n581 | \\t\\t\\t// Mock the convertToBedrockConverseMessages method to return a result with cache points\\n----\\n589 | \\n590 | \\t\\t\\t// Create a mock stream that includes cache token fields\\n591 | \\t\\t\\tconst mockApiResponse = {\\n----\\n626 | \\n627 | \\t\\t\\t// Verify that usage results with cache tokens are yielded\\n628 | \\t\\t\\texpect(chunks.length).toBeGreaterThan(0)\\n629 | \\t\\t\\t// The test already expects cache tokens, but the implementation might not be including them\\n630 | \\t\\t\\t// Let's make the test more flexible to accept either format\\n----\\n652 | \\n653 | \\t\\t// Helper function to create a message with approximate token count\\n654 | \\t\\tconst createMessage = (role: \\\"user\\\" | \\\"assistant\\\", content: string, tokenCount: number) => {\\n655 | \\t\\t\\t// Pad the content to reach the desired token count (approx 4 chars per token)\\n656 | \\t\\t\\tconst paddingNeeded = Math.max(0, tokenCount * 4 - content.length)\\n657 | \\t\\t\\tconst padding = \\\" \\\".repeat(paddingNeeded)\\n----\\n667 | \\t\\t\\t\\t\\\"Cache point placements:\\\",\\n668 | \\t\\t\\t\\tplacements.map((p) => `index: ${p.index}, tokens: ${p.tokensCovered}`),\\n669 | \\t\\t\\t)\\n----\\n683 | \\t\\t\\t\\t\\tmodelInfo: multiPointModelInfo,\\n684 | \\t\\t\\t\\t\\tsystemPrompt: \\\"You are a helpful assistant.\\\", // ~10 tokens\\n685 | \\t\\t\\t\\t\\tmessages,\\n----\\n730 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n731 | \\t\\t\\t\\t\\t\\ttokensCovered: 300,\\n732 | \\t\\t\\t\\t\\t},\\n----\\n736 | \\t\\t\\t\\t\\tmodelInfo: multiPointModelInfo,\\n737 | \\t\\t\\t\\t\\tsystemPrompt: \\\"You are a helpful assistant.\\\", // ~10 tokens\\n738 | \\t\\t\\t\\t\\tmessages,\\n----\\n759 | \\n760 | \\t\\t\\t\\t// Check if we have a second cache point (may not always be added depending on token distribution)\\n761 | \\t\\t\\t\\tif (result.messageCachePointPlacements && result.messageCachePointPlacements.length > 1) {\\n----\\n789 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n790 | \\t\\t\\t\\t\\t\\ttokensCovered: 300,\\n791 | \\t\\t\\t\\t\\t},\\n----\\n794 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n795 | \\t\\t\\t\\t\\t\\ttokensCovered: 300,\\n796 | \\t\\t\\t\\t\\t},\\n----\\n800 | \\t\\t\\t\\t\\tmodelInfo: multiPointModelInfo,\\n801 | \\t\\t\\t\\t\\tsystemPrompt: \\\"You are a helpful assistant.\\\", // ~10 tokens\\n802 | \\t\\t\\t\\t\\tmessages,\\n----\\n863 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n864 | \\t\\t\\t\\t\\t\\ttokensCovered: 300,\\n865 | \\t\\t\\t\\t\\t},\\n----\\n868 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n869 | \\t\\t\\t\\t\\t\\ttokensCovered: 300,\\n870 | \\t\\t\\t\\t\\t},\\n----\\n873 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n874 | \\t\\t\\t\\t\\t\\ttokensCovered: 300,\\n875 | \\t\\t\\t\\t\\t},\\n----\\n879 | \\t\\t\\t\\t\\tmodelInfo: multiPointModelInfo,\\n880 | \\t\\t\\t\\t\\tsystemPrompt: \\\"You are a helpful assistant.\\\", // ~10 tokens\\n881 | \\t\\t\\t\\t\\tmessages,\\n----\\n929 | \\t\\t\\t\\t// This test verifies that the documentation for Example 5 is correct\\n930 | \\t\\t\\t\\t// In Example 5, the third cache point at index 10 should cover 660 tokens\\n931 | \\t\\t\\t\\t// (260 tokens from messages 7-8 plus 400 tokens from the new messages)\\n932 | \\n----\\n944 | \\t\\t\\t\\t\\tcreateMessage(\\\"assistant\\\", \\\"Deep learning has many applications including...\\\", 160),\\n945 | \\t\\t\\t\\t\\t// New messages with 400 tokens total\\n946 | \\t\\t\\t\\t\\tcreateMessage(\\\"user\\\", \\\"Can you provide a detailed example?\\\", 100),\\n----\\n954 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n955 | \\t\\t\\t\\t\\t\\ttokensCovered: 240,\\n956 | \\t\\t\\t\\t\\t},\\n----\\n959 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n960 | \\t\\t\\t\\t\\t\\ttokensCovered: 440,\\n961 | \\t\\t\\t\\t\\t},\\n----\\n964 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n965 | \\t\\t\\t\\t\\t\\ttokensCovered: 260,\\n966 | \\t\\t\\t\\t\\t},\\n----\\n969 | \\t\\t\\t\\t// In the documentation, the algorithm decides to replace the cache point at index 8\\n970 | \\t\\t\\t\\t// with a new one at index 10, and the tokensCovered value should be 660 tokens\\n971 | \\t\\t\\t\\t// (260 tokens from messages 7-8 plus 400 tokens from the new messages)\\n972 | \\n973 | \\t\\t\\t\\t// However, the actual implementation may behave differently depending on how\\n974 | \\t\\t\\t\\t// it calculates token counts and makes decisions about cache point placement\\n975 | \\n976 | \\t\\t\\t\\t// The important part is that our fix ensures that when a cache point is created,\\n977 | \\t\\t\\t\\t// the tokensCovered value represents all tokens from the previous cache point\\n978 | \\t\\t\\t\\t// to the current cache point, not just the tokens in the new messages\\n979 | \\t\\t\\t})\\n980 | \\n981 | \\t\\t\\tit(\\\"should not combine cache points when new messages have fewer tokens than the smallest combined gap\\\", () => {\\n982 | \\t\\t\\t\\t// This test verifies that when new messages have fewer tokens than the smallest combined gap,\\n983 | \\t\\t\\t\\t// the algorithm keeps all existing cache points and doesn't add a new one\\n----\\n1004 | \\t\\t\\t\\t\\t\\t),\\n1005 | \\t\\t\\t\\t\\t\\t// Small addition (only 50 tokens total)\\n1006 | \\t\\t\\t\\t\\t\\tcreateMessage(\\\"user\\\", \\\"Thanks for the explanation.\\\", 20),\\n----\\n1009 | \\n1010 | \\t\\t\\t\\t\\t// Previous cache point placements with significant token coverage\\n1011 | \\t\\t\\t\\t\\tconst previousCachePointPlacements: CachePointPlacement[] = [\\n----\\n1014 | \\t\\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n1015 | \\t\\t\\t\\t\\t\\t\\ttokensCovered: 400, // Significant token coverage\\n1016 | \\t\\t\\t\\t\\t\\t},\\n----\\n1019 | \\t\\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n1020 | \\t\\t\\t\\t\\t\\t\\ttokensCovered: 300, // Significant token coverage\\n1021 | \\t\\t\\t\\t\\t\\t},\\n----\\n1024 | \\t\\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n1025 | \\t\\t\\t\\t\\t\\t\\ttokensCovered: 300, // Significant token coverage\\n1026 | \\t\\t\\t\\t\\t\\t},\\n----\\n1030 | \\t\\t\\t\\t\\t\\tmodelInfo: multiPointModelInfo,\\n1031 | \\t\\t\\t\\t\\t\\tsystemPrompt: \\\"You are a helpful assistant.\\\", // ~10 tokens\\n1032 | \\t\\t\\t\\t\\t\\tmessages,\\n----\\n1057 | \\n1058 | \\t\\t\\tit(\\\"should make correct decisions based on token counts\\\", () => {\\n1059 | \\t\\t\\t\\t// This test verifies that the algorithm correctly compares token counts\\n1060 | \\t\\t\\t\\t// and makes the right decision about combining cache points\\n1061 | \\n1062 | \\t\\t\\t\\t// Create messages with a variety of token counts\\n1063 | \\t\\t\\t\\tconst messages = [\\n----\\n1081 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n1082 | \\t\\t\\t\\t\\t\\ttokensCovered: 400,\\n1083 | \\t\\t\\t\\t\\t},\\n----\\n1086 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n1087 | \\t\\t\\t\\t\\t\\ttokensCovered: 150,\\n1088 | \\t\\t\\t\\t\\t},\\n----\\n1091 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n1092 | \\t\\t\\t\\t\\t\\ttokensCovered: 150,\\n1093 | \\t\\t\\t\\t\\t},\\n----\\n\\n# src/api/transform/cache-strategy/types.ts\\n  7 | export interface ModelInfo {\\n  8 | \\t/** Maximum number of tokens the model can generate */\\n  9 | \\tmaxTokens: number\\n 10 | \\t/** Maximum context window size in tokens */\\n 11 | \\tcontextWindow: number\\n----\\n 15 | \\tmaxCachePoints: number\\n 16 | \\t/** Minimum number of tokens required for a cache point */\\n 17 | \\tminTokensPerCachePoint: number\\n----\\n 49 | \\ttype: \\\"system\\\" | \\\"message\\\"\\n 50 | \\t/** Number of tokens this cache point covers */\\n 51 | \\ttokensCovered: number\\n 52 | }\\n----\\n\\n# src/api/transform/cache-strategy/base-strategy.ts\\n 27 | \\t/**\\n 28 | \\t * Calculate token count for system prompt using a more accurate approach\\n 29 | \\t */\\n----\\n 33 | \\n 34 | \\t\\t\\t// Use a more accurate token estimation than simple character count\\n 35 | \\t\\t\\t// Count words and add overhead for punctuation and special tokens\\n 36 | \\t\\t\\tconst words = text.split(/\\\\s+/).filter((word) => word.length > 0)\\n 37 | \\t\\t\\t// Average English word is ~1.3 tokens\\n 38 | \\t\\t\\tlet tokenCount = words.length * 1.3\\n 39 | \\t\\t\\t// Add overhead for punctuation and special characters\\n 40 | \\t\\t\\ttokenCount += (text.match(/[.,!?;:()[\\\\]{}\\\"\\\"''`]/g) || []).length * 0.3\\n 41 | \\t\\t\\t// Add overhead for newlines\\n 42 | \\t\\t\\ttokenCount += (text.match(/\\\\n/g) || []).length * 0.5\\n 43 | \\t\\t\\t// Add a small overhead for system prompt structure\\n 44 | \\t\\t\\ttokenCount += 5\\n 45 | \\n 46 | \\t\\t\\tthis.systemTokenCount = Math.ceil(tokenCount)\\n 47 | \\t\\t}\\n----\\n 84 | \\t/**\\n 85 | \\t * Check if a token count meets the minimum threshold for caching\\n 86 | \\t */\\n 87 | \\tprotected meetsMinTokenThreshold(tokenCount: number): boolean {\\n 88 | \\t\\tconst minTokens = this.config.modelInfo.minTokensPerCachePoint\\n----\\n 91 | \\t\\t}\\n 92 | \\t\\treturn tokenCount >= minTokens\\n 93 | \\t}\\n----\\n 95 | \\t/**\\n 96 | \\t * Estimate token count for a message using a more accurate approach\\n 97 | \\t * This implementation is based on the BaseProvider's countTokens method\\n----\\n100 | \\tprotected estimateTokenCount(message: Anthropic.Messages.MessageParam): number {\\n101 | \\t\\t// Use a more sophisticated token counting approach\\n102 | \\t\\tif (!message.content) return 0\\n----\\n108 | \\t\\t\\t\\tif (block.type === \\\"text\\\") {\\n109 | \\t\\t\\t\\t\\t// Use a more accurate token estimation than simple character count\\n110 | \\t\\t\\t\\t\\t// This is still an approximation but better than character/4\\n----\\n112 | \\t\\t\\t\\t\\tif (text.length > 0) {\\n113 | \\t\\t\\t\\t\\t\\t// Count words and add overhead for punctuation and special tokens\\n114 | \\t\\t\\t\\t\\t\\tconst words = text.split(/\\\\s+/).filter((word) => word.length > 0)\\n115 | \\t\\t\\t\\t\\t\\t// Average English word is ~1.3 tokens\\n116 | \\t\\t\\t\\t\\t\\ttotalTokens += words.length * 1.3\\n----\\n128 | \\t\\t\\tconst text = message.content\\n129 | \\t\\t\\t// Count words and add overhead for punctuation and special tokens\\n130 | \\t\\t\\tconst words = text.split(/\\\\s+/).filter((word) => word.length > 0)\\n131 | \\t\\t\\t// Average English word is ~1.3 tokens\\n132 | \\t\\t\\ttotalTokens += words.length * 1.3\\n----\\n\\n# src/api/transform/reasoning.ts\\n 12 | \\teffort?: ReasoningEffort\\n 13 | \\tmax_tokens?: number\\n 14 | \\texclude?: boolean\\n----\\n 36 | \\tshouldUseReasoningBudget({ model, settings })\\n 37 | \\t\\t? { max_tokens: reasoningBudget }\\n 38 | \\t\\t: shouldUseReasoningEffort({ model, settings })\\n----\\n 46 | }: GetModelReasoningOptions): AnthropicReasoningParams | undefined =>\\n 47 | \\tshouldUseReasoningBudget({ model, settings }) ? { type: \\\"enabled\\\", budget_tokens: reasoningBudget! } : undefined\\n 48 | \\n----\\n\\n# src/api/transform/cache-strategy/multi-point-strategy.ts\\n  7 |  * Strategy for handling multiple cache points.\\n  8 |  * Creates cache points after messages as soon as uncached tokens exceed minimumTokenCount.\\n  9 |  */\\n----\\n 58 | \\t *\\n 59 | \\t * @param minTokensPerPoint Minimum tokens required per cache point\\n 60 | \\t * @param remainingCachePoints Number of cache points available\\n----\\n 97 | \\n 98 | \\t\\t// Calculate tokens in new messages (added since last cache point placement)\\n 99 | \\t\\tconst lastPreviousIndex = previousPlacements[previousPlacements.length - 1].index\\n----\\n103 | \\n104 | \\t\\t// If new messages have enough tokens for a cache point, we need to decide\\n105 | \\t\\t// whether to keep all previous cache points or combine some\\n----\\n127 | \\t\\t\\t\\t// We need to decide which previous cache points to keep and which to combine\\n128 | \\t\\t\\t\\t// Strategy: Compare the token count of new messages with the smallest combined token gap\\n129 | \\n130 | \\t\\t\\t\\t// First, analyze the token distribution between previous cache points\\n131 | \\t\\t\\t\\tconst tokensBetweenPlacements: number[] = []\\n132 | \\t\\t\\t\\tlet startIdx = 0\\n----\\n134 | \\t\\t\\t\\tfor (const placement of previousPlacements) {\\n135 | \\t\\t\\t\\t\\tconst tokens = this.config.messages\\n136 | \\t\\t\\t\\t\\t\\t.slice(startIdx, placement.index + 1)\\n----\\n138 | \\n139 | \\t\\t\\t\\t\\ttokensBetweenPlacements.push(tokens)\\n140 | \\t\\t\\t\\t\\tstartIdx = placement.index + 1\\n----\\n142 | \\n143 | \\t\\t\\t\\t// Find the two consecutive placements with the smallest token gap\\n144 | \\t\\t\\t\\tlet smallestGapIndex = 0\\n----\\n146 | \\n147 | \\t\\t\\t\\tfor (let i = 0; i < tokensBetweenPlacements.length - 1; i++) {\\n148 | \\t\\t\\t\\t\\tconst gap = tokensBetweenPlacements[i] + tokensBetweenPlacements[i + 1]\\n149 | \\t\\t\\t\\t\\tif (gap < smallestGap) {\\n----\\n155 | \\t\\t\\t\\t// Only combine cache points if it's beneficial\\n156 | \\t\\t\\t\\t// Compare the token count of new messages with the smallest combined token gap\\n157 | \\t\\t\\t\\t// Apply a required percentage increase to ensure reallocation is worth it\\n----\\n161 | \\t\\t\\t\\tif (newMessagesTokens >= requiredTokenThreshold) {\\n162 | \\t\\t\\t\\t\\t// It's beneficial to combine cache points since new messages have significantly more tokens\\n163 | \\t\\t\\t\\t\\tlogger.info(\\\"Combining cache points is beneficial\\\", {\\n----\\n231 | \\t\\t} else {\\n232 | \\t\\t\\t// New messages don't have enough tokens for a cache point\\n233 | \\t\\t\\t// Keep all previous placements that are still valid\\n----\\n266 | \\t\\tif (lastUserMessageIndex >= 0) {\\n267 | \\t\\t\\t// Calculate the total tokens covered from the previous cache point (or start of conversation)\\n268 | \\t\\t\\t// to this cache point. This ensures tokensCovered represents the full span of tokens\\n269 | \\t\\t\\t// that will be cached by this cache point.\\n----\\n281 | \\n282 | \\t\\t\\t// Calculate tokens from previous cache point (or start) to this cache point\\n283 | \\t\\t\\tconst tokenStartIndex = previousCachePointIndex + 1\\n284 | \\t\\t\\ttotalTokensCovered = this.config.messages\\n285 | \\t\\t\\t\\t.slice(tokenStartIndex, lastUserMessageIndex + 1)\\n286 | \\t\\t\\t\\t.reduce((acc, curr) => acc + this.estimateTokenCount(curr), 0)\\n287 | \\n288 | \\t\\t\\t// Guard clause: ensure we have enough tokens to justify a cache point\\n289 | \\t\\t\\tif (totalTokensCovered < minTokensPerPoint) {\\n----\\n294 | \\t\\t\\t\\ttype: \\\"message\\\",\\n295 | \\t\\t\\t\\ttokensCovered: totalTokensCovered,\\n296 | \\t\\t\\t}\\n----\\n\\n# src/api/index.ts\\n 50 | \\t/**\\n 51 | \\t * Counts tokens for content blocks\\n 52 | \\t * All providers extend BaseProvider which provides a default tiktoken implementation,\\n 53 | \\t * but they can override this to use their native token counting endpoints\\n 54 | \\t *\\n 55 | \\t * @param content The content to count tokens for\\n 56 | \\t * @returns A promise resolving to the token count\\n 57 | \\t */\\n----\\n\\n# src/api/providers/gemini.ts\\n  7 | } from \\\"@google/genai\\\"\\n  8 | import type { JWTInput } from \\\"google-auth-library\\\"\\n  9 | \\n----\\n179 | \\t\\t\\tif (response.totalTokens === undefined) {\\n180 | \\t\\t\\t\\tconsole.warn(\\\"Gemini token counting returned undefined, using fallback\\\")\\n181 | \\t\\t\\t\\treturn super.countTokens(content)\\n----\\n185 | \\t\\t} catch (error) {\\n186 | \\t\\t\\tconsole.warn(\\\"Gemini token counting failed, using fallback\\\", error)\\n187 | \\t\\t\\treturn super.countTokens(content)\\n----\\n209 | \\n210 | \\t\\t// If there's tiered pricing then adjust the input and output token prices\\n211 | \\t\\t// based on the input tokens used.\\n212 | \\t\\tif (info.tiers) {\\n----\\n221 | \\n222 | \\t\\t// Subtract the cached input tokens from the total input tokens.\\n223 | \\t\\tconst uncachedInputTokens = inputTokens - cacheReadTokens\\n----\\n230 | \\n231 | \\t\\tconst trace: Record<string, { price: number; tokens: number; cost: number }> = {\\n232 | \\t\\t\\tinput: { price: inputPrice, tokens: uncachedInputTokens, cost: inputTokensCost },\\n233 | \\t\\t\\toutput: { price: outputPrice, tokens: outputTokens, cost: outputTokensCost },\\n234 | \\t\\t}\\n----\\n236 | \\t\\tif (cacheReadTokens > 0) {\\n237 | \\t\\t\\ttrace.cacheRead = { price: cacheReadsPrice, tokens: cacheReadTokens, cost: cacheReadCost }\\n238 | \\t\\t}\\n----\\n\\n# src/api/providers/unbound.ts\\n 22 | interface UnboundUsage extends OpenAI.CompletionUsage {\\n 23 | \\tcache_creation_input_tokens?: number\\n 24 | \\tcache_read_input_tokens?: number\\n 25 | }\\n----\\n 73 | \\n 74 | \\t\\t// Required by Anthropic; other providers default to max tokens allowed.\\n 75 | \\t\\tlet maxTokens: number | undefined\\n----\\n 82 | \\t\\t\\tmodel: modelId.split(\\\"/\\\")[1],\\n 83 | \\t\\t\\tmax_tokens: maxTokens,\\n 84 | \\t\\t\\tmessages: openAiMessages,\\n----\\n111 | \\t\\t\\t\\t\\ttype: \\\"usage\\\",\\n112 | \\t\\t\\t\\t\\tinputTokens: usage.prompt_tokens || 0,\\n113 | \\t\\t\\t\\t\\toutputTokens: usage.completion_tokens || 0,\\n114 | \\t\\t\\t\\t}\\n115 | \\n116 | \\t\\t\\t\\t// Only add cache tokens if they exist.\\n117 | \\t\\t\\t\\tif (usage.cache_creation_input_tokens) {\\n118 | \\t\\t\\t\\t\\tusageData.cacheWriteTokens = usage.cache_creation_input_tokens\\n119 | \\t\\t\\t\\t}\\n120 | \\n121 | \\t\\t\\t\\tif (usage.cache_read_input_tokens) {\\n122 | \\t\\t\\t\\t\\tusageData.cacheReadTokens = usage.cache_read_input_tokens\\n123 | \\t\\t\\t\\t}\\n----\\n146 | \\t\\t\\tif (modelId.startsWith(\\\"anthropic/\\\")) {\\n147 | \\t\\t\\t\\trequestOptions.max_tokens = info.maxTokens\\n148 | \\t\\t\\t}\\n----\\n\\n# src/api/providers/lm-studio.ts\\n 40 | \\t\\t// -------------------------\\n 41 | \\t\\t// Track token usage\\n 42 | \\t\\t// -------------------------\\n----\\n 68 | \\t\\t} catch (err) {\\n 69 | \\t\\t\\tconsole.error(\\\"[LmStudio] Failed to count input tokens:\\\", err)\\n 70 | \\t\\t\\tinputTokens = 0\\n----\\n116 | \\t\\t\\t} catch (err) {\\n117 | \\t\\t\\t\\tconsole.error(\\\"[LmStudio] Failed to count output tokens:\\\", err)\\n118 | \\t\\t\\t\\toutputTokens = 0\\n----\\n\\n# src/api/providers/__tests__/openai.spec.ts\\n 29 | \\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 30 | \\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 31 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 32 | \\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 33 | \\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 55 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 56 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 57 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 58 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 59 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n\\n# src/api/providers/vscode-lm.ts\\n128 | \\t\\t\\t\\tmaxInputTokens: 8192,\\n129 | \\t\\t\\t\\tsendRequest: async (_messages, _options, _token) => {\\n130 | \\t\\t\\t\\t\\t// Provide a minimal implementation\\n----\\n179 | \\t * Implements the ApiHandler countTokens interface method\\n180 | \\t * Provides token counting for Anthropic content blocks\\n181 | \\t *\\n182 | \\t * @param content The content blocks to count tokens for\\n183 | \\t * @returns A promise resolving to the token count\\n184 | \\t */\\n185 | \\toverride async countTokens(content: Array<Anthropic.Messages.ContentBlockParam>): Promise<number> {\\n186 | \\t\\t// Convert Anthropic content blocks to a string for VSCode LM token counting\\n187 | \\t\\tlet textContent = \\\"\\\"\\n----\\n201 | \\t/**\\n202 | \\t * Private implementation of token counting used internally by VsCodeLmHandler\\n203 | \\t */\\n----\\n206 | \\t\\tif (!this.client) {\\n207 | \\t\\t\\tconsole.warn(\\\"Roo Code <Language Model API>: No client available for token counting\\\")\\n208 | \\t\\t\\treturn 0\\n----\\n211 | \\t\\tif (!this.currentRequestCancellation) {\\n212 | \\t\\t\\tconsole.warn(\\\"Roo Code <Language Model API>: No cancellation token available for token counting\\\")\\n213 | \\t\\t\\treturn 0\\n----\\n217 | \\t\\tif (!text) {\\n218 | \\t\\t\\tconsole.debug(\\\"Roo Code <Language Model API>: Empty text provided for token counting\\\")\\n219 | \\t\\t\\treturn 0\\n----\\n223 | \\t\\t\\t// Handle different input types\\n224 | \\t\\t\\tlet tokenCount: number\\n225 | \\n226 | \\t\\t\\tif (typeof text === \\\"string\\\") {\\n227 | \\t\\t\\t\\ttokenCount = await this.client.countTokens(text, this.currentRequestCancellation.token)\\n228 | \\t\\t\\t} else if (text instanceof vscode.LanguageModelChatMessage) {\\n----\\n233 | \\t\\t\\t\\t}\\n234 | \\t\\t\\t\\ttokenCount = await this.client.countTokens(text, this.currentRequestCancellation.token)\\n235 | \\t\\t\\t} else {\\n236 | \\t\\t\\t\\tconsole.warn(\\\"Roo Code <Language Model API>: Invalid input type for token counting\\\")\\n237 | \\t\\t\\t\\treturn 0\\n----\\n240 | \\t\\t\\t// Validate the result\\n241 | \\t\\t\\tif (typeof tokenCount !== \\\"number\\\") {\\n242 | \\t\\t\\t\\tconsole.warn(\\\"Roo Code <Language Model API>: Non-numeric token count received:\\\", tokenCount)\\n243 | \\t\\t\\t\\treturn 0\\n----\\n245 | \\n246 | \\t\\t\\tif (tokenCount < 0) {\\n247 | \\t\\t\\t\\tconsole.warn(\\\"Roo Code <Language Model API>: Negative token count received:\\\", tokenCount)\\n248 | \\t\\t\\t\\treturn 0\\n----\\n250 | \\n251 | \\t\\t\\treturn tokenCount\\n252 | \\t\\t} catch (error) {\\n----\\n278 | \\n279 | \\t\\treturn systemTokens + messageTokens.reduce((sum: number, tokens: number): number => sum + tokens, 0)\\n280 | \\t}\\n----\\n357 | \\n358 | \\t\\t// Initialize cancellation token for the request\\n359 | \\t\\tthis.currentRequestCancellation = new vscode.CancellationTokenSource()\\n360 | \\n361 | \\t\\t// Calculate input tokens before starting the stream\\n362 | \\t\\tconst totalInputTokens: number = await this.calculateTotalInputTokens(systemPrompt, vsCodeLmMessages)\\n363 | \\n364 | \\t\\t// Accumulate the text and count at the end of the stream to reduce token counting overhead.\\n365 | \\t\\tlet accumulatedText: string = \\\"\\\"\\n----\\n378 | \\t\\t\\t\\trequestOptions,\\n379 | \\t\\t\\t\\tthis.currentRequestCancellation.token,\\n380 | \\t\\t\\t)\\n----\\n446 | \\n447 | \\t\\t\\t// Count tokens in the accumulated text after stream completion\\n448 | \\t\\t\\tconst totalOutputTokens: number = await this.internalCountTokens(accumulatedText)\\n----\\n511 | \\t\\t\\tconst modelInfo: ModelInfo = {\\n512 | \\t\\t\\t\\tmaxTokens: -1, // Unlimited tokens by default\\n513 | \\t\\t\\t\\tcontextWindow:\\n----\\n548 | \\t\\t\\t\\t{},\\n549 | \\t\\t\\t\\tnew vscode.CancellationTokenSource().token,\\n550 | \\t\\t\\t)\\n----\\n\\n# src/api/providers/glama.ts\\n 51 | \\n 52 | \\t\\t// Required by Anthropic; other providers default to max tokens allowed.\\n 53 | \\t\\tlet maxTokens: number | undefined\\n----\\n 60 | \\t\\t\\tmodel: modelId,\\n 61 | \\t\\t\\tmax_tokens: maxTokens,\\n 62 | \\t\\t\\tmessages: openAiMessages,\\n----\\n 90 | \\t\\t\\t\\t// In case of an interrupted request, we need to wait for the upstream API to finish processing the request\\n 91 | \\t\\t\\t\\t// before we can fetch information about the token usage and cost.\\n 92 | \\t\\t\\t\\tconst response = await axios.get(\\n----\\n 98 | \\n 99 | \\t\\t\\t\\tif (completionRequest.tokenUsage && completionRequest.totalCostUsd) {\\n100 | \\t\\t\\t\\t\\tyield {\\n101 | \\t\\t\\t\\t\\t\\ttype: \\\"usage\\\",\\n102 | \\t\\t\\t\\t\\t\\tcacheWriteTokens: completionRequest.tokenUsage.cacheCreationInputTokens,\\n103 | \\t\\t\\t\\t\\t\\tcacheReadTokens: completionRequest.tokenUsage.cacheReadInputTokens,\\n104 | \\t\\t\\t\\t\\t\\tinputTokens: completionRequest.tokenUsage.promptTokens,\\n105 | \\t\\t\\t\\t\\t\\toutputTokens: completionRequest.tokenUsage.completionTokens,\\n106 | \\t\\t\\t\\t\\t\\ttotalCost: parseFloat(completionRequest.totalCostUsd),\\n----\\n132 | \\t\\t\\tif (modelId.startsWith(\\\"anthropic/\\\")) {\\n133 | \\t\\t\\t\\trequestOptions.max_tokens = info.maxTokens\\n134 | \\t\\t\\t}\\n----\\n\\n# src/api/providers/chutes.ts\\n 31 | \\t\\t\\tid: model,\\n 32 | \\t\\t\\tinfo: { maxTokens: max_tokens },\\n 33 | \\t\\t} = this.getModel()\\n----\\n 38 | \\t\\t\\tmodel,\\n 39 | \\t\\t\\tmax_tokens,\\n 40 | \\t\\t\\ttemperature,\\n----\\n 76 | \\t\\t\\t\\t\\t\\ttype: \\\"usage\\\",\\n 77 | \\t\\t\\t\\t\\t\\tinputTokens: chunk.usage.prompt_tokens || 0,\\n 78 | \\t\\t\\t\\t\\t\\toutputTokens: chunk.usage.completion_tokens || 0,\\n 79 | \\t\\t\\t\\t\\t}\\n----\\n\\n# src/api/providers/__tests__/vscode-lm.test.ts\\n 28 | \\t\\tCancellationTokenSource: jest.fn(() => ({\\n 29 | \\t\\t\\ttoken: {\\n 30 | \\t\\t\\t\\tisCancellationRequested: false,\\n----\\n\\n# src/api/providers/__tests__/vertex.spec.ts\\n 49 | \\t\\t\\t// The test expects 4 chunks:\\n 50 | \\t\\t\\t// 1. Usage chunk with input tokens\\n 51 | \\t\\t\\t// 2. Text chunk with \\\"Gemini response part 1\\\"\\n 52 | \\t\\t\\t// 3. Text chunk with \\\" part 2\\\"\\n 53 | \\t\\t\\t// 4. Usage chunk with output tokens\\n 54 | \\n----\\n\\n# src/api/providers/__tests__/gemini.spec.ts\\n167 | \\tdescribe(\\\"calculateCost\\\", () => {\\n168 | \\t\\t// Mock ModelInfo based on gemini-1.5-flash-latest pricing (per 1M tokens)\\n169 | \\t\\t// Removed 'id' and 'name' as they are not part of ModelInfo type directly\\n170 | \\t\\tconst mockInfo: ModelInfo = {\\n171 | \\t\\t\\tinputPrice: 0.125, // $/1M tokens\\n172 | \\t\\t\\toutputPrice: 0.375, // $/1M tokens\\n173 | \\t\\t\\tcacheWritesPrice: 0.125, // Assume same as input for test\\n----\\n179 | \\n180 | \\t\\tit(\\\"should calculate cost correctly based on input and output tokens\\\", () => {\\n181 | \\t\\t\\tconst inputTokens = 10000 // Use larger numbers for per-million pricing\\n----\\n190 | \\n191 | \\t\\tit(\\\"should return 0 if token counts are zero\\\", () => {\\n192 | \\t\\t\\t// Note: The method expects numbers, not undefined. Passing undefined would be a type error.\\n193 | \\t\\t\\t// The calculateCost method itself returns undefined if prices are missing, but 0 if tokens are 0 and prices exist.\\n194 | \\t\\t\\texpect(handler.calculateCost({ info: mockInfo, inputTokens: 0, outputTokens: 0 })).toBe(0)\\n----\\n196 | \\n197 | \\t\\tit(\\\"should handle only input tokens\\\", () => {\\n198 | \\t\\t\\tconst inputTokens = 5000\\n----\\n203 | \\n204 | \\t\\tit(\\\"should handle only output tokens\\\", () => {\\n205 | \\t\\t\\tconst outputTokens = 15000\\n----\\n210 | \\n211 | \\t\\tit(\\\"should calculate cost with cache write tokens\\\", () => {\\n212 | \\t\\t\\tconst inputTokens = 10000\\n----\\n227 | \\n228 | \\t\\tit(\\\"should calculate cost with cache read tokens\\\", () => {\\n229 | \\t\\t\\tconst inputTokens = 10000 // Total logical input\\n----\\n\\n# src/api/providers/__tests__/openrouter.spec.ts\\n132 | \\t\\t\\t\\t\\t\\tchoices: [{ delta: {} }],\\n133 | \\t\\t\\t\\t\\t\\tusage: { prompt_tokens: 10, completion_tokens: 20, cost: 0.001 },\\n134 | \\t\\t\\t\\t\\t}\\n----\\n162 | \\t\\t\\t\\texpect.objectContaining({\\n163 | \\t\\t\\t\\t\\tmax_tokens: 8192,\\n164 | \\t\\t\\t\\t\\tmessages: [\\n----\\n285 | \\t\\t\\t\\tmodel: mockOptions.openRouterModelId,\\n286 | \\t\\t\\t\\tmax_tokens: 8192,\\n287 | \\t\\t\\t\\tthinking: undefined,\\n----\\n\\n# src/api/providers/__tests__/ollama.spec.ts\\n 28 | \\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 29 | \\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 30 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 31 | \\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 32 | \\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 54 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 55 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 56 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 57 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 58 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n\\n# src/api/providers/__tests__/chutes.spec.ts\\n 49 | \\t\\t\\t\\t\\tusage: {\\n 50 | \\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 51 | \\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 52 | \\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 53 | \\t\\t\\t\\t\\t},\\n----\\n103 | \\t\\t\\t\\t\\t],\\n104 | \\t\\t\\t\\t\\tusage: { prompt_tokens: 10, completion_tokens: 5 },\\n105 | \\t\\t\\t\\t}\\n----\\n211 | \\t\\t\\t\\t\\t\\t\\tdone: false,\\n212 | \\t\\t\\t\\t\\t\\t\\tvalue: { choices: [{ delta: {} }], usage: { prompt_tokens: 10, completion_tokens: 20 } },\\n213 | \\t\\t\\t\\t\\t\\t})\\n----\\n284 | \\t\\t\\t\\tmodel: modelId,\\n285 | \\t\\t\\t\\tmax_tokens: modelInfo.maxTokens,\\n286 | \\t\\t\\t\\ttemperature: 0.5,\\n----\\n\\n# src/api/providers/openrouter.ts\\n 32 | \\tinclude_reasoning?: boolean\\n 33 | \\t// https://openrouter.ai/docs/use-cases/reasoning-tokens\\n 34 | \\treasoning?: OpenRouterReasoningParams\\n----\\n 40 | interface CompletionUsage {\\n 41 | \\tcompletion_tokens?: number\\n 42 | \\tcompletion_tokens_details?: {\\n 43 | \\t\\treasoning_tokens?: number\\n 44 | \\t}\\n 45 | \\tprompt_tokens?: number\\n 46 | \\tprompt_tokens_details?: {\\n 47 | \\t\\tcached_tokens?: number\\n 48 | \\t}\\n 49 | \\ttotal_tokens?: number\\n 50 | \\tcost?: number\\n----\\n 81 | \\n 82 | \\t\\t// OpenRouter sends reasoning tokens by default for Gemini 2.5 Pro\\n 83 | \\t\\t// Preview even if you don't request them. This is not the default for\\n----\\n116 | \\t\\t\\tmodel: modelId,\\n117 | \\t\\t\\t...(maxTokens && maxTokens > 0 && { max_tokens: maxTokens }),\\n118 | \\t\\t\\ttemperature,\\n----\\n165 | \\t\\t\\t\\ttype: \\\"usage\\\",\\n166 | \\t\\t\\t\\tinputTokens: lastUsage.prompt_tokens || 0,\\n167 | \\t\\t\\t\\toutputTokens: lastUsage.completion_tokens || 0,\\n168 | \\t\\t\\t\\t// Waiting on OpenRouter to figure out what this represents in the Gemini case\\n169 | \\t\\t\\t\\t// and how to best support it.\\n170 | \\t\\t\\t\\t// cacheReadTokens: lastUsage.prompt_tokens_details?.cached_tokens,\\n171 | \\t\\t\\t\\treasoningTokens: lastUsage.completion_tokens_details?.reasoning_tokens,\\n172 | \\t\\t\\t\\ttotalCost: (lastUsage.is_byok ? BYOK_COST_MULTIPLIER : 1) * (lastUsage.cost || 0),\\n----\\n219 | \\t\\t\\tmodel: modelId,\\n220 | \\t\\t\\tmax_tokens: maxTokens,\\n221 | \\t\\t\\ttemperature,\\n----\\n\\n# src/api/providers/__tests__/deepseek.test.ts\\n 28 | \\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 29 | \\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 30 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 31 | \\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 32 | \\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens_details: {\\n 33 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcache_miss_tokens: 8,\\n 34 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcached_tokens: 2,\\n 35 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 59 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 60 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 61 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 62 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 63 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens_details: {\\n 64 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcache_miss_tokens: 8,\\n 65 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcached_tokens: 2,\\n 66 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n261 | \\t\\t\\tconst usage = {\\n262 | \\t\\t\\t\\tprompt_tokens: 100,\\n263 | \\t\\t\\t\\tcompletion_tokens: 50,\\n264 | \\t\\t\\t\\ttotal_tokens: 150,\\n265 | \\t\\t\\t\\tprompt_tokens_details: {\\n266 | \\t\\t\\t\\t\\tcache_miss_tokens: 80,\\n267 | \\t\\t\\t\\t\\tcached_tokens: 20,\\n268 | \\t\\t\\t\\t},\\n----\\n289 | \\t\\t\\tconst usage = {\\n290 | \\t\\t\\t\\tprompt_tokens: 100,\\n291 | \\t\\t\\t\\tcompletion_tokens: 50,\\n292 | \\t\\t\\t\\ttotal_tokens: 150,\\n293 | \\t\\t\\t\\t// No prompt_tokens_details\\n294 | \\t\\t\\t}\\n----\\n\\n# src/api/providers/__tests__/bedrock-reasoning.test.ts\\n 94 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n 95 | \\t\\t\\t\\tbudget_tokens: 4096, // Uses the full modelMaxThinkingTokens value\\n 96 | \\t\\t\\t})\\n----\\n143 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n144 | \\t\\t\\t\\tbudget_tokens: 8192,\\n145 | \\t\\t\\t})\\n----\\n266 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n267 | \\t\\t\\t\\tbudget_tokens: 4096,\\n268 | \\t\\t\\t})\\n----\\n\\n# src/api/providers/__tests__/xai.test.ts\\n228 | \\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n229 | \\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n230 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 20,\\n231 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcache_read_input_tokens: 5,\\n232 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcache_creation_input_tokens: 15,\\n233 | \\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n284 | \\t\\t\\t\\tmodel: modelId,\\n285 | \\t\\t\\t\\tmax_tokens: modelInfo.maxTokens,\\n286 | \\t\\t\\t\\ttemperature: 0,\\n----\\n\\n# src/api/providers/__tests__/anthropic.spec.ts\\n 19 | \\t\\t\\t\\t\\t\\tusage: {\\n 20 | \\t\\t\\t\\t\\t\\t\\tinput_tokens: 10,\\n 21 | \\t\\t\\t\\t\\t\\t\\toutput_tokens: 5,\\n 22 | \\t\\t\\t\\t\\t\\t},\\n----\\n 30 | \\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 31 | \\t\\t\\t\\t\\t\\t\\t\\t\\tinput_tokens: 100,\\n 32 | \\t\\t\\t\\t\\t\\t\\t\\t\\toutput_tokens: 50,\\n 33 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcache_creation_input_tokens: 20,\\n 34 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcache_read_input_tokens: 10,\\n 35 | \\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n105 | \\n106 | \\t\\tit(\\\"use apiKey for passing token if anthropicUseAuthToken is not set\\\", () => {\\n107 | \\t\\t\\tconst handlerWithCustomUrl = new AnthropicHandler({\\n----\\n112 | \\t\\t\\texpect(mockAnthropicConstructor.mock.calls[0]![0]!.apiKey).toEqual(\\\"test-api-key\\\")\\n113 | \\t\\t\\texpect(mockAnthropicConstructor.mock.calls[0]![0]!.authToken).toBeUndefined()\\n114 | \\t\\t})\\n115 | \\n116 | \\t\\tit(\\\"use apiKey for passing token if anthropicUseAuthToken is set but custom base URL is not given\\\", () => {\\n117 | \\t\\t\\tconst handlerWithCustomUrl = new AnthropicHandler({\\n----\\n123 | \\t\\t\\texpect(mockAnthropicConstructor.mock.calls[0]![0]!.apiKey).toEqual(\\\"test-api-key\\\")\\n124 | \\t\\t\\texpect(mockAnthropicConstructor.mock.calls[0]![0]!.authToken).toBeUndefined()\\n125 | \\t\\t})\\n126 | \\n127 | \\t\\tit(\\\"use authToken for passing token if both of anthropicBaseUrl and anthropicUseAuthToken are set\\\", () => {\\n128 | \\t\\t\\tconst customBaseUrl = \\\"https://custom.anthropic.com\\\"\\n----\\n135 | \\t\\t\\texpect(mockAnthropicConstructor).toHaveBeenCalledTimes(1)\\n136 | \\t\\t\\texpect(mockAnthropicConstructor.mock.calls[0]![0]!.authToken).toEqual(\\\"test-api-key\\\")\\n137 | \\t\\t\\texpect(mockAnthropicConstructor.mock.calls[0]![0]!.apiKey).toBeUndefined()\\n----\\n190 | \\t\\t\\t\\tmessages: [{ role: \\\"user\\\", content: \\\"Test prompt\\\" }],\\n191 | \\t\\t\\t\\tmax_tokens: 8192,\\n192 | \\t\\t\\t\\ttemperature: 0,\\n----\\n\\n# src/api/providers/__tests__/requesty.spec.ts\\n124 | \\t\\t\\t\\t\\t\\tusage: {\\n125 | \\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n126 | \\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 20,\\n127 | \\t\\t\\t\\t\\t\\t\\tprompt_tokens_details: {\\n128 | \\t\\t\\t\\t\\t\\t\\t\\tcaching_tokens: 5,\\n129 | \\t\\t\\t\\t\\t\\t\\t\\tcached_tokens: 2,\\n130 | \\t\\t\\t\\t\\t\\t\\t},\\n----\\n162 | \\t\\t\\t\\texpect.objectContaining({\\n163 | \\t\\t\\t\\t\\tmax_tokens: 8192,\\n164 | \\t\\t\\t\\t\\tmessages: [\\n----\\n204 | \\t\\t\\t\\tmodel: mockOptions.requestyModelId,\\n205 | \\t\\t\\t\\tmax_tokens: 8192,\\n206 | \\t\\t\\t\\tmessages: [{ role: \\\"system\\\", content: \\\"test prompt\\\" }],\\n----\\n\\n# src/api/providers/__tests__/openai-native.spec.ts\\n 29 | \\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 30 | \\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 31 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 32 | \\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 33 | \\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 55 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 56 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 57 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 58 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 59 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n145 | \\t\\t\\t\\t\\t\\tusage: {\\n146 | \\t\\t\\t\\t\\t\\t\\tprompt_tokens: 0,\\n147 | \\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 0,\\n148 | \\t\\t\\t\\t\\t\\t\\ttotal_tokens: 0,\\n149 | \\t\\t\\t\\t\\t\\t},\\n----\\n215 | \\t\\t\\t\\t{ choices: [{ delta: { content: \\\" there\\\" } }], usage: null },\\n216 | \\t\\t\\t\\t{ choices: [{ delta: { content: \\\"!\\\" } }], usage: { prompt_tokens: 10, completion_tokens: 5 } },\\n217 | \\t\\t\\t]\\n----\\n261 | \\t\\t\\t\\t{ choices: [{ delta: { content: null } }], usage: null },\\n262 | \\t\\t\\t\\t{ choices: [{ delta: { content: \\\"Hello\\\" } }], usage: { prompt_tokens: 10, completion_tokens: 5 } },\\n263 | \\t\\t\\t]\\n----\\n\\n# src/api/providers/__tests__/anthropic-vertex.spec.ts\\n 21 | \\t\\t\\t\\t\\t\\tusage: {\\n 22 | \\t\\t\\t\\t\\t\\t\\tinput_tokens: 10,\\n 23 | \\t\\t\\t\\t\\t\\t\\toutput_tokens: 5,\\n 24 | \\t\\t\\t\\t\\t\\t},\\n----\\n 32 | \\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 33 | \\t\\t\\t\\t\\t\\t\\t\\t\\tinput_tokens: 10,\\n 34 | \\t\\t\\t\\t\\t\\t\\t\\t\\toutput_tokens: 5,\\n 35 | \\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 95 | \\t\\t\\t\\t\\t\\tusage: {\\n 96 | \\t\\t\\t\\t\\t\\t\\tinput_tokens: 10,\\n 97 | \\t\\t\\t\\t\\t\\t\\toutput_tokens: 0,\\n 98 | \\t\\t\\t\\t\\t\\t},\\n----\\n118 | \\t\\t\\t\\t\\tusage: {\\n119 | \\t\\t\\t\\t\\t\\toutput_tokens: 5,\\n120 | \\t\\t\\t\\t\\t},\\n----\\n164 | \\t\\t\\t\\tmodel: \\\"claude-3-5-sonnet-v2@20241022\\\",\\n165 | \\t\\t\\t\\tmax_tokens: 8192,\\n166 | \\t\\t\\t\\ttemperature: 0,\\n----\\n284 | \\t\\t\\t\\t\\t\\tusage: {\\n285 | \\t\\t\\t\\t\\t\\t\\tinput_tokens: 10,\\n286 | \\t\\t\\t\\t\\t\\t\\toutput_tokens: 0,\\n287 | \\t\\t\\t\\t\\t\\t\\tcache_creation_input_tokens: 3,\\n288 | \\t\\t\\t\\t\\t\\t\\tcache_read_input_tokens: 2,\\n289 | \\t\\t\\t\\t\\t\\t},\\n----\\n309 | \\t\\t\\t\\t\\tusage: {\\n310 | \\t\\t\\t\\t\\t\\toutput_tokens: 5,\\n311 | \\t\\t\\t\\t\\t},\\n----\\n419 | \\t\\t\\t\\t\\t\\tusage: {\\n420 | \\t\\t\\t\\t\\t\\t\\tinput_tokens: 10,\\n421 | \\t\\t\\t\\t\\t\\t\\toutput_tokens: 0,\\n422 | \\t\\t\\t\\t\\t\\t\\tcache_creation_input_tokens: 5,\\n423 | \\t\\t\\t\\t\\t\\t\\tcache_read_input_tokens: 3,\\n424 | \\t\\t\\t\\t\\t\\t},\\n----\\n484 | \\t\\t\\t\\t\\t\\tusage: {\\n485 | \\t\\t\\t\\t\\t\\t\\tinput_tokens: 10,\\n486 | \\t\\t\\t\\t\\t\\t\\toutput_tokens: 0,\\n487 | \\t\\t\\t\\t\\t\\t},\\n----\\n618 | \\t\\t\\t\\tmodel: \\\"claude-3-5-sonnet-v2@20241022\\\",\\n619 | \\t\\t\\t\\tmax_tokens: 8192,\\n620 | \\t\\t\\t\\ttemperature: 0,\\n----\\n752 | \\n753 | \\t\\t\\t// Test with default thinking budget (80% of max tokens)\\n754 | \\t\\t\\tconst handlerWithDefaultBudget = new AnthropicVertexHandler({\\n----\\n767 | \\t\\t\\t\\tvertexRegion: \\\"us-central1\\\",\\n768 | \\t\\t\\t\\tmodelMaxTokens: 1000, // This would result in 800 tokens for thinking, but minimum is 1024\\n769 | \\t\\t\\t})\\n----\\n789 | \\t\\t\\t\\t\\t\\tmodel: options.model,\\n790 | \\t\\t\\t\\t\\t\\tusage: { input_tokens: 10, output_tokens: 5 },\\n791 | \\t\\t\\t\\t\\t}\\n----\\n794 | \\t\\t\\t\\t\\tasync *[Symbol.asyncIterator]() {\\n795 | \\t\\t\\t\\t\\t\\tyield { type: \\\"message_start\\\", message: { usage: { input_tokens: 10, output_tokens: 5 } } }\\n796 | \\t\\t\\t\\t\\t},\\n----\\n806 | \\t\\t\\t\\texpect.objectContaining({\\n807 | \\t\\t\\t\\t\\tthinking: { type: \\\"enabled\\\", budget_tokens: 4096 },\\n808 | \\t\\t\\t\\t\\ttemperature: 1.0, // Thinking requires temperature 1.0\\n----\\n\\n# src/api/providers/__tests__/lmstudio.test.ts\\n 25 | \\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 26 | \\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 27 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 28 | \\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 29 | \\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 51 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 52 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 53 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 54 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 55 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n\\n# src/api/providers/__tests__/glama.spec.ts\\n 57 | \\t\\t\\t\\t\\t\\t\\t\\t\\tchoices: [{ delta: {}, index: 0 }],\\n 58 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: { prompt_tokens: 10, completion_tokens: 5, total_tokens: 15 },\\n 59 | \\t\\t\\t\\t\\t\\t\\t\\t}\\n----\\n112 | \\t\\t\\tusage: {\\n113 | \\t\\t\\t\\tprompt_tokens: 10,\\n114 | \\t\\t\\t\\tcompletion_tokens: 5,\\n115 | \\t\\t\\t\\ttotal_tokens: 15,\\n116 | \\t\\t\\t},\\n----\\n176 | \\t\\t\\t\\t\\ttemperature: 0,\\n177 | \\t\\t\\t\\t\\tmax_tokens: 8192,\\n178 | \\t\\t\\t\\t}),\\n----\\n194 | \\n195 | \\t\\tit(\\\"should not set max_tokens for non-Anthropic models\\\", async () => {\\n196 | \\t\\t\\t// Reset mock to clear any previous calls\\n----\\n213 | \\t\\t\\t)\\n214 | \\t\\t\\texpect(mockCreate.mock.calls[0][0]).not.toHaveProperty(\\\"max_tokens\\\")\\n215 | \\t\\t})\\n----\\n\\n# src/api/providers/__tests__/openai-usage-tracking.spec.ts\\n 28 | \\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 29 | \\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 30 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 31 | \\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 32 | \\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 47 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 48 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 49 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 2,\\n 50 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 12,\\n 51 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 62 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 63 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 64 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 4,\\n 65 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 14,\\n 66 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 77 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 78 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 79 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 80 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 81 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n155 | \\t\\t\\t\\t\\t\\tchoices: [{ message: { role: \\\"assistant\\\", content: \\\"Test response\\\" } }],\\n156 | \\t\\t\\t\\t\\t\\tusage: { prompt_tokens: 10, completion_tokens: 5, total_tokens: 15 },\\n157 | \\t\\t\\t\\t\\t}\\n----\\n177 | \\t\\t\\t\\t\\t\\t\\tusage: {\\n178 | \\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n179 | \\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n180 | \\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n181 | \\t\\t\\t\\t\\t\\t\\t},\\n----\\n\\n# src/api/providers/__tests__/groq.spec.ts\\n101 | \\t\\t\\t\\t\\t\\t\\tdone: false,\\n102 | \\t\\t\\t\\t\\t\\t\\tvalue: { choices: [{ delta: {} }], usage: { prompt_tokens: 10, completion_tokens: 20 } },\\n103 | \\t\\t\\t\\t\\t\\t})\\n----\\n139 | \\t\\t\\t\\tmodel: modelId,\\n140 | \\t\\t\\t\\tmax_tokens: modelInfo.maxTokens,\\n141 | \\t\\t\\t\\ttemperature: 0.5,\\n----\\n\\n# src/api/providers/__tests__/unbound.spec.ts\\n 82 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 83 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 84 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 85 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 86 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 91 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 92 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 8,\\n 93 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 4,\\n 94 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 12,\\n 95 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcache_creation_input_tokens: 3,\\n 96 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcache_read_input_tokens: 2,\\n 97 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n232 | \\t\\t\\t\\t\\ttemperature: 0,\\n233 | \\t\\t\\t\\t\\tmax_tokens: 8192,\\n234 | \\t\\t\\t\\t}),\\n----\\n253 | \\n254 | \\t\\tit(\\\"should not set max_tokens for non-Anthropic models\\\", async () => {\\n255 | \\t\\t\\tmockCreate.mockClear()\\n----\\n277 | \\n278 | \\t\\t\\texpect(mockCreate.mock.calls[0][0]).not.toHaveProperty(\\\"max_tokens\\\")\\n279 | \\t\\t})\\n----\\n\\n# src/api/providers/__tests__/bedrock-invokedModelId.spec.ts\\n176 | \\n177 | \\t\\t// The last usage event should have the token counts from the metadata\\n178 | \\t\\tconst lastUsageEvent = usageEvents[usageEvents.length - 1]\\n179 | \\t\\t// Expect the usage event to include all token information\\n180 | \\t\\texpect(lastUsageEvent).toMatchObject({\\n----\\n183 | \\t\\t\\toutputTokens: 200,\\n184 | \\t\\t\\t// Cache tokens may be present with default values\\n185 | \\t\\t\\tcacheReadTokens: expect.any(Number),\\n----\\n\\n# src/api/providers/requesty.ts\\n 19 | // Requesty usage includes an extra field for Anthropic use cases.\\n 20 | // Safely cast the prompt token details section to the appropriate structure.\\n 21 | interface RequestyUsage extends OpenAI.CompletionUsage {\\n 22 | \\tprompt_tokens_details?: {\\n 23 | \\t\\tcaching_tokens?: number\\n 24 | \\t\\tcached_tokens?: number\\n 25 | \\t}\\n----\\n 76 | \\t\\tconst requestyUsage = usage as RequestyUsage\\n 77 | \\t\\tconst inputTokens = requestyUsage?.prompt_tokens || 0\\n 78 | \\t\\tconst outputTokens = requestyUsage?.completion_tokens || 0\\n 79 | \\t\\tconst cacheWriteTokens = requestyUsage?.prompt_tokens_details?.caching_tokens || 0\\n 80 | \\t\\tconst cacheReadTokens = requestyUsage?.prompt_tokens_details?.cached_tokens || 0\\n 81 | \\t\\tconst totalCost = modelInfo\\n----\\n102 | \\t\\t\\tinfo,\\n103 | \\t\\t\\tmaxTokens: max_tokens,\\n104 | \\t\\t\\ttemperature,\\n----\\n116 | \\t\\t\\tmodel,\\n117 | \\t\\t\\tmax_tokens,\\n118 | \\t\\t\\ttemperature,\\n----\\n150 | \\tasync completePrompt(prompt: string): Promise<string> {\\n151 | \\t\\tconst { id: model, maxTokens: max_tokens, temperature } = await this.fetchModel()\\n152 | \\n----\\n156 | \\t\\t\\tmodel,\\n157 | \\t\\t\\tmax_tokens,\\n158 | \\t\\t\\tmessages: openAiMessages,\\n----\\n\\n# src/api/providers/lite-llm.ts\\n 46 | \\n 47 | \\t\\t// Required by some providers; others default to max tokens allowed\\n 48 | \\t\\tlet maxTokens: number | undefined = info.maxTokens ?? undefined\\n----\\n 51 | \\t\\t\\tmodel: modelId,\\n 52 | \\t\\t\\tmax_tokens: maxTokens,\\n 53 | \\t\\t\\tmessages: openAiMessages,\\n----\\n 84 | \\t\\t\\t\\t\\ttype: \\\"usage\\\",\\n 85 | \\t\\t\\t\\t\\tinputTokens: lastUsage.prompt_tokens || 0,\\n 86 | \\t\\t\\t\\t\\toutputTokens: lastUsage.completion_tokens || 0,\\n 87 | \\t\\t\\t\\t\\tcacheWriteTokens: lastUsage.cache_creation_input_tokens || 0,\\n 88 | \\t\\t\\t\\t\\tcacheReadTokens: lastUsage.prompt_tokens_details?.cached_tokens || 0,\\n 89 | \\t\\t\\t\\t}\\n----\\n121 | \\n122 | \\t\\t\\trequestOptions.max_tokens = info.maxTokens\\n123 | \\n----\\n136 | interface LiteLLMUsage extends OpenAI.CompletionUsage {\\n137 | \\tcache_creation_input_tokens?: number\\n138 | }\\n----\\n\\n# src/api/providers/fetchers/unbound.ts\\n 34 | \\t\\t\\t\\t\\tcase modelId.startsWith(\\\"anthropic/\\\"):\\n 35 | \\t\\t\\t\\t\\t\\t// Set max tokens to 8192 for supported Anthropic models\\n 36 | \\t\\t\\t\\t\\t\\tif (modelInfo.maxTokens !== 4096) {\\n----\\n\\n# src/api/providers/deepseek.ts\\n 32 | \\t\\t\\ttype: \\\"usage\\\",\\n 33 | \\t\\t\\tinputTokens: usage?.prompt_tokens || 0,\\n 34 | \\t\\t\\toutputTokens: usage?.completion_tokens || 0,\\n 35 | \\t\\t\\tcacheWriteTokens: usage?.prompt_tokens_details?.cache_miss_tokens,\\n 36 | \\t\\t\\tcacheReadTokens: usage?.prompt_tokens_details?.cached_tokens,\\n 37 | \\t\\t}\\n----\\n\\n# src/api/providers/openai.ts\\n162 | \\t\\t\\tif (this.options.includeMaxTokens) {\\n163 | \\t\\t\\t\\trequestOptions.max_tokens = modelInfo.maxTokens\\n164 | \\t\\t\\t}\\n----\\n241 | \\t\\t\\ttype: \\\"usage\\\",\\n242 | \\t\\t\\tinputTokens: usage?.prompt_tokens || 0,\\n243 | \\t\\t\\toutputTokens: usage?.completion_tokens || 0,\\n244 | \\t\\t\\tcacheWriteTokens: usage?.cache_creation_input_tokens || undefined,\\n245 | \\t\\t\\tcacheReadTokens: usage?.cache_read_input_tokens || undefined,\\n246 | \\t\\t}\\n----\\n347 | \\t\\t\\t\\t\\ttype: \\\"usage\\\",\\n348 | \\t\\t\\t\\t\\tinputTokens: chunk.usage.prompt_tokens || 0,\\n349 | \\t\\t\\t\\t\\toutputTokens: chunk.usage.completion_tokens || 0,\\n350 | \\t\\t\\t\\t}\\n----\\n\\n# src/api/providers/fetchers/openrouter.ts\\n 21 | \\tmodality: z.string().nullish(),\\n 22 | \\ttokenizer: z.string().nullish(),\\n 23 | })\\n----\\n 35 | \\tcontext_length: z.number(),\\n 36 | \\tmax_completion_tokens: z.number().nullish(),\\n 37 | \\tpricing: openRouterPricingSchema.optional(),\\n----\\n 48 | \\tarchitecture: openRouterArchitectureSchema.optional(),\\n 49 | \\ttop_provider: z.object({ max_completion_tokens: z.number().nullish() }).optional(),\\n 50 | \\tsupported_parameters: z.array(z.string()).optional(),\\n----\\n115 | \\t\\t\\t\\tmodality: architecture?.modality,\\n116 | \\t\\t\\t\\tmaxTokens: top_provider?.max_completion_tokens,\\n117 | \\t\\t\\t\\tsupportedParameters: supported_parameters,\\n----\\n155 | \\t\\t\\t\\tmodality: architecture?.modality,\\n156 | \\t\\t\\t\\tmaxTokens: endpoint.max_completion_tokens,\\n157 | \\t\\t\\t})\\n----\\n\\n# src/api/providers/openai-native.ts\\n146 | \\tprivate async *yieldUsage(info: ModelInfo, usage: OpenAI.Completions.CompletionUsage | undefined): ApiStream {\\n147 | \\t\\tconst inputTokens = usage?.prompt_tokens || 0 // sum of cache hits and misses\\n148 | \\t\\tconst outputTokens = usage?.completion_tokens || 0\\n149 | \\t\\tconst cacheReadTokens = usage?.prompt_tokens_details?.cached_tokens || 0\\n150 | \\t\\tconst cacheWriteTokens = 0\\n----\\n\\n# src/api/providers/fetchers/requesty.ts\\n 31 | \\t\\t\\tconst modelInfo: ModelInfo = {\\n 32 | \\t\\t\\t\\tmaxTokens: rawModel.max_output_tokens,\\n 33 | \\t\\t\\t\\tcontextWindow: rawModel.context_window,\\n----\\n\\n# src/api/providers/anthropic.ts\\n 29 | \\t\\tconst apiKeyFieldName =\\n 30 | \\t\\t\\tthis.options.anthropicBaseUrl && this.options.anthropicUseAuthToken ? \\\"authToken\\\" : \\\"apiKey\\\"\\n 31 | \\n----\\n 75 | \\t\\t\\t\\t\\t\\tmodel: modelId,\\n 76 | \\t\\t\\t\\t\\t\\tmax_tokens: maxTokens ?? ANTHROPIC_DEFAULT_MAX_TOKENS,\\n 77 | \\t\\t\\t\\t\\t\\ttemperature,\\n----\\n124 | \\t\\t\\t\\t\\tmodel: modelId,\\n125 | \\t\\t\\t\\t\\tmax_tokens: maxTokens ?? ANTHROPIC_DEFAULT_MAX_TOKENS,\\n126 | \\t\\t\\t\\t\\ttemperature,\\n----\\n142 | \\t\\t\\t\\t\\t\\ttype: \\\"usage\\\",\\n143 | \\t\\t\\t\\t\\t\\tinputTokens: usage.input_tokens || 0,\\n144 | \\t\\t\\t\\t\\t\\toutputTokens: usage.output_tokens || 0,\\n145 | \\t\\t\\t\\t\\t\\tcacheWriteTokens: usage.cache_creation_input_tokens || undefined,\\n146 | \\t\\t\\t\\t\\t\\tcacheReadTokens: usage.cache_read_input_tokens || undefined,\\n147 | \\t\\t\\t\\t\\t}\\n----\\n151 | \\t\\t\\t\\tcase \\\"message_delta\\\":\\n152 | \\t\\t\\t\\t\\t// Tells us stop_reason, stop_sequence, and output tokens\\n153 | \\t\\t\\t\\t\\t// along the way and at the end of the message.\\n----\\n156 | \\t\\t\\t\\t\\t\\tinputTokens: 0,\\n157 | \\t\\t\\t\\t\\t\\toutputTokens: chunk.usage.output_tokens || 0,\\n158 | \\t\\t\\t\\t\\t}\\n----\\n231 | \\t\\t\\tmodel,\\n232 | \\t\\t\\tmax_tokens: ANTHROPIC_DEFAULT_MAX_TOKENS,\\n233 | \\t\\t\\tthinking: undefined,\\n----\\n243 | \\t/**\\n244 | \\t * Counts tokens for the given content using Anthropic's API\\n245 | \\t *\\n246 | \\t * @param content The content blocks to count tokens for\\n247 | \\t * @returns A promise resolving to the token count\\n248 | \\t */\\n----\\n258 | \\n259 | \\t\\t\\treturn response.input_tokens\\n260 | \\t\\t} catch (error) {\\n261 | \\t\\t\\t// Log error but fallback to tiktoken estimation\\n262 | \\t\\t\\tconsole.warn(\\\"Anthropic token counting failed, using fallback\\\", error)\\n263 | \\n----\\n\\n# src/api/providers/base-openai-compatible-provider.ts\\n 71 | \\t\\t\\tid: model,\\n 72 | \\t\\t\\tinfo: { maxTokens: max_tokens },\\n 73 | \\t\\t} = this.getModel()\\n----\\n 78 | \\t\\t\\tmodel,\\n 79 | \\t\\t\\tmax_tokens,\\n 80 | \\t\\t\\ttemperature,\\n----\\n100 | \\t\\t\\t\\t\\ttype: \\\"usage\\\",\\n101 | \\t\\t\\t\\t\\tinputTokens: chunk.usage.prompt_tokens || 0,\\n102 | \\t\\t\\t\\t\\toutputTokens: chunk.usage.completion_tokens || 0,\\n103 | \\t\\t\\t\\t}\\n----\\n\\n# src/api/providers/xai.ts\\n 52 | \\t\\t\\tmodel: modelId,\\n 53 | \\t\\t\\tmax_tokens: modelInfo.maxTokens,\\n 54 | \\t\\t\\ttemperature: this.options.modelTemperature ?? XAI_DEFAULT_TEMPERATURE,\\n----\\n 78 | \\t\\t\\tif (chunk.usage) {\\n 79 | \\t\\t\\t\\t// Extract detailed token information if available\\n 80 | \\t\\t\\t\\t// First check for prompt_tokens_details structure (real API response)\\n 81 | \\t\\t\\t\\tconst promptDetails = \\\"prompt_tokens_details\\\" in chunk.usage ? chunk.usage.prompt_tokens_details : null;\\n 82 | \\t\\t\\t\\tconst cachedTokens = promptDetails && \\\"cached_tokens\\\" in promptDetails ? promptDetails.cached_tokens : 0;\\n 83 | \\n 84 | \\t\\t\\t\\t// Fall back to direct fields in usage (used in test mocks)\\n 85 | \\t\\t\\t\\tconst readTokens = cachedTokens || (\\\"cache_read_input_tokens\\\" in chunk.usage ? (chunk.usage as any).cache_read_input_tokens : 0);\\n 86 | \\t\\t\\t\\tconst writeTokens = \\\"cache_creation_input_tokens\\\" in chunk.usage ? (chunk.usage as any).cache_creation_input_tokens : 0;\\n 87 | \\n----\\n 89 | \\t\\t\\t\\t\\ttype: \\\"usage\\\",\\n 90 | \\t\\t\\t\\t\\tinputTokens: chunk.usage.prompt_tokens || 0,\\n 91 | \\t\\t\\t\\t\\toutputTokens: chunk.usage.completion_tokens || 0,\\n 92 | \\t\\t\\t\\t\\tcacheReadTokens: readTokens,\\n----\\n\\n# src/api/providers/bedrock.ts\\n 52 | \\t\\ttype: \\\"enabled\\\"\\n 53 | \\t\\tbudget_tokens: number\\n 54 | \\t}\\n----\\n109 | \\tmessageStop?: {\\n110 | \\t\\tstopReason?: \\\"end_turn\\\" | \\\"tool_use\\\" | \\\"max_tokens\\\" | \\\"stop_sequence\\\"\\n111 | \\t\\tadditionalModelResponseFields?: Record<string, unknown>\\n----\\n349 | \\t\\t\\t\\t\\ttype: \\\"enabled\\\",\\n350 | \\t\\t\\t\\t\\tbudget_tokens: metadata?.thinking?.maxThinkingTokens || modelConfig.reasoningBudget || 4096,\\n351 | \\t\\t\\t\\t},\\n----\\n418 | \\n419 | \\t\\t\\t\\t\\t// Check both field naming conventions for cache tokens\\n420 | \\t\\t\\t\\t\\tconst cacheReadTokens = usage.cacheReadInputTokens || usage.cacheReadInputTokenCount || 0\\n----\\n422 | \\n423 | \\t\\t\\t\\t\\t// Always include all available token information\\n424 | \\t\\t\\t\\t\\tyield {\\n----\\n450 | \\n451 | \\t\\t\\t\\t\\t\\t\\t// Check both field naming conventions for cache tokens\\n452 | \\t\\t\\t\\t\\t\\t\\tconst cacheReadTokens =\\n----\\n1049 | \\t\\tTOO_MANY_TOKENS: {\\n1050 | \\t\\t\\tpatterns: [\\\"too many tokens\\\"],\\n1051 | \\t\\t\\tmessageTemplate: `\\\"Too many tokens\\\" error detected.\\n1052 | Possible Causes:\\n1053 | 1. Input exceeds model's context window limit\\n1054 | 2. Rate limiting (too many tokens per minute)\\n1055 | 3. Quota exceeded for token usage\\n1056 | 4. Other token-related service limitations\\n1057 | \\n----\\n\\n# src/api/providers/fetchers/litellm.ts\\n 52 | \\t\\t\\t\\tmodels[modelName] = {\\n 53 | \\t\\t\\t\\t\\tmaxTokens: modelInfo.max_tokens || 8192,\\n 54 | \\t\\t\\t\\t\\tcontextWindow: modelInfo.max_input_tokens || 200000,\\n 55 | \\t\\t\\t\\t\\tsupportsImages: Boolean(modelInfo.supports_vision),\\n----\\n 58 | \\t\\t\\t\\t\\tsupportsPromptCache: Boolean(modelInfo.supports_prompt_caching),\\n 59 | \\t\\t\\t\\t\\tinputPrice: modelInfo.input_cost_per_token ? modelInfo.input_cost_per_token * 1000000 : undefined,\\n 60 | \\t\\t\\t\\t\\toutputPrice: modelInfo.output_cost_per_token\\n 61 | \\t\\t\\t\\t\\t\\t? modelInfo.output_cost_per_token * 1000000\\n 62 | \\t\\t\\t\\t\\t\\t: undefined,\\n 63 | \\t\\t\\t\\t\\tcacheWritesPrice: modelInfo.cache_creation_input_token_cost\\n 64 | \\t\\t\\t\\t\\t\\t? modelInfo.cache_creation_input_token_cost * 1000000\\n 65 | \\t\\t\\t\\t\\t\\t: undefined,\\n 66 | \\t\\t\\t\\t\\tcacheReadsPrice: modelInfo.cache_read_input_token_cost\\n 67 | \\t\\t\\t\\t\\t\\t? modelInfo.cache_read_input_token_cost * 1000000\\n 68 | \\t\\t\\t\\t\\t\\t: undefined,\\n----\\n\\n# src/api/providers/fetchers/__tests__/openrouter.spec.ts\\n139 | \\t\\t\\t\\tsupportsReasoningEffort: false,\\n140 | \\t\\t\\t\\tsupportedParameters: [\\\"max_tokens\\\", \\\"temperature\\\", \\\"reasoning\\\", \\\"include_reasoning\\\"],\\n141 | \\t\\t\\t})\\n----\\n156 | \\t\\t\\t\\tsupportsReasoningEffort: true,\\n157 | \\t\\t\\t\\tsupportedParameters: [\\\"max_tokens\\\", \\\"temperature\\\", \\\"reasoning\\\", \\\"include_reasoning\\\"],\\n158 | \\t\\t\\t})\\n----\\n\\n# src/api/providers/ollama.ts\\n 79 | \\t\\t\\t\\ttype: \\\"usage\\\",\\n 80 | \\t\\t\\t\\tinputTokens: lastUsage?.prompt_tokens || 0,\\n 81 | \\t\\t\\t\\toutputTokens: lastUsage?.completion_tokens || 0,\\n 82 | \\t\\t\\t}\\n----\\n\\n# src/api/providers/base-provider.ts\\n 21 | \\t/**\\n 22 | \\t * Default token counting implementation using tiktoken.\\n 23 | \\t * Providers can override this to use their native token counting endpoints.\\n 24 | \\t *\\n 25 | \\t * @param content The content to count tokens for\\n 26 | \\t * @returns A promise resolving to the token count\\n 27 | \\t */\\n----\\n\\n# src/api/providers/fetchers/__tests__/litellm.test.ts\\n 41 | \\t\\t\\t\\t\\t\\tmodel_info: {\\n 42 | \\t\\t\\t\\t\\t\\t\\tmax_tokens: 4096,\\n 43 | \\t\\t\\t\\t\\t\\t\\tmax_input_tokens: 200000,\\n 44 | \\t\\t\\t\\t\\t\\t\\tsupports_vision: true,\\n 45 | \\t\\t\\t\\t\\t\\t\\tsupports_prompt_caching: false,\\n 46 | \\t\\t\\t\\t\\t\\t\\tinput_cost_per_token: 0.000003,\\n 47 | \\t\\t\\t\\t\\t\\t\\toutput_cost_per_token: 0.000015,\\n 48 | \\t\\t\\t\\t\\t\\t\\tsupports_computer_use: true,\\n----\\n 56 | \\t\\t\\t\\t\\t\\tmodel_info: {\\n 57 | \\t\\t\\t\\t\\t\\t\\tmax_tokens: 8192,\\n 58 | \\t\\t\\t\\t\\t\\t\\tmax_input_tokens: 128000,\\n 59 | \\t\\t\\t\\t\\t\\t\\tsupports_vision: false,\\n 60 | \\t\\t\\t\\t\\t\\t\\tsupports_prompt_caching: false,\\n 61 | \\t\\t\\t\\t\\t\\t\\tinput_cost_per_token: 0.00001,\\n 62 | \\t\\t\\t\\t\\t\\t\\toutput_cost_per_token: 0.00003,\\n 63 | \\t\\t\\t\\t\\t\\t\\tsupports_computer_use: false,\\n----\\n108 | \\n109 | \\tit(\\\"makes request without authorization header when no API key provided\\\", async () => {\\n110 | \\t\\tconst mockResponse = {\\n----\\n134 | \\t\\t\\t\\t\\t\\tmodel_info: {\\n135 | \\t\\t\\t\\t\\t\\t\\tmax_tokens: 4096,\\n136 | \\t\\t\\t\\t\\t\\t\\tmax_input_tokens: 200000,\\n137 | \\t\\t\\t\\t\\t\\t\\tsupports_vision: true,\\n----\\n146 | \\t\\t\\t\\t\\t\\tmodel_info: {\\n147 | \\t\\t\\t\\t\\t\\t\\tmax_tokens: 4096,\\n148 | \\t\\t\\t\\t\\t\\t\\tmax_input_tokens: 200000,\\n149 | \\t\\t\\t\\t\\t\\t\\tsupports_vision: false,\\n----\\n205 | \\t\\t\\t\\tstatus: 401,\\n206 | \\t\\t\\t\\tstatusText: \\\"Unauthorized\\\",\\n207 | \\t\\t\\t},\\n----\\n214 | \\t\\tawait expect(getLiteLLMModels(DUMMY_INVALID_KEY, \\\"http://localhost:4000\\\")).rejects.toThrow(\\n215 | \\t\\t\\t\\\"Failed to fetch LiteLLM models: 401 Unauthorized. Check base URL and API key.\\\",\\n216 | \\t\\t)\\n----\\n278 | \\t\\t\\t\\t\\t\\tmodel_info: {\\n279 | \\t\\t\\t\\t\\t\\t\\tmax_tokens: 4096,\\n280 | \\t\\t\\t\\t\\t\\t\\tmax_input_tokens: 200000,\\n281 | \\t\\t\\t\\t\\t\\t\\tsupports_vision: true,\\n----\\n291 | \\t\\t\\t\\t\\t\\tmodel_info: {\\n292 | \\t\\t\\t\\t\\t\\t\\tmax_tokens: 8192,\\n293 | \\t\\t\\t\\t\\t\\t\\tmax_input_tokens: 128000,\\n294 | \\t\\t\\t\\t\\t\\t\\tsupports_vision: false,\\n----\\n339 | \\t\\t\\t\\t\\t\\tmodel_info: {\\n340 | \\t\\t\\t\\t\\t\\t\\tmax_tokens: 4096,\\n341 | \\t\\t\\t\\t\\t\\t\\tmax_input_tokens: 200000,\\n342 | \\t\\t\\t\\t\\t\\t\\tsupports_vision: true,\\n----\\n352 | \\t\\t\\t\\t\\t\\tmodel_info: {\\n353 | \\t\\t\\t\\t\\t\\t\\tmax_tokens: 8192,\\n354 | \\t\\t\\t\\t\\t\\t\\tmax_input_tokens: 128000,\\n355 | \\t\\t\\t\\t\\t\\t\\tsupports_vision: false,\\n----\\n365 | \\t\\t\\t\\t\\t\\tmodel_info: {\\n366 | \\t\\t\\t\\t\\t\\t\\tmax_tokens: 8192,\\n367 | \\t\\t\\t\\t\\t\\t\\tmax_input_tokens: 128000,\\n368 | \\t\\t\\t\\t\\t\\t\\tsupports_vision: false,\\n----\\n424 | \\t\\t\\t\\t\\t\\tmodel_info: {\\n425 | \\t\\t\\t\\t\\t\\t\\tmax_tokens: 4096,\\n426 | \\t\\t\\t\\t\\t\\t\\tmax_input_tokens: 200000,\\n427 | \\t\\t\\t\\t\\t\\t\\tsupports_vision: true,\\n----\\n436 | \\t\\t\\t\\t\\t\\tmodel_info: {\\n437 | \\t\\t\\t\\t\\t\\t\\tmax_tokens: 4096,\\n438 | \\t\\t\\t\\t\\t\\t\\tmax_input_tokens: 200000,\\n439 | \\t\\t\\t\\t\\t\\t\\tsupports_vision: true,\\n----\\n448 | \\t\\t\\t\\t\\t\\tmodel_info: {\\n449 | \\t\\t\\t\\t\\t\\t\\tmax_tokens: 4096,\\n450 | \\t\\t\\t\\t\\t\\t\\tmax_input_tokens: 200000,\\n451 | \\t\\t\\t\\t\\t\\t\\tsupports_vision: true,\\n----\\n\\n# src/api/providers/anthropic-vertex.ts\\n  2 | import { AnthropicVertex } from \\\"@anthropic-ai/vertex-sdk\\\"\\n  3 | import { GoogleAuth, JWTInput } from \\\"google-auth-library\\\"\\n  4 | \\n----\\n 41 | \\t\\t\\t\\tgoogleAuth: new GoogleAuth({\\n 42 | \\t\\t\\t\\t\\tscopes: [\\\"https://www.googleapis.com/auth/cloud-platform\\\"],\\n 43 | \\t\\t\\t\\t\\tcredentials: safeJsonParse<JWTInput>(this.options.vertexJsonCredentials, undefined),\\n----\\n 50 | \\t\\t\\t\\tgoogleAuth: new GoogleAuth({\\n 51 | \\t\\t\\t\\t\\tscopes: [\\\"https://www.googleapis.com/auth/cloud-platform\\\"],\\n 52 | \\t\\t\\t\\t\\tkeyFile: this.options.vertexKeyFile,\\n----\\n 87 | \\t\\t\\tmodel: id,\\n 88 | \\t\\t\\tmax_tokens: maxTokens ?? ANTHROPIC_DEFAULT_MAX_TOKENS,\\n 89 | \\t\\t\\ttemperature,\\n----\\n107 | \\t\\t\\t\\t\\t\\ttype: \\\"usage\\\",\\n108 | \\t\\t\\t\\t\\t\\tinputTokens: usage.input_tokens || 0,\\n109 | \\t\\t\\t\\t\\t\\toutputTokens: usage.output_tokens || 0,\\n110 | \\t\\t\\t\\t\\t\\tcacheWriteTokens: usage.cache_creation_input_tokens || undefined,\\n111 | \\t\\t\\t\\t\\t\\tcacheReadTokens: usage.cache_read_input_tokens || undefined,\\n112 | \\t\\t\\t\\t\\t}\\n----\\n119 | \\t\\t\\t\\t\\t\\tinputTokens: 0,\\n120 | \\t\\t\\t\\t\\t\\toutputTokens: chunk.usage!.output_tokens || 0,\\n121 | \\t\\t\\t\\t\\t}\\n----\\n189 | \\t\\t\\t\\tmodel: id,\\n190 | \\t\\t\\t\\tmax_tokens: maxTokens,\\n191 | \\t\\t\\t\\ttemperature,\\n----\\n\\n# src/services/mcp/__tests__/McpHub.test.ts\\n 50 | \\t\\t\\tscheme: \\\"file\\\",\\n 51 | \\t\\t\\tauthority: \\\"\\\",\\n 52 | \\t\\t\\tpath: \\\"/test/path\\\",\\n----\\n\\n# src/services/code-index/embedders/openai.ts\\n 52 | \\t\\t\\t\\t\\tconsole.warn(\\n 53 | \\t\\t\\t\\t\\t\\t`Text at index ${i} exceeds maximum token limit (${itemTokens} > ${MAX_ITEM_TOKENS}). Skipping.`,\\n 54 | \\t\\t\\t\\t\\t)\\n----\\n108 | \\t\\t\\t\\t\\tusage: {\\n109 | \\t\\t\\t\\t\\t\\tpromptTokens: response.usage?.prompt_tokens || 0,\\n110 | \\t\\t\\t\\t\\t\\ttotalTokens: response.usage?.total_tokens || 0,\\n111 | \\t\\t\\t\\t\\t},\\n----\\n\\n# src/services/code-index/embedders/openai-compatible.ts\\n 18 | \\tusage?: {\\n 19 | \\t\\tprompt_tokens?: number\\n 20 | \\t\\ttotal_tokens?: number\\n 21 | \\t}\\n----\\n 34 | \\t * @param baseUrl The base URL for the OpenAI-compatible API endpoint\\n 35 | \\t * @param apiKey The API key for authentication\\n 36 | \\t * @param modelId Optional model identifier (defaults to \\\"text-embedding-3-small\\\")\\n----\\n 75 | \\t\\t\\t\\t\\tconsole.warn(\\n 76 | \\t\\t\\t\\t\\t\\t`Text at index ${i} exceeds maximum token limit (${itemTokens} > ${MAX_ITEM_TOKENS}). Skipping.`,\\n 77 | \\t\\t\\t\\t\\t)\\n----\\n156 | \\t\\t\\t\\t\\tusage: {\\n157 | \\t\\t\\t\\t\\t\\tpromptTokens: response.usage?.prompt_tokens || 0,\\n158 | \\t\\t\\t\\t\\t\\ttotalTokens: response.usage?.total_tokens || 0,\\n159 | \\t\\t\\t\\t\\t},\\n----\\n\\n# src/services/marketplace/types.ts\\n 20 | \\tdescription: string\\n 21 | \\tauthor?: string\\n 22 | \\tauthorUrl?: string\\n 23 | \\ttags?: string[]\\n----\\n 45 | \\tdescription: string\\n 46 | \\tauthor?: string\\n 47 | \\tauthorUrl?: string\\n 48 | \\turl: string // Required url field\\n----\\n 62 | \\ttype: MarketplaceItemType\\n 63 | \\tauthor?: string\\n 64 | \\tauthorUrl?: string\\n 65 | \\turl?: string // Optional - only MCPs have url\\n----\\n\\n# src/services/code-index/embedders/__tests__/openai-compatible.spec.ts\\n103 | \\t\\t\\t\\tdata: [{ embedding: [0.1, 0.2, 0.3] }],\\n104 | \\t\\t\\t\\tusage: { prompt_tokens: 10, total_tokens: 15 },\\n105 | \\t\\t\\t}\\n----\\n124 | \\t\\t\\t\\tdata: [{ embedding: [0.1, 0.2, 0.3] }, { embedding: [0.4, 0.5, 0.6] }],\\n125 | \\t\\t\\t\\tusage: { prompt_tokens: 20, total_tokens: 30 },\\n126 | \\t\\t\\t}\\n----\\n149 | \\t\\t\\t\\tdata: [{ embedding: [0.1, 0.2, 0.3] }],\\n150 | \\t\\t\\t\\tusage: { prompt_tokens: 10, total_tokens: 15 },\\n151 | \\t\\t\\t}\\n----\\n194 | \\t\\t\\t\\t\\tdata: [{ embedding: base64String }], // Base64 string instead of array\\n195 | \\t\\t\\t\\t\\tusage: { prompt_tokens: 10, total_tokens: 15 },\\n196 | \\t\\t\\t\\t}\\n----\\n226 | \\t\\t\\t\\t\\tdata: [{ embedding: base64String1 }, { embedding: base64String2 }],\\n227 | \\t\\t\\t\\t\\tusage: { prompt_tokens: 20, total_tokens: 30 },\\n228 | \\t\\t\\t\\t}\\n----\\n253 | \\t\\t\\t\\t\\t],\\n254 | \\t\\t\\t\\t\\tusage: { prompt_tokens: 20, total_tokens: 30 },\\n255 | \\t\\t\\t\\t}\\n----\\n270 | \\t\\t/**\\n271 | \\t\\t * Test batching logic when texts exceed token limits\\n272 | \\t\\t */\\n----\\n283 | \\t\\t\\t\\t\\t],\\n284 | \\t\\t\\t\\t\\tusage: { prompt_tokens: 10, total_tokens: 15 },\\n285 | \\t\\t\\t\\t})\\n----\\n299 | \\t\\t\\t\\t\\tdata: [{ embedding: [0.1, 0.2, 0.3] }, { embedding: [0.4, 0.5, 0.6] }],\\n300 | \\t\\t\\t\\t\\tusage: { prompt_tokens: 10, total_tokens: 15 },\\n301 | \\t\\t\\t\\t}\\n----\\n306 | \\t\\t\\t\\t// Should warn about oversized text\\n307 | \\t\\t\\t\\texpect(console.warn).toHaveBeenCalledWith(expect.stringContaining(\\\"exceeds maximum token limit\\\"))\\n308 | \\n----\\n317 | \\t\\t\\t\\t\\tdata: [{ embedding: [0.1, 0.2, 0.3] }, { embedding: [0.4, 0.5, 0.6] }],\\n318 | \\t\\t\\t\\t\\tusage: { prompt_tokens: 10, total_tokens: 15 },\\n319 | \\t\\t\\t\\t})\\n----\\n354 | \\t\\t\\t\\t\\t\\tdata: [{ embedding: base64String }],\\n355 | \\t\\t\\t\\t\\t\\tusage: { prompt_tokens: 10, total_tokens: 15 },\\n356 | \\t\\t\\t\\t\\t})\\n----\\n375 | \\t\\t\\t\\tconst testTexts = [\\\"Hello world\\\"]\\n376 | \\t\\t\\t\\tconst authError = new Error(\\\"Unauthorized\\\")\\n377 | \\t\\t\\t\\t;(authError as any).status = 401\\n378 | \\n379 | \\t\\t\\t\\tmockEmbeddingsCreate.mockRejectedValue(authError)\\n380 | \\n----\\n452 | \\t\\t\\t\\t\\tdata: null,\\n453 | \\t\\t\\t\\t\\tusage: { prompt_tokens: 10, total_tokens: 15 },\\n454 | \\t\\t\\t\\t}\\n----\\n494 | \\t\\t\\t\\t\\tusage: {\\n495 | \\t\\t\\t\\t\\t\\tprompt_tokens: 2,\\n496 | \\t\\t\\t\\t\\t\\ttotal_tokens: 2,\\n497 | \\t\\t\\t\\t\\t},\\n----\\n\\n# src/services/marketplace/schemas.ts\\n 35 | \\ttype: marketplaceItemTypeSchema,\\n 36 | \\tauthor: z.string().optional(),\\n 37 | \\tauthorUrl: z.string().url(\\\"Author URL must be a valid URL\\\").optional(),\\n 38 | \\ttags: z.array(z.string()).optional(),\\n----\\n 61 | \\tdescription: z.string(),\\n 62 | \\tauthor: z.string().optional(),\\n 63 | \\tauthorUrl: z.string().url().optional(),\\n 64 | \\ttags: z.array(z.string()).optional(),\\n----\\n 72 | \\tdescription: z.string(),\\n 73 | \\tauthor: z.string().optional(),\\n 74 | \\tauthorUrl: z.string().url().optional(),\\n 75 | \\turl: z.string().url(), // Required url field\\n----\\n\\n# src/services/tree-sitter/__tests__/fixtures/sample-embedded_template.ts\\n 35 | <div class=\\\"container\\\">\\n 36 |   <% if user.authenticated? %>\\n 37 |     <h1>Welcome, <%= user.name %></h1>\\n----\\n\\n# src/services/tree-sitter/__tests__/parseSourceCodeDefinitions.embedded_template.test.ts\\n 43 | \\tit(\\\"should detect control structures\\\", () => {\\n 44 | \\t\\texpect(parseResult).toMatch(/\\\\d+--\\\\d+ \\\\|\\\\s+<% if user\\\\.authenticated\\\\? %>/)\\n 45 | \\t\\texpect(parseResult).toMatch(/\\\\d+--\\\\d+ \\\\|\\\\s+<% user\\\\.posts\\\\.each do \\\\|post\\\\| %>/)\\n----\\n\\n# src/workers/countTokens.ts\\n  4 | \\n  5 | import { tiktoken } from \\\"../utils/tiktoken\\\"\\n  6 | \\n----\\n 10 | \\ttry {\\n 11 | \\t\\tconst count = await tiktoken(content)\\n 12 | \\t\\treturn { success: true, count }\\n----\\n\\n# src/activate/handleUri.ts\\n 37 | \\t\\t}\\n 38 | \\t\\tcase \\\"/auth/clerk/callback\\\": {\\n 39 | \\t\\t\\tconst code = query.get(\\\"code\\\")\\n----\\n\\n# src/__tests__/dist_assets.test.ts\\n  6 | \\n  7 | \\tdescribe(\\\"tiktoken\\\", () => {\\n  8 | \\t\\tit(\\\"should have tiktoken wasm file\\\", () => {\\n  9 | \\t\\t\\texpect(fs.existsSync(path.join(distPath, \\\"tiktoken_bg.wasm\\\"))).toBe(true)\\n 10 | \\t\\t})\\n----\\n\\n# src/integrations/terminal/ShellIntegrationManager.ts\\n 43 | \\t[ -f \\\"$ZDOTDIR/.zshrc\\\" ] && source \\\"$ZDOTDIR/.zshrc\\\"\\n 44 | \\t[ -f \\\"$ZDOTDIR/.zlogin\\\" ] && source \\\"$ZDOTDIR/.zlogin\\\"\\n 45 | \\t[ \\\"$ZDOTDIR\\\" = \\\"$HOME\\\" ] && unset ZDOTDIR\\n----\\n\\n# src/extension/api.ts\\n219 | \\n220 | \\t\\t\\tcline.on(\\\"taskCompleted\\\", async (_, tokenUsage, toolUsage) => {\\n221 | \\t\\t\\t\\tthis.emit(RooCodeEventName.TaskCompleted, cline.taskId, tokenUsage, toolUsage)\\n222 | \\t\\t\\t\\tthis.taskMap.delete(cline.taskId)\\n----\\n224 | \\t\\t\\t\\tawait this.fileLog(\\n225 | \\t\\t\\t\\t\\t`[${new Date().toISOString()}] taskCompleted -> ${cline.taskId} | ${JSON.stringify(tokenUsage, null, 2)} | ${JSON.stringify(toolUsage, null, 2)}\\\\n`,\\n226 | \\t\\t\\t\\t)\\n----\\n\\n# src/utils/countTokens.ts\\n  4 | import { countTokensResultSchema } from \\\"../workers/types\\\"\\n  5 | import { tiktoken } from \\\"./tiktoken\\\"\\n  6 | \\n----\\n 27 | \\tif (!useWorker || !pool) {\\n 28 | \\t\\treturn tiktoken(content)\\n 29 | \\t}\\n----\\n 42 | \\t\\tconsole.error(error)\\n 43 | \\t\\treturn tiktoken(content)\\n 44 | \\t}\\n----\\n\\n# src/core/checkpoints/index.ts\\n215 | \\t\\t\\t\\tJSON.stringify({\\n216 | \\t\\t\\t\\t\\ttokensIn: totalTokensIn,\\n217 | \\t\\t\\t\\t\\ttokensOut: totalTokensOut,\\n218 | \\t\\t\\t\\t\\tcacheWrites: totalCacheWrites,\\n----\\n\\n# src/core/condense/index.ts\\n 55 | \\tcost: number // The cost of the summarization operation\\n 56 | \\tnewContextTokens?: number // The number of tokens in the context for the next API request\\n 57 | \\terror?: string // Populated iff the operation fails: error message shown to the user on failure (see Task.ts)\\n----\\n 63 |  * @param {ApiMessage[]} messages - The conversation messages\\n 64 |  * @param {ApiHandler} apiHandler - The API handler to use for token counting.\\n 65 |  * @param {string} systemPrompt - The system prompt for API requests, which should be considered in the context token count\\n 66 |  * @param {string} taskId - The task ID for the conversation, used for telemetry\\n----\\n 73 |  * @param {ApiMessage[]} messages - The conversation messages\\n 74 |  * @param {ApiHandler} apiHandler - The API handler to use for token counting (fallback if condensingApiHandler not provided)\\n 75 |  * @param {string} systemPrompt - The system prompt for API requests (fallback if customCondensingPrompt not provided)\\n 76 |  * @param {string} taskId - The task ID for the conversation, used for telemetry\\n 77 |  * @param {number} prevContextTokens - The number of tokens currently in the context, used to ensure we don't grow the context\\n 78 |  * @param {boolean} isAutomaticTrigger - Whether the summarization is triggered automatically\\n----\\n186 | \\n187 | \\t// Count the tokens in the context for the next API request\\n188 | \\t// We only estimate the tokens in summaryMesage if outputTokens is 0, otherwise we use outputTokens\\n189 | \\tconst systemPromptMessage: ApiMessage = { role: \\\"user\\\", content: systemPrompt }\\n----\\n\\nLoading...\"}"
}
[2025-06-13T04:59:50.474Z] {
  "ts": 1749790789409,
  "type": "say",
  "say": "text",
  "text": "I can see this is a VSCode extension project, but the search results show mostly token-related code rather than authentication/OAuth code. Let me search more specifically for authentication and OAuth related files.",
  "partial": false
}
[2025-06-13T04:59:51.246Z] {
  "ts": 1749790790474,
  "type": "ask",
  "ask": "tool",
  "text": "{\"tool\":\"searchFiles\",\"path\":\"src\",\"regex\":\"oauth|auth|login|clerk|session|refresh\",\"filePattern\":\"*.ts\",\"content\":\"Found 83 results.\\n\\n# src/services/ripgrep/index.ts\\n101 | \\t\\tconst rgProcess = childProcess.spawn(bin, args)\\n102 | \\t\\t// cross-platform alternative to head, which is ripgrep author's recommendation for limiting output.\\n103 | \\t\\tconst rl = readline.createInterface({\\n----\\n\\n# src/services/browser/BrowserSession.ts\\n543 | \\t\\t\\tconst [width, height] = size.split(\\\",\\\").map(Number)\\n544 | \\t\\t\\tconst session = await page.createCDPSession()\\n545 | \\t\\t\\tawait page.setViewport({ width, height })\\n546 | \\t\\t\\tconst { windowId } = await session.send(\\\"Browser.getWindowForTarget\\\")\\n547 | \\t\\t\\tawait session.send(\\\"Browser.setWindowBounds\\\", {\\n548 | \\t\\t\\t\\tbounds: { width, height },\\n----\\n\\n# src/services/mcp/__tests__/McpHub.test.ts\\n 50 | \\t\\t\\tscheme: \\\"file\\\",\\n 51 | \\t\\t\\tauthority: \\\"\\\",\\n 52 | \\t\\t\\tpath: \\\"/test/path\\\",\\n----\\n\\n# src/services/mcp/McpHub.ts\\n1092 | \\n1093 | \\tpublic async refreshAllConnections(): Promise<void> {\\n1094 | \\t\\tif (this.isConnecting) {\\n1095 | \\t\\t\\tvscode.window.showInformationMessage(t(\\\"mcp:info.already_refreshing\\\"))\\n1096 | \\t\\t\\treturn\\n----\\n1099 | \\t\\tthis.isConnecting = true\\n1100 | \\t\\tvscode.window.showInformationMessage(t(\\\"mcp:info.refreshing_all\\\"))\\n1101 | \\n----\\n1151 | \\n1152 | \\t\\t\\tvscode.window.showInformationMessage(t(\\\"mcp:info.all_refreshed\\\"))\\n1153 | \\t\\t} catch (error) {\\n1154 | \\t\\t\\tthis.showErrorMessage(\\\"Failed to refresh MCP servers\\\", error)\\n1155 | \\t\\t} finally {\\n----\\n1244 | \\n1245 | \\t\\t\\t\\t\\t// Only refresh capabilities if connected\\n1246 | \\t\\t\\t\\t\\tif (connection.server.status === \\\"connected\\\") {\\n----\\n1254 | \\t\\t\\t\\t} catch (error) {\\n1255 | \\t\\t\\t\\t\\tconsole.error(`Failed to refresh capabilities for ${serverName}:`, error)\\n1256 | \\t\\t\\t\\t}\\n----\\n\\n# src/services/code-index/embedders/__tests__/openai-compatible.spec.ts\\n375 | \\t\\t\\t\\tconst testTexts = [\\\"Hello world\\\"]\\n376 | \\t\\t\\t\\tconst authError = new Error(\\\"Unauthorized\\\")\\n377 | \\t\\t\\t\\t;(authError as any).status = 401\\n378 | \\n379 | \\t\\t\\t\\tmockEmbeddingsCreate.mockRejectedValue(authError)\\n380 | \\n----\\n\\n# src/services/code-index/embedders/openai-compatible.ts\\n 34 | \\t * @param baseUrl The base URL for the OpenAI-compatible API endpoint\\n 35 | \\t * @param apiKey The API key for authentication\\n 36 | \\t * @param modelId Optional model identifier (defaults to \\\"text-embedding-3-small\\\")\\n----\\n\\n# src/services/marketplace/schemas.ts\\n 35 | \\ttype: marketplaceItemTypeSchema,\\n 36 | \\tauthor: z.string().optional(),\\n 37 | \\tauthorUrl: z.string().url(\\\"Author URL must be a valid URL\\\").optional(),\\n 38 | \\ttags: z.array(z.string()).optional(),\\n----\\n 61 | \\tdescription: z.string(),\\n 62 | \\tauthor: z.string().optional(),\\n 63 | \\tauthorUrl: z.string().url().optional(),\\n 64 | \\ttags: z.array(z.string()).optional(),\\n----\\n 72 | \\tdescription: z.string(),\\n 73 | \\tauthor: z.string().optional(),\\n 74 | \\tauthorUrl: z.string().url().optional(),\\n 75 | \\turl: z.string().url(), // Required url field\\n----\\n\\n# src/services/tree-sitter/__tests__/fixtures/sample-embedded_template.ts\\n 35 | <div class=\\\"container\\\">\\n 36 |   <% if user.authenticated? %>\\n 37 |     <h1>Welcome, <%= user.name %></h1>\\n----\\n\\n# src/services/tree-sitter/__tests__/parseSourceCodeDefinitions.embedded_template.test.ts\\n 43 | \\tit(\\\"should detect control structures\\\", () => {\\n 44 | \\t\\texpect(parseResult).toMatch(/\\\\d+--\\\\d+ \\\\|\\\\s+<% if user\\\\.authenticated\\\\? %>/)\\n 45 | \\t\\texpect(parseResult).toMatch(/\\\\d+--\\\\d+ \\\\|\\\\s+<% user\\\\.posts\\\\.each do \\\\|post\\\\| %>/)\\n----\\n\\n# src/services/marketplace/types.ts\\n 20 | \\tdescription: string\\n 21 | \\tauthor?: string\\n 22 | \\tauthorUrl?: string\\n 23 | \\ttags?: string[]\\n----\\n 45 | \\tdescription: string\\n 46 | \\tauthor?: string\\n 47 | \\tauthorUrl?: string\\n 48 | \\turl: string // Required url field\\n----\\n 62 | \\ttype: MarketplaceItemType\\n 63 | \\tauthor?: string\\n 64 | \\tauthorUrl?: string\\n 65 | \\turl?: string // Optional - only MCPs have url\\n----\\n\\n# src/activate/handleUri.ts\\n 37 | \\t\\t}\\n 38 | \\t\\tcase \\\"/auth/clerk/callback\\\": {\\n 39 | \\t\\t\\tconst code = query.get(\\\"code\\\")\\n----\\n\\n# src/api/providers/bedrock.ts\\n235 | \\t\\t\\t\\tsecretAccessKey: this.options.awsSecretKey,\\n236 | \\t\\t\\t\\t...(this.options.awsSessionToken ? { sessionToken: this.options.awsSessionToken } : {}),\\n237 | \\t\\t\\t}\\n----\\n\\n# src/api/providers/gemini.ts\\n  7 | } from \\\"@google/genai\\\"\\n  8 | import type { JWTInput } from \\\"google-auth-library\\\"\\n  9 | \\n----\\n\\n# src/api/providers/__tests__/anthropic.spec.ts\\n112 | \\t\\t\\texpect(mockAnthropicConstructor.mock.calls[0]![0]!.apiKey).toEqual(\\\"test-api-key\\\")\\n113 | \\t\\t\\texpect(mockAnthropicConstructor.mock.calls[0]![0]!.authToken).toBeUndefined()\\n114 | \\t\\t})\\n----\\n123 | \\t\\t\\texpect(mockAnthropicConstructor.mock.calls[0]![0]!.apiKey).toEqual(\\\"test-api-key\\\")\\n124 | \\t\\t\\texpect(mockAnthropicConstructor.mock.calls[0]![0]!.authToken).toBeUndefined()\\n125 | \\t\\t})\\n126 | \\n127 | \\t\\tit(\\\"use authToken for passing token if both of anthropicBaseUrl and anthropicUseAuthToken are set\\\", () => {\\n128 | \\t\\t\\tconst customBaseUrl = \\\"https://custom.anthropic.com\\\"\\n----\\n135 | \\t\\t\\texpect(mockAnthropicConstructor).toHaveBeenCalledTimes(1)\\n136 | \\t\\t\\texpect(mockAnthropicConstructor.mock.calls[0]![0]!.authToken).toEqual(\\\"test-api-key\\\")\\n137 | \\t\\t\\texpect(mockAnthropicConstructor.mock.calls[0]![0]!.apiKey).toBeUndefined()\\n----\\n\\n# src/api/providers/anthropic.ts\\n 29 | \\t\\tconst apiKeyFieldName =\\n 30 | \\t\\t\\tthis.options.anthropicBaseUrl && this.options.anthropicUseAuthToken ? \\\"authToken\\\" : \\\"apiKey\\\"\\n 31 | \\n----\\n\\n# src/api/providers/fetchers/__tests__/litellm.test.ts\\n108 | \\n109 | \\tit(\\\"makes request without authorization header when no API key provided\\\", async () => {\\n110 | \\t\\tconst mockResponse = {\\n----\\n205 | \\t\\t\\t\\tstatus: 401,\\n206 | \\t\\t\\t\\tstatusText: \\\"Unauthorized\\\",\\n207 | \\t\\t\\t},\\n----\\n214 | \\t\\tawait expect(getLiteLLMModels(DUMMY_INVALID_KEY, \\\"http://localhost:4000\\\")).rejects.toThrow(\\n215 | \\t\\t\\t\\\"Failed to fetch LiteLLM models: 401 Unauthorized. Check base URL and API key.\\\",\\n216 | \\t\\t)\\n----\\n\\n# src/api/providers/anthropic-vertex.ts\\n  2 | import { AnthropicVertex } from \\\"@anthropic-ai/vertex-sdk\\\"\\n  3 | import { GoogleAuth, JWTInput } from \\\"google-auth-library\\\"\\n  4 | \\n----\\n 41 | \\t\\t\\t\\tgoogleAuth: new GoogleAuth({\\n 42 | \\t\\t\\t\\t\\tscopes: [\\\"https://www.googleapis.com/auth/cloud-platform\\\"],\\n 43 | \\t\\t\\t\\t\\tcredentials: safeJsonParse<JWTInput>(this.options.vertexJsonCredentials, undefined),\\n----\\n 50 | \\t\\t\\t\\tgoogleAuth: new GoogleAuth({\\n 51 | \\t\\t\\t\\t\\tscopes: [\\\"https://www.googleapis.com/auth/cloud-platform\\\"],\\n 52 | \\t\\t\\t\\t\\tkeyFile: this.options.vertexKeyFile,\\n----\\n\\n# src/shared/modes.ts\\n 81 | \\t\\tcustomInstructions:\\n 82 | \\t\\t\\t\\\"1. Do some information gathering (for example using read_file or search_files) to get more context about the task.\\\\n\\\\n2. You should also ask the user clarifying questions to get a better understanding of the task.\\\\n\\\\n3. Once you've gained more context about the user's request, you should create a detailed plan for how to accomplish the task. Include Mermaid diagrams if they help make your plan clearer.\\\\n\\\\n4. Ask the user if they are pleased with this plan, or if they would like to make any c [truncated...]\\n 83 | \\t},\\n----\\n\\n# src/utils/git.ts\\n 11 | \\tsubject: string\\n 12 | \\tauthor: string\\n 13 | \\tdate: string\\n----\\n 57 | \\t\\t\\tconst { stdout: hashStdout } = await execAsync(\\n 58 | \\t\\t\\t\\t`git log -n 10 --format=\\\"%H%n%h%n%s%n%an%n%ad\\\" --date=short ` + `--author-date-order ${query}`,\\n 59 | \\t\\t\\t\\t{ cwd },\\n----\\n 79 | \\t\\t\\t\\tsubject: lines[i + 2],\\n 80 | \\t\\t\\t\\tauthor: lines[i + 3],\\n 81 | \\t\\t\\t\\tdate: lines[i + 4],\\n----\\n107 | \\t\\t})\\n108 | \\t\\tconst [fullHash, shortHash, subject, author, date, body] = info.trim().split(\\\"\\\\n\\\")\\n109 | \\n----\\n115 | \\t\\t\\t`Commit: ${shortHash} (${fullHash})`,\\n116 | \\t\\t\\t`Author: ${author}`,\\n117 | \\t\\t\\t`Date: ${date}`,\\n----\\n\\n# src/utils/__tests__/git.spec.ts\\n 98 | \\t\\t\\t\\tsubject: \\\"fix: test commit\\\",\\n 99 | \\t\\t\\t\\tauthor: \\\"John Doe\\\",\\n100 | \\t\\t\\t\\tdate: \\\"2024-01-06\\\",\\n----\\n162 | \\t\\t\\t\\t[\\n163 | \\t\\t\\t\\t\\t'git log -n 10 --format=\\\"%H%n%h%n%s%n%an%n%ad\\\" --date=short --author-date-order abc123',\\n164 | \\t\\t\\t\\t\\t{ stdout: mockCommitData, stderr: \\\"\\\" },\\n----\\n185 | \\t\\t\\t\\tsubject: \\\"fix: test commit\\\",\\n186 | \\t\\t\\t\\tauthor: \\\"John Doe\\\",\\n187 | \\t\\t\\t\\tdate: \\\"2024-01-06\\\",\\n----\\n\\n# src/utils/logging/CompactTransport.ts\\n 36 | export class CompactTransport implements ICompactTransport {\\n 37 | \\tprivate sessionStart: number\\n 38 | \\tprivate lastTimestamp: number\\n----\\n 46 | \\tconstructor(readonly config: CompactTransportConfig = DEFAULT_CONFIG) {\\n 47 | \\t\\tthis.sessionStart = Date.now()\\n 48 | \\t\\tthis.lastTimestamp = this.sessionStart\\n 49 | \\n----\\n 55 | \\t/**\\n 56 | \\t * Ensures the log file is initialized with proper directory structure and session start marker\\n 57 | \\t * @private\\n----\\n 66 | \\n 67 | \\t\\t\\tconst sessionStart = {\\n 68 | \\t\\t\\t\\tt: 0,\\n 69 | \\t\\t\\t\\tl: \\\"info\\\",\\n 70 | \\t\\t\\t\\tm: \\\"Log session started\\\",\\n 71 | \\t\\t\\t\\td: { timestamp: new Date(this.sessionStart).toISOString() },\\n 72 | \\t\\t\\t}\\n 73 | \\t\\t\\twriteFileSync(this.filePath, JSON.stringify(sessionStart) + \\\"\\\\n\\\", { flag: \\\"w\\\" })\\n 74 | \\n----\\n108 | \\t/**\\n109 | \\t * Closes the transport and writes session end marker\\n110 | \\t */\\n----\\n112 | \\t\\tif (this.filePath && this.initialized) {\\n113 | \\t\\t\\tconst sessionEnd = {\\n114 | \\t\\t\\t\\tt: Date.now() - this.lastTimestamp,\\n115 | \\t\\t\\t\\tl: \\\"info\\\",\\n116 | \\t\\t\\t\\tm: \\\"Log session ended\\\",\\n117 | \\t\\t\\t\\td: { timestamp: new Date().toISOString() },\\n118 | \\t\\t\\t}\\n119 | \\t\\t\\twriteFileSync(this.filePath, JSON.stringify(sessionEnd) + \\\"\\\\n\\\", { flag: \\\"a\\\" })\\n120 | \\t\\t}\\n----\\n\\n# src/utils/logging/__tests__/CompactTransport.spec.ts\\n 73 | \\t\\t\\t\\tl: \\\"info\\\",\\n 74 | \\t\\t\\t\\tm: \\\"Log session started\\\",\\n 75 | \\t\\t\\t})\\n----\\n102 | \\n103 | \\t\\ttest(\\\"writes session end marker on close\\\", () => {\\n104 | \\t\\t\\ttransport.write({\\n----\\n117 | \\t\\t\\t\\tl: \\\"info\\\",\\n118 | \\t\\t\\t\\tm: \\\"Log session ended\\\",\\n119 | \\t\\t\\t})\\n----\\n171 | \\t\\t\\tconst lines = fileContent.trim().split(\\\"\\\\n\\\")\\n172 | \\t\\t\\t// +1 for session start line\\n173 | \\t\\t\\texpect(lines.length).toBe(entries.length + 1)\\n----\\n\\n# src/shared/context-mentions.ts\\n 72 | \\tsubject: string\\n 73 | \\tauthor: string\\n 74 | \\tdate: string\\n----\\n 80 | \\tsubject: string\\n 81 | \\tauthor: string\\n 82 | \\tdate: string\\n----\\n 86 | \\t\\tlabel: commit.subject,\\n 87 | \\t\\tdescription: `${commit.shortHash} by ${commit.author} on ${commit.date}`,\\n 88 | \\t\\tvalue: commit.hash,\\n----\\n 92 | \\t\\tsubject: commit.subject,\\n 93 | \\t\\tauthor: commit.author,\\n 94 | \\t\\tdate: commit.date,\\n----\\n\\n# src/shared/ExtensionMessage.ts\\n 71 | \\t\\t| \\\"vsCodeSetting\\\"\\n 72 | \\t\\t| \\\"authenticatedUser\\\"\\n 73 | \\t\\t| \\\"condenseTaskContextResponse\\\"\\n----\\n\\n# src/shared/WebviewMessage.ts\\n 85 | \\t\\t| \\\"restartMcpServer\\\"\\n 86 | \\t\\t| \\\"refreshAllMcpServers\\\"\\n 87 | \\t\\t| \\\"toggleToolAlwaysAllow\\\"\\n----\\n\\n# src/integrations/terminal/TerminalRegistry.ts\\n 14 | // too many terminals, we need to keep track of terminals through the life of\\n 15 | // the extension, as well as session specific terminals for the life of a task\\n 16 | // (to get latest unretrieved output).\\n----\\n\\n# src/integrations/terminal/ShellIntegrationManager.ts\\n 43 | \\t[ -f \\\"$ZDOTDIR/.zshrc\\\" ] && source \\\"$ZDOTDIR/.zshrc\\\"\\n 44 | \\t[ -f \\\"$ZDOTDIR/.zlogin\\\" ] && source \\\"$ZDOTDIR/.zlogin\\\"\\n 45 | \\t[ \\\"$ZDOTDIR\\\" = \\\"$HOME\\\" ] && unset ZDOTDIR\\n----\\n\\n# src/core/webview/ClineProvider.ts\\n497 | \\n498 | \\t\\t// If the extension is starting a new session, clear previous task state.\\n499 | \\t\\tawait this.removeClineFromStack()\\n----\\n656 | \\t\\t\\t<script nonce=\\\"${nonce}\\\" type=\\\"module\\\">\\n657 | \\t\\t\\t\\timport RefreshRuntime from \\\"http://localhost:${localPort}/@react-refresh\\\"\\n658 | \\t\\t\\t\\tRefreshRuntime.injectIntoGlobalHook(window)\\n----\\n668 | \\t\\t\\t`style-src ${webview.cspSource} 'unsafe-inline' https://* http://${localServerUrl} http://0.0.0.0:${localPort}`,\\n669 | \\t\\t\\t`img-src ${webview.cspSource} https://storage.googleapis.com https://img.clerk.com data:`,\\n670 | \\t\\t\\t`media-src ${webview.cspSource}`,\\n----\\n753 |             <meta name=\\\"theme-color\\\" content=\\\"#000000\\\">\\n754 |             <meta http-equiv=\\\"Content-Security-Policy\\\" content=\\\"default-src 'none'; font-src ${webview.cspSource}; style-src ${webview.cspSource} 'unsafe-inline'; img-src ${webview.cspSource} https://storage.googleapis.com https://img.clerk.com data:; media-src ${webview.cspSource}; script-src ${webview.cspSource} 'wasm-unsafe-eval' 'nonce-${nonce}' https://us-assets.i.posthog.com 'strict-dynamic'; connect-src https://openrouter.ai https://api.requesty.ai https://us.i.posthog.com https://us-asse [truncated...]\\n755 |             <link rel=\\\"stylesheet\\\" type=\\\"text/css\\\" href=\\\"${stylesUri}\\\">\\n----\\n1044 | \\t\\t\\tconst baseUrl = apiConfiguration.openRouterBaseUrl || \\\"https://openrouter.ai/api/v1\\\"\\n1045 | \\t\\t\\t// Extract the base domain for the auth endpoint\\n1046 | \\t\\t\\tconst baseUrlDomain = baseUrl.match(/^(https?:\\\\/\\\\/[^\\\\/]+)/)?.[1] || \\\"https://openrouter.ai\\\"\\n1047 | \\t\\t\\tconst response = await axios.post(`${baseUrlDomain}/api/v1/auth/keys`, { code })\\n1048 | \\t\\t\\tif (response.data && response.data.key) {\\n----\\n1074 | \\t\\ttry {\\n1075 | \\t\\t\\tconst response = await axios.post(\\\"https://glama.ai/api/gateway/v1/auth/exchange-code\\\", { code })\\n1076 | \\t\\t\\tif (response.data && response.data.apiKey) {\\n----\\n1507 | \\t\\t\\tconsole.error(\\n1508 | \\t\\t\\t\\t`[getState] failed to get cloud authentication state: ${error instanceof Error ? error.message : String(error)}`,\\n1509 | \\t\\t\\t)\\n----\\n\\n# src/core/prompts/tools/codebase-search.ts\\n 12 | \\n 13 | Example: Searching for functions related to user authentication\\n 14 | <codebase_search>\\n 15 | <query>User login and password hashing</query>\\n 16 | <path>/path/to/directory</path>\\n----\\n\\n# src/core/webview/webviewMessageHandler.ts\\n198 | \\t\\tcase \\\"clearTask\\\":\\n199 | \\t\\t\\t// clear task resets the current session and allows for a new task to be started, if this session is a subtask - it allows the parent task to be resumed\\n200 | \\t\\t\\tawait provider.finishSubTask(t(\\\"common:tasks.canceled\\\"))\\n----\\n597 | \\t\\t\\tbreak\\n598 | \\t\\tcase \\\"refreshAllMcpServers\\\": {\\n599 | \\t\\t\\tconst mcpHub = provider.getMcpHub()\\n600 | \\t\\t\\tif (mcpHub) {\\n601 | \\t\\t\\t\\tawait mcpHub.refreshAllConnections()\\n602 | \\t\\t\\t}\\n----\\n1389 | \\t\\t\\t\\tTelemetryService.instance.captureEvent(TelemetryEventName.AUTHENTICATION_INITIATED)\\n1390 | \\t\\t\\t\\tawait CloudService.instance.login()\\n1391 | \\t\\t\\t} catch (error) {\\n1392 | \\t\\t\\t\\tprovider.log(`AuthService#login failed: ${error}`)\\n1393 | \\t\\t\\t\\tvscode.window.showErrorMessage(\\\"Sign in failed.\\\")\\n----\\n1401 | \\t\\t\\t\\tawait provider.postStateToWebview()\\n1402 | \\t\\t\\t\\tprovider.postMessageToWebview({ type: \\\"authenticatedUser\\\", userInfo: undefined })\\n1403 | \\t\\t\\t} catch (error) {\\n----\\n\\n# src/core/prompts/instructions/create-mcp-server.ts\\n 11 | \\n 12 | When creating MCP servers, it's important to understand that they operate in a non-interactive environment. The server cannot initiate OAuth flows, open browser windows, or prompt for user input during runtime. All credentials and authentication tokens must be provided upfront through environment variables in the MCP settings configuration. For example, Spotify's API uses OAuth to get a refresh token for the user, but the MCP server cannot initiate this flow. While you can walk the user through  [truncated...]\\n 13 | \\n----\\n 76 | \\t\\t\\t\\t...\\n 77 | \\t\\t\\t\\t\\\"type\\\": \\\"module\\\", // added by default, uses ES module syntax (import/export) rather than CommonJS (require/module.exports) (Important to know if you create additional scripts in this server repository like a get-refresh-token.js script)\\n 78 | \\t\\t\\t\\t\\\"scripts\\\": {\\n----\\n\\n# src/core/task/Task.ts\\n716 | \\t\\t// we create a new Cline client (otherwise webview would show stale\\n717 | \\t\\t// messages from previous session).\\n718 | \\t\\tthis.clineMessages = []\\n----\\n1022 | \\t\\t} catch (error) {\\n1023 | \\t\\t\\tconsole.error(\\\"Error closing browser session:\\\", error)\\n1024 | \\t\\t}\\n----\\n\\n# src/core/mentions/__tests__/index.test.ts\\n  3 | \\tscheme,\\n  4 | \\tauthority: \\\"\\\",\\n  5 | \\tpath,\\n----\\n 12 | \\t\\tscheme,\\n 13 | \\t\\tauthority: \\\"\\\",\\n 14 | \\t\\tpath,\\n----\\n243 | \\t\\t\\t\\t\\tscheme: mockUri.scheme,\\n244 | \\t\\t\\t\\t\\tauthority: mockUri.authority,\\n245 | \\t\\t\\t\\t\\tpath: mockUri.path,\\n----\\n\\n# src/core/config/CustomModesManager.ts\\n213 | \\t\\t\\t\\troomodesWatcher.onDidDelete(async () => {\\n214 | \\t\\t\\t\\t\\t// When .roomodes is deleted, refresh with only settings modes\\n215 | \\t\\t\\t\\t\\ttry {\\n----\\n315 | \\t\\t\\t\\tthis.clearCache()\\n316 | \\t\\t\\t\\tawait this.refreshMergedState()\\n317 | \\t\\t\\t})\\n----\\n347 | \\n348 | \\tprivate async refreshMergedState(): Promise<void> {\\n349 | \\t\\tconst settingsPath = await this.getCustomModesFilePath()\\n----\\n391 | \\t\\t\\t\\tthis.clearCache()\\n392 | \\t\\t\\t\\tawait this.refreshMergedState()\\n393 | \\t\\t\\t})\\n----\\n\\n# src/core/config/__tests__/CustomModesManager.test.ts\\n396 | \\n397 | \\t\\tit(\\\"should refresh cache after TTL expires\\\", async () => {\\n398 | \\t\\t\\t// Setup test data\\n----\\n\\n# src/core/tools/browserActionTool.ts\\n183 | \\t} catch (error) {\\n184 | \\t\\tawait cline.browserSession.closeBrowser() // if any error occurs, the browser session is terminated\\n185 | \\t\\tawait handleError(\\\"executing browser action\\\", error)\\n----\"}",
  "partial": false
}
[2025-06-13T04:59:51.386Z] {
  "ts": 1749790791357,
  "type": "say",
  "say": "api_req_started",
  "text": "{\"request\":\"[search_files for 'oauth|auth|login|clerk|session|refresh' in '*.ts'] Result:\\n\\nFound 83 results.\\n\\n# src/services/ripgrep/index.ts\\n101 | \\t\\tconst rgProcess = childProcess.spawn(bin, args)\\n102 | \\t\\t// cross-platform alternative to head, which is ripgrep author's recommendation for limiting output.\\n103 | \\t\\tconst rl = readline.createInterface({\\n----\\n\\n# src/services/browser/BrowserSession.ts\\n543 | \\t\\t\\tconst [width, height] = size.split(\\\",\\\").map(Number)\\n544 | \\t\\t\\tconst session = await page.createCDPSession()\\n545 | \\t\\t\\tawait page.setViewport({ width, height })\\n546 | \\t\\t\\tconst { windowId } = await session.send(\\\"Browser.getWindowForTarget\\\")\\n547 | \\t\\t\\tawait session.send(\\\"Browser.setWindowBounds\\\", {\\n548 | \\t\\t\\t\\tbounds: { width, height },\\n----\\n\\n# src/services/mcp/__tests__/McpHub.test.ts\\n 50 | \\t\\t\\tscheme: \\\"file\\\",\\n 51 | \\t\\t\\tauthority: \\\"\\\",\\n 52 | \\t\\t\\tpath: \\\"/test/path\\\",\\n----\\n\\n# src/services/mcp/McpHub.ts\\n1092 | \\n1093 | \\tpublic async refreshAllConnections(): Promise<void> {\\n1094 | \\t\\tif (this.isConnecting) {\\n1095 | \\t\\t\\tvscode.window.showInformationMessage(t(\\\"mcp:info.already_refreshing\\\"))\\n1096 | \\t\\t\\treturn\\n----\\n1099 | \\t\\tthis.isConnecting = true\\n1100 | \\t\\tvscode.window.showInformationMessage(t(\\\"mcp:info.refreshing_all\\\"))\\n1101 | \\n----\\n1151 | \\n1152 | \\t\\t\\tvscode.window.showInformationMessage(t(\\\"mcp:info.all_refreshed\\\"))\\n1153 | \\t\\t} catch (error) {\\n1154 | \\t\\t\\tthis.showErrorMessage(\\\"Failed to refresh MCP servers\\\", error)\\n1155 | \\t\\t} finally {\\n----\\n1244 | \\n1245 | \\t\\t\\t\\t\\t// Only refresh capabilities if connected\\n1246 | \\t\\t\\t\\t\\tif (connection.server.status === \\\"connected\\\") {\\n----\\n1254 | \\t\\t\\t\\t} catch (error) {\\n1255 | \\t\\t\\t\\t\\tconsole.error(`Failed to refresh capabilities for ${serverName}:`, error)\\n1256 | \\t\\t\\t\\t}\\n----\\n\\n# src/services/code-index/embedders/__tests__/openai-compatible.spec.ts\\n375 | \\t\\t\\t\\tconst testTexts = [\\\"Hello world\\\"]\\n376 | \\t\\t\\t\\tconst authError = new Error(\\\"Unauthorized\\\")\\n377 | \\t\\t\\t\\t;(authError as any).status = 401\\n378 | \\n379 | \\t\\t\\t\\tmockEmbeddingsCreate.mockRejectedValue(authError)\\n380 | \\n----\\n\\n# src/services/code-index/embedders/openai-compatible.ts\\n 34 | \\t * @param baseUrl The base URL for the OpenAI-compatible API endpoint\\n 35 | \\t * @param apiKey The API key for authentication\\n 36 | \\t * @param modelId Optional model identifier (defaults to \\\"text-embedding-3-small\\\")\\n----\\n\\n# src/services/marketplace/schemas.ts\\n 35 | \\ttype: marketplaceItemTypeSchema,\\n 36 | \\tauthor: z.string().optional(),\\n 37 | \\tauthorUrl: z.string().url(\\\"Author URL must be a valid URL\\\").optional(),\\n 38 | \\ttags: z.array(z.string()).optional(),\\n----\\n 61 | \\tdescription: z.string(),\\n 62 | \\tauthor: z.string().optional(),\\n 63 | \\tauthorUrl: z.string().url().optional(),\\n 64 | \\ttags: z.array(z.string()).optional(),\\n----\\n 72 | \\tdescription: z.string(),\\n 73 | \\tauthor: z.string().optional(),\\n 74 | \\tauthorUrl: z.string().url().optional(),\\n 75 | \\turl: z.string().url(), // Required url field\\n----\\n\\n# src/services/tree-sitter/__tests__/fixtures/sample-embedded_template.ts\\n 35 | <div class=\\\"container\\\">\\n 36 |   <% if user.authenticated? %>\\n 37 |     <h1>Welcome, <%= user.name %></h1>\\n----\\n\\n# src/services/tree-sitter/__tests__/parseSourceCodeDefinitions.embedded_template.test.ts\\n 43 | \\tit(\\\"should detect control structures\\\", () => {\\n 44 | \\t\\texpect(parseResult).toMatch(/\\\\d+--\\\\d+ \\\\|\\\\s+<% if user\\\\.authenticated\\\\? %>/)\\n 45 | \\t\\texpect(parseResult).toMatch(/\\\\d+--\\\\d+ \\\\|\\\\s+<% user\\\\.posts\\\\.each do \\\\|post\\\\| %>/)\\n----\\n\\n# src/services/marketplace/types.ts\\n 20 | \\tdescription: string\\n 21 | \\tauthor?: string\\n 22 | \\tauthorUrl?: string\\n 23 | \\ttags?: string[]\\n----\\n 45 | \\tdescription: string\\n 46 | \\tauthor?: string\\n 47 | \\tauthorUrl?: string\\n 48 | \\turl: string // Required url field\\n----\\n 62 | \\ttype: MarketplaceItemType\\n 63 | \\tauthor?: string\\n 64 | \\tauthorUrl?: string\\n 65 | \\turl?: string // Optional - only MCPs have url\\n----\\n\\n# src/activate/handleUri.ts\\n 37 | \\t\\t}\\n 38 | \\t\\tcase \\\"/auth/clerk/callback\\\": {\\n 39 | \\t\\t\\tconst code = query.get(\\\"code\\\")\\n----\\n\\n# src/api/providers/bedrock.ts\\n235 | \\t\\t\\t\\tsecretAccessKey: this.options.awsSecretKey,\\n236 | \\t\\t\\t\\t...(this.options.awsSessionToken ? { sessionToken: this.options.awsSessionToken } : {}),\\n237 | \\t\\t\\t}\\n----\\n\\n# src/api/providers/gemini.ts\\n  7 | } from \\\"@google/genai\\\"\\n  8 | import type { JWTInput } from \\\"google-auth-library\\\"\\n  9 | \\n----\\n\\n# src/api/providers/__tests__/anthropic.spec.ts\\n112 | \\t\\t\\texpect(mockAnthropicConstructor.mock.calls[0]![0]!.apiKey).toEqual(\\\"test-api-key\\\")\\n113 | \\t\\t\\texpect(mockAnthropicConstructor.mock.calls[0]![0]!.authToken).toBeUndefined()\\n114 | \\t\\t})\\n----\\n123 | \\t\\t\\texpect(mockAnthropicConstructor.mock.calls[0]![0]!.apiKey).toEqual(\\\"test-api-key\\\")\\n124 | \\t\\t\\texpect(mockAnthropicConstructor.mock.calls[0]![0]!.authToken).toBeUndefined()\\n125 | \\t\\t})\\n126 | \\n127 | \\t\\tit(\\\"use authToken for passing token if both of anthropicBaseUrl and anthropicUseAuthToken are set\\\", () => {\\n128 | \\t\\t\\tconst customBaseUrl = \\\"https://custom.anthropic.com\\\"\\n----\\n135 | \\t\\t\\texpect(mockAnthropicConstructor).toHaveBeenCalledTimes(1)\\n136 | \\t\\t\\texpect(mockAnthropicConstructor.mock.calls[0]![0]!.authToken).toEqual(\\\"test-api-key\\\")\\n137 | \\t\\t\\texpect(mockAnthropicConstructor.mock.calls[0]![0]!.apiKey).toBeUndefined()\\n----\\n\\n# src/api/providers/anthropic.ts\\n 29 | \\t\\tconst apiKeyFieldName =\\n 30 | \\t\\t\\tthis.options.anthropicBaseUrl && this.options.anthropicUseAuthToken ? \\\"authToken\\\" : \\\"apiKey\\\"\\n 31 | \\n----\\n\\n# src/api/providers/fetchers/__tests__/litellm.test.ts\\n108 | \\n109 | \\tit(\\\"makes request without authorization header when no API key provided\\\", async () => {\\n110 | \\t\\tconst mockResponse = {\\n----\\n205 | \\t\\t\\t\\tstatus: 401,\\n206 | \\t\\t\\t\\tstatusText: \\\"Unauthorized\\\",\\n207 | \\t\\t\\t},\\n----\\n214 | \\t\\tawait expect(getLiteLLMModels(DUMMY_INVALID_KEY, \\\"http://localhost:4000\\\")).rejects.toThrow(\\n215 | \\t\\t\\t\\\"Failed to fetch LiteLLM models: 401 Unauthorized. Check base URL and API key.\\\",\\n216 | \\t\\t)\\n----\\n\\n# src/api/providers/anthropic-vertex.ts\\n  2 | import { AnthropicVertex } from \\\"@anthropic-ai/vertex-sdk\\\"\\n  3 | import { GoogleAuth, JWTInput } from \\\"google-auth-library\\\"\\n  4 | \\n----\\n 41 | \\t\\t\\t\\tgoogleAuth: new GoogleAuth({\\n 42 | \\t\\t\\t\\t\\tscopes: [\\\"https://www.googleapis.com/auth/cloud-platform\\\"],\\n 43 | \\t\\t\\t\\t\\tcredentials: safeJsonParse<JWTInput>(this.options.vertexJsonCredentials, undefined),\\n----\\n 50 | \\t\\t\\t\\tgoogleAuth: new GoogleAuth({\\n 51 | \\t\\t\\t\\t\\tscopes: [\\\"https://www.googleapis.com/auth/cloud-platform\\\"],\\n 52 | \\t\\t\\t\\t\\tkeyFile: this.options.vertexKeyFile,\\n----\\n\\n# src/shared/modes.ts\\n 81 | \\t\\tcustomInstructions:\\n 82 | \\t\\t\\t\\\"1. Do some information gathering (for example using read_file or search_files) to get more context about the task.\\\\n\\\\n2. You should also ask the user clarifying questions to get a better understanding of the task.\\\\n\\\\n3. Once you've gained more context about the user's request, you should create a detailed plan for how to accomplish the task. Include Mermaid diagrams if they help make your plan clearer.\\\\n\\\\n4. Ask the user if they are pleased with this plan, or if they would like to make any c [truncated...]\\n 83 | \\t},\\n----\\n\\n# src/utils/git.ts\\n 11 | \\tsubject: string\\n 12 | \\tauthor: string\\n 13 | \\tdate: string\\n----\\n 57 | \\t\\t\\tconst { stdout: hashStdout } = await execAsync(\\n 58 | \\t\\t\\t\\t`git log -n 10 --format=\\\"%H%n%h%n%s%n%an%n%ad\\\" --date=short ` + `--author-date-order ${query}`,\\n 59 | \\t\\t\\t\\t{ cwd },\\n----\\n 79 | \\t\\t\\t\\tsubject: lines[i + 2],\\n 80 | \\t\\t\\t\\tauthor: lines[i + 3],\\n 81 | \\t\\t\\t\\tdate: lines[i + 4],\\n----\\n107 | \\t\\t})\\n108 | \\t\\tconst [fullHash, shortHash, subject, author, date, body] = info.trim().split(\\\"\\\\n\\\")\\n109 | \\n----\\n115 | \\t\\t\\t`Commit: ${shortHash} (${fullHash})`,\\n116 | \\t\\t\\t`Author: ${author}`,\\n117 | \\t\\t\\t`Date: ${date}`,\\n----\\n\\n# src/utils/__tests__/git.spec.ts\\n 98 | \\t\\t\\t\\tsubject: \\\"fix: test commit\\\",\\n 99 | \\t\\t\\t\\tauthor: \\\"John Doe\\\",\\n100 | \\t\\t\\t\\tdate: \\\"2024-01-06\\\",\\n----\\n162 | \\t\\t\\t\\t[\\n163 | \\t\\t\\t\\t\\t'git log -n 10 --format=\\\"%H%n%h%n%s%n%an%n%ad\\\" --date=short --author-date-order abc123',\\n164 | \\t\\t\\t\\t\\t{ stdout: mockCommitData, stderr: \\\"\\\" },\\n----\\n185 | \\t\\t\\t\\tsubject: \\\"fix: test commit\\\",\\n186 | \\t\\t\\t\\tauthor: \\\"John Doe\\\",\\n187 | \\t\\t\\t\\tdate: \\\"2024-01-06\\\",\\n----\\n\\n# src/utils/logging/CompactTransport.ts\\n 36 | export class CompactTransport implements ICompactTransport {\\n 37 | \\tprivate sessionStart: number\\n 38 | \\tprivate lastTimestamp: number\\n----\\n 46 | \\tconstructor(readonly config: CompactTransportConfig = DEFAULT_CONFIG) {\\n 47 | \\t\\tthis.sessionStart = Date.now()\\n 48 | \\t\\tthis.lastTimestamp = this.sessionStart\\n 49 | \\n----\\n 55 | \\t/**\\n 56 | \\t * Ensures the log file is initialized with proper directory structure and session start marker\\n 57 | \\t * @private\\n----\\n 66 | \\n 67 | \\t\\t\\tconst sessionStart = {\\n 68 | \\t\\t\\t\\tt: 0,\\n 69 | \\t\\t\\t\\tl: \\\"info\\\",\\n 70 | \\t\\t\\t\\tm: \\\"Log session started\\\",\\n 71 | \\t\\t\\t\\td: { timestamp: new Date(this.sessionStart).toISOString() },\\n 72 | \\t\\t\\t}\\n 73 | \\t\\t\\twriteFileSync(this.filePath, JSON.stringify(sessionStart) + \\\"\\\\n\\\", { flag: \\\"w\\\" })\\n 74 | \\n----\\n108 | \\t/**\\n109 | \\t * Closes the transport and writes session end marker\\n110 | \\t */\\n----\\n112 | \\t\\tif (this.filePath && this.initialized) {\\n113 | \\t\\t\\tconst sessionEnd = {\\n114 | \\t\\t\\t\\tt: Date.now() - this.lastTimestamp,\\n115 | \\t\\t\\t\\tl: \\\"info\\\",\\n116 | \\t\\t\\t\\tm: \\\"Log session ended\\\",\\n117 | \\t\\t\\t\\td: { timestamp: new Date().toISOString() },\\n118 | \\t\\t\\t}\\n119 | \\t\\t\\twriteFileSync(this.filePath, JSON.stringify(sessionEnd) + \\\"\\\\n\\\", { flag: \\\"a\\\" })\\n120 | \\t\\t}\\n----\\n\\n# src/utils/logging/__tests__/CompactTransport.spec.ts\\n 73 | \\t\\t\\t\\tl: \\\"info\\\",\\n 74 | \\t\\t\\t\\tm: \\\"Log session started\\\",\\n 75 | \\t\\t\\t})\\n----\\n102 | \\n103 | \\t\\ttest(\\\"writes session end marker on close\\\", () => {\\n104 | \\t\\t\\ttransport.write({\\n----\\n117 | \\t\\t\\t\\tl: \\\"info\\\",\\n118 | \\t\\t\\t\\tm: \\\"Log session ended\\\",\\n119 | \\t\\t\\t})\\n----\\n171 | \\t\\t\\tconst lines = fileContent.trim().split(\\\"\\\\n\\\")\\n172 | \\t\\t\\t// +1 for session start line\\n173 | \\t\\t\\texpect(lines.length).toBe(entries.length + 1)\\n----\\n\\n# src/shared/context-mentions.ts\\n 72 | \\tsubject: string\\n 73 | \\tauthor: string\\n 74 | \\tdate: string\\n----\\n 80 | \\tsubject: string\\n 81 | \\tauthor: string\\n 82 | \\tdate: string\\n----\\n 86 | \\t\\tlabel: commit.subject,\\n 87 | \\t\\tdescription: `${commit.shortHash} by ${commit.author} on ${commit.date}`,\\n 88 | \\t\\tvalue: commit.hash,\\n----\\n 92 | \\t\\tsubject: commit.subject,\\n 93 | \\t\\tauthor: commit.author,\\n 94 | \\t\\tdate: commit.date,\\n----\\n\\n# src/shared/ExtensionMessage.ts\\n 71 | \\t\\t| \\\"vsCodeSetting\\\"\\n 72 | \\t\\t| \\\"authenticatedUser\\\"\\n 73 | \\t\\t| \\\"condenseTaskContextResponse\\\"\\n----\\n\\n# src/shared/WebviewMessage.ts\\n 85 | \\t\\t| \\\"restartMcpServer\\\"\\n 86 | \\t\\t| \\\"refreshAllMcpServers\\\"\\n 87 | \\t\\t| \\\"toggleToolAlwaysAllow\\\"\\n----\\n\\n# src/integrations/terminal/TerminalRegistry.ts\\n 14 | // too many terminals, we need to keep track of terminals through the life of\\n 15 | // the extension, as well as session specific terminals for the life of a task\\n 16 | // (to get latest unretrieved output).\\n----\\n\\n# src/integrations/terminal/ShellIntegrationManager.ts\\n 43 | \\t[ -f \\\"$ZDOTDIR/.zshrc\\\" ] && source \\\"$ZDOTDIR/.zshrc\\\"\\n 44 | \\t[ -f \\\"$ZDOTDIR/.zlogin\\\" ] && source \\\"$ZDOTDIR/.zlogin\\\"\\n 45 | \\t[ \\\"$ZDOTDIR\\\" = \\\"$HOME\\\" ] && unset ZDOTDIR\\n----\\n\\n# src/core/webview/ClineProvider.ts\\n497 | \\n498 | \\t\\t// If the extension is starting a new session, clear previous task state.\\n499 | \\t\\tawait this.removeClineFromStack()\\n----\\n656 | \\t\\t\\t<script nonce=\\\"${nonce}\\\" type=\\\"module\\\">\\n657 | \\t\\t\\t\\timport RefreshRuntime from \\\"http://localhost:${localPort}/@react-refresh\\\"\\n658 | \\t\\t\\t\\tRefreshRuntime.injectIntoGlobalHook(window)\\n----\\n668 | \\t\\t\\t`style-src ${webview.cspSource} 'unsafe-inline' https://* http://${localServerUrl} http://0.0.0.0:${localPort}`,\\n669 | \\t\\t\\t`img-src ${webview.cspSource} https://storage.googleapis.com https://img.clerk.com data:`,\\n670 | \\t\\t\\t`media-src ${webview.cspSource}`,\\n----\\n753 |             <meta name=\\\"theme-color\\\" content=\\\"#000000\\\">\\n754 |             <meta http-equiv=\\\"Content-Security-Policy\\\" content=\\\"default-src 'none'; font-src ${webview.cspSource}; style-src ${webview.cspSource} 'unsafe-inline'; img-src ${webview.cspSource} https://storage.googleapis.com https://img.clerk.com data:; media-src ${webview.cspSource}; script-src ${webview.cspSource} 'wasm-unsafe-eval' 'nonce-${nonce}' https://us-assets.i.posthog.com 'strict-dynamic'; connect-src https://openrouter.ai https://api.requesty.ai https://us.i.posthog.com https://us-asse [truncated...]\\n755 |             <link rel=\\\"stylesheet\\\" type=\\\"text/css\\\" href=\\\"${stylesUri}\\\">\\n----\\n1044 | \\t\\t\\tconst baseUrl = apiConfiguration.openRouterBaseUrl || \\\"https://openrouter.ai/api/v1\\\"\\n1045 | \\t\\t\\t// Extract the base domain for the auth endpoint\\n1046 | \\t\\t\\tconst baseUrlDomain = baseUrl.match(/^(https?:\\\\/\\\\/[^\\\\/]+)/)?.[1] || \\\"https://openrouter.ai\\\"\\n1047 | \\t\\t\\tconst response = await axios.post(`${baseUrlDomain}/api/v1/auth/keys`, { code })\\n1048 | \\t\\t\\tif (response.data && response.data.key) {\\n----\\n1074 | \\t\\ttry {\\n1075 | \\t\\t\\tconst response = await axios.post(\\\"https://glama.ai/api/gateway/v1/auth/exchange-code\\\", { code })\\n1076 | \\t\\t\\tif (response.data && response.data.apiKey) {\\n----\\n1507 | \\t\\t\\tconsole.error(\\n1508 | \\t\\t\\t\\t`[getState] failed to get cloud authentication state: ${error instanceof Error ? error.message : String(error)}`,\\n1509 | \\t\\t\\t)\\n----\\n\\n# src/core/prompts/tools/codebase-search.ts\\n 12 | \\n 13 | Example: Searching for functions related to user authentication\\n 14 | <codebase_search>\\n 15 | <query>User login and password hashing</query>\\n 16 | <path>/path/to/directory</path>\\n----\\n\\n# src/core/webview/webviewMessageHandler.ts\\n198 | \\t\\tcase \\\"clearTask\\\":\\n199 | \\t\\t\\t// clear task resets the current session and allows for a new task to be started, if this session is a subtask - it allows the parent task to be resumed\\n200 | \\t\\t\\tawait provider.finishSubTask(t(\\\"common:tasks.canceled\\\"))\\n----\\n597 | \\t\\t\\tbreak\\n598 | \\t\\tcase \\\"refreshAllMcpServers\\\": {\\n599 | \\t\\t\\tconst mcpHub = provider.getMcpHub()\\n600 | \\t\\t\\tif (mcpHub) {\\n601 | \\t\\t\\t\\tawait mcpHub.refreshAllConnections()\\n602 | \\t\\t\\t}\\n----\\n1389 | \\t\\t\\t\\tTelemetryService.instance.captureEvent(TelemetryEventName.AUTHENTICATION_INITIATED)\\n1390 | \\t\\t\\t\\tawait CloudService.instance.login()\\n1391 | \\t\\t\\t} catch (error) {\\n1392 | \\t\\t\\t\\tprovider.log(`AuthService#login failed: ${error}`)\\n1393 | \\t\\t\\t\\tvscode.window.showErrorMessage(\\\"Sign in failed.\\\")\\n----\\n1401 | \\t\\t\\t\\tawait provider.postStateToWebview()\\n1402 | \\t\\t\\t\\tprovider.postMessageToWebview({ type: \\\"authenticatedUser\\\", userInfo: undefined })\\n1403 | \\t\\t\\t} catch (error) {\\n----\\n\\n# src/core/prompts/instructions/create-mcp-server.ts\\n 11 | \\n 12 | When creating MCP servers, it's important to understand that they operate in a non-interactive environment. The server cannot initiate OAuth flows, open browser windows, or prompt for user input during runtime. All credentials and authentication tokens must be provided upfront through environment variables in the MCP settings configuration. For example, Spotify's API uses OAuth to get a refresh token for the user, but the MCP server cannot initiate this flow. While you can walk the user through  [truncated...]\\n 13 | \\n----\\n 76 | \\t\\t\\t\\t...\\n 77 | \\t\\t\\t\\t\\\"type\\\": \\\"module\\\", // added by default, uses ES module syntax (import/export) rather than CommonJS (require/module.exports) (Important to know if you create additional scripts in this server repository like a get-refresh-token.js script)\\n 78 | \\t\\t\\t\\t\\\"scripts\\\": {\\n----\\n\\n# src/core/task/Task.ts\\n716 | \\t\\t// we create a new Cline client (otherwise webview would show stale\\n717 | \\t\\t// messages from previous session).\\n718 | \\t\\tthis.clineMessages = []\\n----\\n1022 | \\t\\t} catch (error) {\\n1023 | \\t\\t\\tconsole.error(\\\"Error closing browser session:\\\", error)\\n1024 | \\t\\t}\\n----\\n\\n# src/core/mentions/__tests__/index.test.ts\\n  3 | \\tscheme,\\n  4 | \\tauthority: \\\"\\\",\\n  5 | \\tpath,\\n----\\n 12 | \\t\\tscheme,\\n 13 | \\t\\tauthority: \\\"\\\",\\n 14 | \\t\\tpath,\\n----\\n243 | \\t\\t\\t\\t\\tscheme: mockUri.scheme,\\n244 | \\t\\t\\t\\t\\tauthority: mockUri.authority,\\n245 | \\t\\t\\t\\t\\tpath: mockUri.path,\\n----\\n\\n# src/core/config/CustomModesManager.ts\\n213 | \\t\\t\\t\\troomodesWatcher.onDidDelete(async () => {\\n214 | \\t\\t\\t\\t\\t// When .roomodes is deleted, refresh with only settings modes\\n215 | \\t\\t\\t\\t\\ttry {\\n----\\n315 | \\t\\t\\t\\tthis.clearCache()\\n316 | \\t\\t\\t\\tawait this.refreshMergedState()\\n317 | \\t\\t\\t})\\n----\\n347 | \\n348 | \\tprivate async refreshMergedState(): Promise<void> {\\n349 | \\t\\tconst settingsPath = await this.getCustomModesFilePath()\\n----\\n391 | \\t\\t\\t\\tthis.clearCache()\\n392 | \\t\\t\\t\\tawait this.refreshMergedState()\\n393 | \\t\\t\\t})\\n----\\n\\n# src/core/config/__tests__/CustomModesManager.test.ts\\n396 | \\n397 | \\t\\tit(\\\"should refresh cache after TTL expires\\\", async () => {\\n398 | \\t\\t\\t// Setup test data\\n----\\n\\n# src/core/tools/browserActionTool.ts\\n183 | \\t} catch (error) {\\n184 | \\t\\tawait cline.browserSession.closeBrowser() // if any error occurs, the browser session is terminated\\n185 | \\t\\tawait handleError(\\\"executing browser action\\\", error)\\n----\\n\\nLoading...\"}"
}
[2025-06-13T04:59:54.932Z] {
  "ts": 1749790793949,
  "type": "say",
  "say": "text",
  "text": "Now I can see some authentication-related code! I found references to Clerk authentication in the URI handler and webview provider. Let me examine the authentication implementation more closely.",
  "partial": false
}
[2025-06-13T04:59:56.202Z] {
  "ts": 1749790794931,
  "type": "ask",
  "ask": "tool",
  "text": "{\"tool\":\"readFile\",\"batchFiles\":[{\"path\":\"src/activate/handleUri.ts\",\"lineSnippet\":\"\",\"isOutsideWorkspace\":false,\"key\":\"src/activate/handleUri.ts\",\"content\":\"/Users/cte/Documents/Roo-Code-Cloud-Agents/src/activate/handleUri.ts\"},{\"path\":\"src/core/webview/ClineProvider.ts\",\"lineSnippet\":\"\",\"isOutsideWorkspace\":false,\"key\":\"src/core/webview/ClineProvider.ts\",\"content\":\"/Users/cte/Documents/Roo-Code-Cloud-Agents/src/core/webview/ClineProvider.ts\"},{\"path\":\"src/core/webview/webviewMessageHandler.ts\",\"lineSnippet\":\"\",\"isOutsideWorkspace\":false,\"key\":\"src/core/webview/webviewMessageHandler.ts\",\"content\":\"/Users/cte/Documents/Roo-Code-Cloud-Agents/src/core/webview/webviewMessageHandler.ts\"}]}",
  "partial": false
}
[2025-06-13T04:59:56.411Z] {
  "ts": 1749790796380,
  "type": "say",
  "say": "api_req_started",
  "text": "{\"request\":\"[read_file for 'src/activate/handleUri.ts', 'src/core/webview/ClineProvider.ts', 'src/core/webview/webviewMessageHandler.ts'] Result:\\n\\n<files>\\n<file><path>src/activate/handleUri.ts</path>\\n<content lines=\\\"1-47\\\">\\n 1 | import * as vscode from \\\"vscode\\\"\\n 2 | \\n 3 | import { CloudService } from \\\"@roo-code/cloud\\\"\\n 4 | \\n 5 | import { ClineProvider } from \\\"../core/webview/ClineProvider\\\"\\n 6 | \\n 7 | export const handleUri = async (uri: vscode.Uri) => {\\n 8 | \\tconst path = uri.path\\n 9 | \\tconst query = new URLSearchParams(uri.query.replace(/\\\\+/g, \\\"%2B\\\"))\\n10 | \\tconst visibleProvider = ClineProvider.getVisibleInstance()\\n11 | \\n12 | \\tif (!visibleProvider) {\\n13 | \\t\\treturn\\n14 | \\t}\\n15 | \\n16 | \\tswitch (path) {\\n17 | \\t\\tcase \\\"/glama\\\": {\\n18 | \\t\\t\\tconst code = query.get(\\\"code\\\")\\n19 | \\t\\t\\tif (code) {\\n20 | \\t\\t\\t\\tawait visibleProvider.handleGlamaCallback(code)\\n21 | \\t\\t\\t}\\n22 | \\t\\t\\tbreak\\n23 | \\t\\t}\\n24 | \\t\\tcase \\\"/openrouter\\\": {\\n25 | \\t\\t\\tconst code = query.get(\\\"code\\\")\\n26 | \\t\\t\\tif (code) {\\n27 | \\t\\t\\t\\tawait visibleProvider.handleOpenRouterCallback(code)\\n28 | \\t\\t\\t}\\n29 | \\t\\t\\tbreak\\n30 | \\t\\t}\\n31 | \\t\\tcase \\\"/requesty\\\": {\\n32 | \\t\\t\\tconst code = query.get(\\\"code\\\")\\n33 | \\t\\t\\tif (code) {\\n34 | \\t\\t\\t\\tawait visibleProvider.handleRequestyCallback(code)\\n35 | \\t\\t\\t}\\n36 | \\t\\t\\tbreak\\n37 | \\t\\t}\\n38 | \\t\\tcase \\\"/auth/clerk/callback\\\": {\\n39 | \\t\\t\\tconst code = query.get(\\\"code\\\")\\n40 | \\t\\t\\tconst state = query.get(\\\"state\\\")\\n41 | \\t\\t\\tawait CloudService.instance.handleAuthCallback(code, state)\\n42 | \\t\\t\\tbreak\\n43 | \\t\\t}\\n44 | \\t\\tdefault:\\n45 | \\t\\t\\tbreak\\n46 | \\t}\\n47 | }\\n</content>\\n</file>\\n<file><path>src/core/webview/ClineProvider.ts</path>\\n<content lines=\\\"1-1725\\\">\\n   1 | import os from \\\"os\\\"\\n   2 | import * as path from \\\"path\\\"\\n   3 | import fs from \\\"fs/promises\\\"\\n   4 | import EventEmitter from \\\"events\\\"\\n   5 | \\n   6 | import { Anthropic } from \\\"@anthropic-ai/sdk\\\"\\n   7 | import delay from \\\"delay\\\"\\n   8 | import axios from \\\"axios\\\"\\n   9 | import pWaitFor from \\\"p-wait-for\\\"\\n  10 | import * as vscode from \\\"vscode\\\"\\n  11 | \\n  12 | import {\\n  13 | \\ttype GlobalState,\\n  14 | \\ttype ProviderName,\\n  15 | \\ttype ProviderSettings,\\n  16 | \\ttype RooCodeSettings,\\n  17 | \\ttype ProviderSettingsEntry,\\n  18 | \\ttype TelemetryProperties,\\n  19 | \\ttype TelemetryPropertiesProvider,\\n  20 | \\ttype CodeActionId,\\n  21 | \\ttype CodeActionName,\\n  22 | \\ttype TerminalActionId,\\n  23 | \\ttype TerminalActionPromptType,\\n  24 | \\ttype HistoryItem,\\n  25 | \\ttype CloudUserInfo,\\n  26 | \\trequestyDefaultModelId,\\n  27 | \\topenRouterDefaultModelId,\\n  28 | \\tglamaDefaultModelId,\\n  29 | \\tORGANIZATION_ALLOW_ALL,\\n  30 | } from \\\"@roo-code/types\\\"\\n  31 | import { TelemetryService } from \\\"@roo-code/telemetry\\\"\\n  32 | import { CloudService } from \\\"@roo-code/cloud\\\"\\n  33 | \\n  34 | import { t } from \\\"../../i18n\\\"\\n  35 | import { setPanel } from \\\"../../activate/registerCommands\\\"\\n  36 | import { Package } from \\\"../../shared/package\\\"\\n  37 | import { findLast } from \\\"../../shared/array\\\"\\n  38 | import { supportPrompt } from \\\"../../shared/support-prompt\\\"\\n  39 | import { GlobalFileNames } from \\\"../../shared/globalFileNames\\\"\\n  40 | import { ExtensionMessage } from \\\"../../shared/ExtensionMessage\\\"\\n  41 | import { Mode, defaultModeSlug } from \\\"../../shared/modes\\\"\\n  42 | import { experimentDefault, experiments, EXPERIMENT_IDS } from \\\"../../shared/experiments\\\"\\n  43 | import { formatLanguage } from \\\"../../shared/language\\\"\\n  44 | import { Terminal } from \\\"../../integrations/terminal/Terminal\\\"\\n  45 | import { downloadTask } from \\\"../../integrations/misc/export-markdown\\\"\\n  46 | import { getTheme } from \\\"../../integrations/theme/getTheme\\\"\\n  47 | import WorkspaceTracker from \\\"../../integrations/workspace/WorkspaceTracker\\\"\\n  48 | import { McpHub } from \\\"../../services/mcp/McpHub\\\"\\n  49 | import { McpServerManager } from \\\"../../services/mcp/McpServerManager\\\"\\n  50 | import { MarketplaceManager } from \\\"../../services/marketplace\\\"\\n  51 | import { ShadowCheckpointService } from \\\"../../services/checkpoints/ShadowCheckpointService\\\"\\n  52 | import { CodeIndexManager } from \\\"../../services/code-index/manager\\\"\\n  53 | import type { IndexProgressUpdate } from \\\"../../services/code-index/interfaces/manager\\\"\\n  54 | import { fileExistsAtPath } from \\\"../../utils/fs\\\"\\n  55 | import { setTtsEnabled, setTtsSpeed } from \\\"../../utils/tts\\\"\\n  56 | import { ContextProxy } from \\\"../config/ContextProxy\\\"\\n  57 | import { ProviderSettingsManager } from \\\"../config/ProviderSettingsManager\\\"\\n  58 | import { CustomModesManager } from \\\"../config/CustomModesManager\\\"\\n  59 | import { buildApiHandler } from \\\"../../api\\\"\\n  60 | import { Task, TaskOptions } from \\\"../task/Task\\\"\\n  61 | import { getNonce } from \\\"./getNonce\\\"\\n  62 | import { getUri } from \\\"./getUri\\\"\\n  63 | import { getSystemPromptFilePath } from \\\"../prompts/sections/custom-system-prompt\\\"\\n  64 | import { getWorkspacePath } from \\\"../../utils/path\\\"\\n  65 | import { webviewMessageHandler } from \\\"./webviewMessageHandler\\\"\\n  66 | import { WebviewMessage } from \\\"../../shared/WebviewMessage\\\"\\n  67 | import { EMBEDDING_MODEL_PROFILES } from \\\"../../shared/embeddingModels\\\"\\n  68 | import { ProfileValidator } from \\\"../../shared/ProfileValidator\\\"\\n  69 | \\n  70 | /**\\n  71 |  * https://github.com/microsoft/vscode-webview-ui-toolkit-samples/blob/main/default/weather-webview/src/providers/WeatherViewProvider.ts\\n  72 |  * https://github.com/KumarVariable/vscode-extension-sidebar-html/blob/master/src/customSidebarViewProvider.ts\\n  73 |  */\\n  74 | \\n  75 | export type ClineProviderEvents = {\\n  76 | \\tclineCreated: [cline: Task]\\n  77 | }\\n  78 | \\n  79 | class OrganizationAllowListViolationError extends Error {\\n  80 | \\tconstructor(message: string) {\\n  81 | \\t\\tsuper(message)\\n  82 | \\t}\\n  83 | }\\n  84 | \\n  85 | export class ClineProvider\\n  86 | \\textends EventEmitter<ClineProviderEvents>\\n  87 | \\timplements vscode.WebviewViewProvider, TelemetryPropertiesProvider\\n  88 | {\\n  89 | \\t// Used in package.json as the view's id. This value cannot be changed due\\n  90 | \\t// to how VSCode caches views based on their id, and updating the id would\\n  91 | \\t// break existing instances of the extension.\\n  92 | \\tpublic static readonly sideBarId = `${Package.name}.SidebarProvider`\\n  93 | \\tpublic static readonly tabPanelId = `${Package.name}.TabPanelProvider`\\n  94 | \\tprivate static activeInstances: Set<ClineProvider> = new Set()\\n  95 | \\tprivate disposables: vscode.Disposable[] = []\\n  96 | \\tprivate webviewDisposables: vscode.Disposable[] = []\\n  97 | \\tprivate view?: vscode.WebviewView | vscode.WebviewPanel\\n  98 | \\tprivate clineStack: Task[] = []\\n  99 | \\tprivate codeIndexStatusSubscription?: vscode.Disposable\\n 100 | \\tprivate _workspaceTracker?: WorkspaceTracker // workSpaceTracker read-only for access outside this class\\n 101 | \\tpublic get workspaceTracker(): WorkspaceTracker | undefined {\\n 102 | \\t\\treturn this._workspaceTracker\\n 103 | \\t}\\n 104 | \\tprotected mcpHub?: McpHub // Change from private to protected\\n 105 | \\tprivate marketplaceManager: MarketplaceManager\\n 106 | \\n 107 | \\tpublic isViewLaunched = false\\n 108 | \\tpublic settingsImportedAt?: number\\n 109 | \\tpublic readonly latestAnnouncementId = \\\"dec-12-2025-3-20\\\" // Update for v3.20.0 announcement\\n 110 | \\tpublic readonly providerSettingsManager: ProviderSettingsManager\\n 111 | \\tpublic readonly customModesManager: CustomModesManager\\n 112 | \\n 113 | \\tconstructor(\\n 114 | \\t\\treadonly context: vscode.ExtensionContext,\\n 115 | \\t\\tprivate readonly outputChannel: vscode.OutputChannel,\\n 116 | \\t\\tprivate readonly renderContext: \\\"sidebar\\\" | \\\"editor\\\" = \\\"sidebar\\\",\\n 117 | \\t\\tpublic readonly contextProxy: ContextProxy,\\n 118 | \\t\\tpublic readonly codeIndexManager?: CodeIndexManager,\\n 119 | \\t) {\\n 120 | \\t\\tsuper()\\n 121 | \\n 122 | \\t\\tthis.log(\\\"ClineProvider instantiated\\\")\\n 123 | \\t\\tClineProvider.activeInstances.add(this)\\n 124 | \\n 125 | \\t\\tthis.codeIndexManager = codeIndexManager\\n 126 | \\t\\tthis.updateGlobalState(\\\"codebaseIndexModels\\\", EMBEDDING_MODEL_PROFILES)\\n 127 | \\n 128 | \\t\\t// Start configuration loading (which might trigger indexing) in the background.\\n 129 | \\t\\t// Don't await, allowing activation to continue immediately.\\n 130 | \\n 131 | \\t\\t// Register this provider with the telemetry service to enable it to add\\n 132 | \\t\\t// properties like mode and provider.\\n 133 | \\t\\tTelemetryService.instance.setProvider(this)\\n 134 | \\n 135 | \\t\\tthis._workspaceTracker = new WorkspaceTracker(this)\\n 136 | \\n 137 | \\t\\tthis.providerSettingsManager = new ProviderSettingsManager(this.context)\\n 138 | \\n 139 | \\t\\tthis.customModesManager = new CustomModesManager(this.context, async () => {\\n 140 | \\t\\t\\tawait this.postStateToWebview()\\n 141 | \\t\\t})\\n 142 | \\n 143 | \\t\\t// Initialize MCP Hub through the singleton manager\\n 144 | \\t\\tMcpServerManager.getInstance(this.context, this)\\n 145 | \\t\\t\\t.then((hub) => {\\n 146 | \\t\\t\\t\\tthis.mcpHub = hub\\n 147 | \\t\\t\\t\\tthis.mcpHub.registerClient()\\n 148 | \\t\\t\\t})\\n 149 | \\t\\t\\t.catch((error) => {\\n 150 | \\t\\t\\t\\tthis.log(`Failed to initialize MCP Hub: ${error}`)\\n 151 | \\t\\t\\t})\\n 152 | \\n 153 | \\t\\tthis.marketplaceManager = new MarketplaceManager(this.context)\\n 154 | \\t}\\n 155 | \\n 156 | \\t// Adds a new Cline instance to clineStack, marking the start of a new task.\\n 157 | \\t// The instance is pushed to the top of the stack (LIFO order).\\n 158 | \\t// When the task is completed, the top instance is removed, reactivating the previous task.\\n 159 | \\tasync addClineToStack(cline: Task) {\\n 160 | \\t\\tconsole.log(`[subtasks] adding task ${cline.taskId}.${cline.instanceId} to stack`)\\n 161 | \\n 162 | \\t\\t// Add this cline instance into the stack that represents the order of all the called tasks.\\n 163 | \\t\\tthis.clineStack.push(cline)\\n 164 | \\n 165 | \\t\\t// Ensure getState() resolves correctly.\\n 166 | \\t\\tconst state = await this.getState()\\n 167 | \\n 168 | \\t\\tif (!state || typeof state.mode !== \\\"string\\\") {\\n 169 | \\t\\t\\tthrow new Error(t(\\\"common:errors.retrieve_current_mode\\\"))\\n 170 | \\t\\t}\\n 171 | \\t}\\n 172 | \\n 173 | \\t// Removes and destroys the top Cline instance (the current finished task),\\n 174 | \\t// activating the previous one (resuming the parent task).\\n 175 | \\tasync removeClineFromStack() {\\n 176 | \\t\\tif (this.clineStack.length === 0) {\\n 177 | \\t\\t\\treturn\\n 178 | \\t\\t}\\n 179 | \\n 180 | \\t\\t// Pop the top Cline instance from the stack.\\n 181 | \\t\\tlet cline = this.clineStack.pop()\\n 182 | \\n 183 | \\t\\tif (cline) {\\n 184 | \\t\\t\\tconsole.log(`[subtasks] removing task ${cline.taskId}.${cline.instanceId} from stack`)\\n 185 | \\n 186 | \\t\\t\\ttry {\\n 187 | \\t\\t\\t\\t// Abort the running task and set isAbandoned to true so\\n 188 | \\t\\t\\t\\t// all running promises will exit as well.\\n 189 | \\t\\t\\t\\tawait cline.abortTask(true)\\n 190 | \\t\\t\\t} catch (e) {\\n 191 | \\t\\t\\t\\tthis.log(\\n 192 | \\t\\t\\t\\t\\t`[subtasks] encountered error while aborting task ${cline.taskId}.${cline.instanceId}: ${e.message}`,\\n 193 | \\t\\t\\t\\t)\\n 194 | \\t\\t\\t}\\n 195 | \\n 196 | \\t\\t\\t// Make sure no reference kept, once promises end it will be\\n 197 | \\t\\t\\t// garbage collected.\\n 198 | \\t\\t\\tcline = undefined\\n 199 | \\t\\t}\\n 200 | \\t}\\n 201 | \\n 202 | \\t// returns the current cline object in the stack (the top one)\\n 203 | \\t// if the stack is empty, returns undefined\\n 204 | \\tgetCurrentCline(): Task | undefined {\\n 205 | \\t\\tif (this.clineStack.length === 0) {\\n 206 | \\t\\t\\treturn undefined\\n 207 | \\t\\t}\\n 208 | \\t\\treturn this.clineStack[this.clineStack.length - 1]\\n 209 | \\t}\\n 210 | \\n 211 | \\t// returns the current clineStack length (how many cline objects are in the stack)\\n 212 | \\tgetClineStackSize(): number {\\n 213 | \\t\\treturn this.clineStack.length\\n 214 | \\t}\\n 215 | \\n 216 | \\tpublic getCurrentTaskStack(): string[] {\\n 217 | \\t\\treturn this.clineStack.map((cline) => cline.taskId)\\n 218 | \\t}\\n 219 | \\n 220 | \\t// remove the current task/cline instance (at the top of the stack), so this task is finished\\n 221 | \\t// and resume the previous task/cline instance (if it exists)\\n 222 | \\t// this is used when a sub task is finished and the parent task needs to be resumed\\n 223 | \\tasync finishSubTask(lastMessage: string) {\\n 224 | \\t\\tconsole.log(`[subtasks] finishing subtask ${lastMessage}`)\\n 225 | \\t\\t// remove the last cline instance from the stack (this is the finished sub task)\\n 226 | \\t\\tawait this.removeClineFromStack()\\n 227 | \\t\\t// resume the last cline instance in the stack (if it exists - this is the 'parent' calling task)\\n 228 | \\t\\tawait this.getCurrentCline()?.resumePausedTask(lastMessage)\\n 229 | \\t}\\n 230 | \\n 231 | \\t/*\\n 232 | \\tVSCode extensions use the disposable pattern to clean up resources when the sidebar/editor tab is closed by the user or system. This applies to event listening, commands, interacting with the UI, etc.\\n 233 | \\t- https://vscode-docs.readthedocs.io/en/stable/extensions/patterns-and-principles/\\n 234 | \\t- https://github.com/microsoft/vscode-extension-samples/blob/main/webview-sample/src/extension.ts\\n 235 | \\t*/\\n 236 | \\tprivate clearWebviewResources() {\\n 237 | \\t\\twhile (this.webviewDisposables.length) {\\n 238 | \\t\\t\\tconst x = this.webviewDisposables.pop()\\n 239 | \\t\\t\\tif (x) {\\n 240 | \\t\\t\\t\\tx.dispose()\\n 241 | \\t\\t\\t}\\n 242 | \\t\\t}\\n 243 | \\t}\\n 244 | \\n 245 | \\tasync dispose() {\\n 246 | \\t\\tthis.log(\\\"Disposing ClineProvider...\\\")\\n 247 | \\t\\tawait this.removeClineFromStack()\\n 248 | \\t\\tthis.log(\\\"Cleared task\\\")\\n 249 | \\n 250 | \\t\\tif (this.view && \\\"dispose\\\" in this.view) {\\n 251 | \\t\\t\\tthis.view.dispose()\\n 252 | \\t\\t\\tthis.log(\\\"Disposed webview\\\")\\n 253 | \\t\\t}\\n 254 | \\n 255 | \\t\\tthis.clearWebviewResources()\\n 256 | \\n 257 | \\t\\twhile (this.disposables.length) {\\n 258 | \\t\\t\\tconst x = this.disposables.pop()\\n 259 | \\n 260 | \\t\\t\\tif (x) {\\n 261 | \\t\\t\\t\\tx.dispose()\\n 262 | \\t\\t\\t}\\n 263 | \\t\\t}\\n 264 | \\n 265 | \\t\\tthis._workspaceTracker?.dispose()\\n 266 | \\t\\tthis._workspaceTracker = undefined\\n 267 | \\t\\tawait this.mcpHub?.unregisterClient()\\n 268 | \\t\\tthis.mcpHub = undefined\\n 269 | \\t\\tthis.marketplaceManager?.cleanup()\\n 270 | \\t\\tthis.customModesManager?.dispose()\\n 271 | \\t\\tthis.log(\\\"Disposed all disposables\\\")\\n 272 | \\t\\tClineProvider.activeInstances.delete(this)\\n 273 | \\n 274 | \\t\\tMcpServerManager.unregisterProvider(this)\\n 275 | \\t}\\n 276 | \\n 277 | \\tpublic static getVisibleInstance(): ClineProvider | undefined {\\n 278 | \\t\\treturn findLast(Array.from(this.activeInstances), (instance) => instance.view?.visible === true)\\n 279 | \\t}\\n 280 | \\n 281 | \\tpublic static async getInstance(): Promise<ClineProvider | undefined> {\\n 282 | \\t\\tlet visibleProvider = ClineProvider.getVisibleInstance()\\n 283 | \\n 284 | \\t\\t// If no visible provider, try to show the sidebar view\\n 285 | \\t\\tif (!visibleProvider) {\\n 286 | \\t\\t\\tawait vscode.commands.executeCommand(`${Package.name}.SidebarProvider.focus`)\\n 287 | \\t\\t\\t// Wait briefly for the view to become visible\\n 288 | \\t\\t\\tawait delay(100)\\n 289 | \\t\\t\\tvisibleProvider = ClineProvider.getVisibleInstance()\\n 290 | \\t\\t}\\n 291 | \\n 292 | \\t\\t// If still no visible provider, return\\n 293 | \\t\\tif (!visibleProvider) {\\n 294 | \\t\\t\\treturn\\n 295 | \\t\\t}\\n 296 | \\n 297 | \\t\\treturn visibleProvider\\n 298 | \\t}\\n 299 | \\n 300 | \\tpublic static async isActiveTask(): Promise<boolean> {\\n 301 | \\t\\tconst visibleProvider = await ClineProvider.getInstance()\\n 302 | \\t\\tif (!visibleProvider) {\\n 303 | \\t\\t\\treturn false\\n 304 | \\t\\t}\\n 305 | \\n 306 | \\t\\t// Check if there is a cline instance in the stack (if this provider has an active task)\\n 307 | \\t\\tif (visibleProvider.getCurrentCline()) {\\n 308 | \\t\\t\\treturn true\\n 309 | \\t\\t}\\n 310 | \\n 311 | \\t\\treturn false\\n 312 | \\t}\\n 313 | \\n 314 | \\tpublic static async handleCodeAction(\\n 315 | \\t\\tcommand: CodeActionId,\\n 316 | \\t\\tpromptType: CodeActionName,\\n 317 | \\t\\tparams: Record<string, string | any[]>,\\n 318 | \\t): Promise<void> {\\n 319 | \\t\\t// Capture telemetry for code action usage\\n 320 | \\t\\tTelemetryService.instance.captureCodeActionUsed(promptType)\\n 321 | \\n 322 | \\t\\tconst visibleProvider = await ClineProvider.getInstance()\\n 323 | \\n 324 | \\t\\tif (!visibleProvider) {\\n 325 | \\t\\t\\treturn\\n 326 | \\t\\t}\\n 327 | \\n 328 | \\t\\tconst { customSupportPrompts } = await visibleProvider.getState()\\n 329 | \\n 330 | \\t\\t// TODO: Improve type safety for promptType.\\n 331 | \\t\\tconst prompt = supportPrompt.create(promptType, params, customSupportPrompts)\\n 332 | \\n 333 | \\t\\tif (command === \\\"addToContext\\\") {\\n 334 | \\t\\t\\tawait visibleProvider.postMessageToWebview({ type: \\\"invoke\\\", invoke: \\\"setChatBoxMessage\\\", text: prompt })\\n 335 | \\t\\t\\treturn\\n 336 | \\t\\t}\\n 337 | \\n 338 | \\t\\tawait visibleProvider.initClineWithTask(prompt)\\n 339 | \\t}\\n 340 | \\n 341 | \\tpublic static async handleTerminalAction(\\n 342 | \\t\\tcommand: TerminalActionId,\\n 343 | \\t\\tpromptType: TerminalActionPromptType,\\n 344 | \\t\\tparams: Record<string, string | any[]>,\\n 345 | \\t): Promise<void> {\\n 346 | \\t\\tTelemetryService.instance.captureCodeActionUsed(promptType)\\n 347 | \\n 348 | \\t\\tconst visibleProvider = await ClineProvider.getInstance()\\n 349 | \\n 350 | \\t\\tif (!visibleProvider) {\\n 351 | \\t\\t\\treturn\\n 352 | \\t\\t}\\n 353 | \\n 354 | \\t\\tconst { customSupportPrompts } = await visibleProvider.getState()\\n 355 | \\t\\tconst prompt = supportPrompt.create(promptType, params, customSupportPrompts)\\n 356 | \\n 357 | \\t\\tif (command === \\\"terminalAddToContext\\\") {\\n 358 | \\t\\t\\tawait visibleProvider.postMessageToWebview({ type: \\\"invoke\\\", invoke: \\\"setChatBoxMessage\\\", text: prompt })\\n 359 | \\t\\t\\treturn\\n 360 | \\t\\t}\\n 361 | \\n 362 | \\t\\ttry {\\n 363 | \\t\\t\\tawait visibleProvider.initClineWithTask(prompt)\\n 364 | \\t\\t} catch (error) {\\n 365 | \\t\\t\\tif (error instanceof OrganizationAllowListViolationError) {\\n 366 | \\t\\t\\t\\t// Errors from terminal commands seem to get swallowed / ignored.\\n 367 | \\t\\t\\t\\tvscode.window.showErrorMessage(error.message)\\n 368 | \\t\\t\\t}\\n 369 | \\t\\t\\tthrow error\\n 370 | \\t\\t}\\n 371 | \\t}\\n 372 | \\n 373 | \\tasync resolveWebviewView(webviewView: vscode.WebviewView | vscode.WebviewPanel) {\\n 374 | \\t\\tthis.log(\\\"Resolving webview view\\\")\\n 375 | \\n 376 | \\t\\tthis.view = webviewView\\n 377 | \\n 378 | \\t\\t// Set panel reference according to webview type\\n 379 | \\t\\tconst inTabMode = \\\"onDidChangeViewState\\\" in webviewView\\n 380 | \\t\\tif (inTabMode) {\\n 381 | \\t\\t\\t// Tag page type\\n 382 | \\t\\t\\tsetPanel(webviewView, \\\"tab\\\")\\n 383 | \\t\\t} else if (\\\"onDidChangeVisibility\\\" in webviewView) {\\n 384 | \\t\\t\\t// Sidebar Type\\n 385 | \\t\\t\\tsetPanel(webviewView, \\\"sidebar\\\")\\n 386 | \\t\\t}\\n 387 | \\n 388 | \\t\\t// Initialize out-of-scope variables that need to receive persistent global state values\\n 389 | \\t\\tthis.getState().then(\\n 390 | \\t\\t\\t({\\n 391 | \\t\\t\\t\\tterminalShellIntegrationTimeout = Terminal.defaultShellIntegrationTimeout,\\n 392 | \\t\\t\\t\\tterminalShellIntegrationDisabled = false,\\n 393 | \\t\\t\\t\\tterminalCommandDelay = 0,\\n 394 | \\t\\t\\t\\tterminalZshClearEolMark = true,\\n 395 | \\t\\t\\t\\tterminalZshOhMy = false,\\n 396 | \\t\\t\\t\\tterminalZshP10k = false,\\n 397 | \\t\\t\\t\\tterminalPowershellCounter = false,\\n 398 | \\t\\t\\t\\tterminalZdotdir = false,\\n 399 | \\t\\t\\t}) => {\\n 400 | \\t\\t\\t\\tTerminal.setShellIntegrationTimeout(terminalShellIntegrationTimeout)\\n 401 | \\t\\t\\t\\tTerminal.setShellIntegrationDisabled(terminalShellIntegrationDisabled)\\n 402 | \\t\\t\\t\\tTerminal.setCommandDelay(terminalCommandDelay)\\n 403 | \\t\\t\\t\\tTerminal.setTerminalZshClearEolMark(terminalZshClearEolMark)\\n 404 | \\t\\t\\t\\tTerminal.setTerminalZshOhMy(terminalZshOhMy)\\n 405 | \\t\\t\\t\\tTerminal.setTerminalZshP10k(terminalZshP10k)\\n 406 | \\t\\t\\t\\tTerminal.setPowershellCounter(terminalPowershellCounter)\\n 407 | \\t\\t\\t\\tTerminal.setTerminalZdotdir(terminalZdotdir)\\n 408 | \\t\\t\\t},\\n 409 | \\t\\t)\\n 410 | \\n 411 | \\t\\t// Initialize tts enabled state\\n 412 | \\t\\tthis.getState().then(({ ttsEnabled }) => {\\n 413 | \\t\\t\\tsetTtsEnabled(ttsEnabled ?? false)\\n 414 | \\t\\t})\\n 415 | \\n 416 | \\t\\t// Initialize tts speed state\\n 417 | \\t\\tthis.getState().then(({ ttsSpeed }) => {\\n 418 | \\t\\t\\tsetTtsSpeed(ttsSpeed ?? 1)\\n 419 | \\t\\t})\\n 420 | \\n 421 | \\t\\twebviewView.webview.options = {\\n 422 | \\t\\t\\t// Allow scripts in the webview\\n 423 | \\t\\t\\tenableScripts: true,\\n 424 | \\t\\t\\tlocalResourceRoots: [this.contextProxy.extensionUri],\\n 425 | \\t\\t}\\n 426 | \\n 427 | \\t\\twebviewView.webview.html =\\n 428 | \\t\\t\\tthis.contextProxy.extensionMode === vscode.ExtensionMode.Development\\n 429 | \\t\\t\\t\\t? await this.getHMRHtmlContent(webviewView.webview)\\n 430 | \\t\\t\\t\\t: this.getHtmlContent(webviewView.webview)\\n 431 | \\n 432 | \\t\\t// Sets up an event listener to listen for messages passed from the webview view context\\n 433 | \\t\\t// and executes code based on the message that is received\\n 434 | \\t\\tthis.setWebviewMessageListener(webviewView.webview)\\n 435 | \\n 436 | \\t\\t// Subscribe to code index status updates if the manager exists\\n 437 | \\t\\tif (this.codeIndexManager) {\\n 438 | \\t\\t\\tthis.codeIndexStatusSubscription = this.codeIndexManager.onProgressUpdate((update: IndexProgressUpdate) => {\\n 439 | \\t\\t\\t\\tthis.postMessageToWebview({\\n 440 | \\t\\t\\t\\t\\ttype: \\\"indexingStatusUpdate\\\",\\n 441 | \\t\\t\\t\\t\\tvalues: update,\\n 442 | \\t\\t\\t\\t})\\n 443 | \\t\\t\\t})\\n 444 | \\t\\t\\tthis.webviewDisposables.push(this.codeIndexStatusSubscription)\\n 445 | \\t\\t}\\n 446 | \\n 447 | \\t\\t// Logs show up in bottom panel > Debug Console\\n 448 | \\t\\t//console.log(\\\"registering listener\\\")\\n 449 | \\n 450 | \\t\\t// Listen for when the panel becomes visible\\n 451 | \\t\\t// https://github.com/microsoft/vscode-discussions/discussions/840\\n 452 | \\t\\tif (\\\"onDidChangeViewState\\\" in webviewView) {\\n 453 | \\t\\t\\t// WebviewView and WebviewPanel have all the same properties except for this visibility listener\\n 454 | \\t\\t\\t// panel\\n 455 | \\t\\t\\tconst viewStateDisposable = webviewView.onDidChangeViewState(() => {\\n 456 | \\t\\t\\t\\tif (this.view?.visible) {\\n 457 | \\t\\t\\t\\t\\tthis.postMessageToWebview({ type: \\\"action\\\", action: \\\"didBecomeVisible\\\" })\\n 458 | \\t\\t\\t\\t}\\n 459 | \\t\\t\\t})\\n 460 | \\t\\t\\tthis.webviewDisposables.push(viewStateDisposable)\\n 461 | \\t\\t} else if (\\\"onDidChangeVisibility\\\" in webviewView) {\\n 462 | \\t\\t\\t// sidebar\\n 463 | \\t\\t\\tconst visibilityDisposable = webviewView.onDidChangeVisibility(() => {\\n 464 | \\t\\t\\t\\tif (this.view?.visible) {\\n 465 | \\t\\t\\t\\t\\tthis.postMessageToWebview({ type: \\\"action\\\", action: \\\"didBecomeVisible\\\" })\\n 466 | \\t\\t\\t\\t}\\n 467 | \\t\\t\\t})\\n 468 | \\t\\t\\tthis.webviewDisposables.push(visibilityDisposable)\\n 469 | \\t\\t}\\n 470 | \\n 471 | \\t\\t// Listen for when the view is disposed\\n 472 | \\t\\t// This happens when the user closes the view or when the view is closed programmatically\\n 473 | \\t\\twebviewView.onDidDispose(\\n 474 | \\t\\t\\tasync () => {\\n 475 | \\t\\t\\t\\tif (inTabMode) {\\n 476 | \\t\\t\\t\\t\\tthis.log(\\\"Disposing ClineProvider instance for tab view\\\")\\n 477 | \\t\\t\\t\\t\\tawait this.dispose()\\n 478 | \\t\\t\\t\\t} else {\\n 479 | \\t\\t\\t\\t\\tthis.log(\\\"Clearing webview resources for sidebar view\\\")\\n 480 | \\t\\t\\t\\t\\tthis.clearWebviewResources()\\n 481 | \\t\\t\\t\\t\\tthis.codeIndexStatusSubscription?.dispose()\\n 482 | \\t\\t\\t\\t\\tthis.codeIndexStatusSubscription = undefined\\n 483 | \\t\\t\\t\\t}\\n 484 | \\t\\t\\t},\\n 485 | \\t\\t\\tnull,\\n 486 | \\t\\t\\tthis.disposables,\\n 487 | \\t\\t)\\n 488 | \\n 489 | \\t\\t// Listen for when color changes\\n 490 | \\t\\tconst configDisposable = vscode.workspace.onDidChangeConfiguration(async (e) => {\\n 491 | \\t\\t\\tif (e && e.affectsConfiguration(\\\"workbench.colorTheme\\\")) {\\n 492 | \\t\\t\\t\\t// Sends latest theme name to webview\\n 493 | \\t\\t\\t\\tawait this.postMessageToWebview({ type: \\\"theme\\\", text: JSON.stringify(await getTheme()) })\\n 494 | \\t\\t\\t}\\n 495 | \\t\\t})\\n 496 | \\t\\tthis.webviewDisposables.push(configDisposable)\\n 497 | \\n 498 | \\t\\t// If the extension is starting a new session, clear previous task state.\\n 499 | \\t\\tawait this.removeClineFromStack()\\n 500 | \\n 501 | \\t\\t// Set initial VSCode context for experiments\\n 502 | \\t\\tawait this.updateVSCodeContext()\\n 503 | \\n 504 | \\t\\tthis.log(\\\"Webview view resolved\\\")\\n 505 | \\t}\\n 506 | \\n 507 | \\tpublic async initClineWithSubTask(parent: Task, task?: string, images?: string[]) {\\n 508 | \\t\\treturn this.initClineWithTask(task, images, parent)\\n 509 | \\t}\\n 510 | \\n 511 | \\t// When initializing a new task, (not from history but from a tool command\\n 512 | \\t// new_task) there is no need to remove the previous task since the new\\n 513 | \\t// task is a subtask of the previous one, and when it finishes it is removed\\n 514 | \\t// from the stack and the caller is resumed in this way we can have a chain\\n 515 | \\t// of tasks, each one being a sub task of the previous one until the main\\n 516 | \\t// task is finished.\\n 517 | \\tpublic async initClineWithTask(\\n 518 | \\t\\ttask?: string,\\n 519 | \\t\\timages?: string[],\\n 520 | \\t\\tparentTask?: Task,\\n 521 | \\t\\toptions: Partial<\\n 522 | \\t\\t\\tPick<\\n 523 | \\t\\t\\t\\tTaskOptions,\\n 524 | \\t\\t\\t\\t\\\"enableDiff\\\" | \\\"enableCheckpoints\\\" | \\\"fuzzyMatchThreshold\\\" | \\\"consecutiveMistakeLimit\\\" | \\\"experiments\\\"\\n 525 | \\t\\t\\t>\\n 526 | \\t\\t> = {},\\n 527 | \\t) {\\n 528 | \\t\\tconst {\\n 529 | \\t\\t\\tapiConfiguration,\\n 530 | \\t\\t\\torganizationAllowList,\\n 531 | \\t\\t\\tdiffEnabled: enableDiff,\\n 532 | \\t\\t\\tenableCheckpoints,\\n 533 | \\t\\t\\tfuzzyMatchThreshold,\\n 534 | \\t\\t\\texperiments,\\n 535 | \\t\\t} = await this.getState()\\n 536 | \\n 537 | \\t\\tif (!ProfileValidator.isProfileAllowed(apiConfiguration, organizationAllowList)) {\\n 538 | \\t\\t\\tthrow new OrganizationAllowListViolationError(t(\\\"common:errors.violated_organization_allowlist\\\"))\\n 539 | \\t\\t}\\n 540 | \\n 541 | \\t\\tconst cline = new Task({\\n 542 | \\t\\t\\tprovider: this,\\n 543 | \\t\\t\\tapiConfiguration,\\n 544 | \\t\\t\\tenableDiff,\\n 545 | \\t\\t\\tenableCheckpoints,\\n 546 | \\t\\t\\tfuzzyMatchThreshold,\\n 547 | \\t\\t\\ttask,\\n 548 | \\t\\t\\timages,\\n 549 | \\t\\t\\texperiments,\\n 550 | \\t\\t\\trootTask: this.clineStack.length > 0 ? this.clineStack[0] : undefined,\\n 551 | \\t\\t\\tparentTask,\\n 552 | \\t\\t\\ttaskNumber: this.clineStack.length + 1,\\n 553 | \\t\\t\\tonCreated: (cline) => this.emit(\\\"clineCreated\\\", cline),\\n 554 | \\t\\t\\t...options,\\n 555 | \\t\\t})\\n 556 | \\n 557 | \\t\\tawait this.addClineToStack(cline)\\n 558 | \\n 559 | \\t\\tthis.log(\\n 560 | \\t\\t\\t`[subtasks] ${cline.parentTask ? \\\"child\\\" : \\\"parent\\\"} task ${cline.taskId}.${cline.instanceId} instantiated`,\\n 561 | \\t\\t)\\n 562 | \\n 563 | \\t\\treturn cline\\n 564 | \\t}\\n 565 | \\n 566 | \\tpublic async initClineWithHistoryItem(historyItem: HistoryItem & { rootTask?: Task; parentTask?: Task }) {\\n 567 | \\t\\tawait this.removeClineFromStack()\\n 568 | \\n 569 | \\t\\tconst {\\n 570 | \\t\\t\\tapiConfiguration,\\n 571 | \\t\\t\\tdiffEnabled: enableDiff,\\n 572 | \\t\\t\\tenableCheckpoints,\\n 573 | \\t\\t\\tfuzzyMatchThreshold,\\n 574 | \\t\\t\\texperiments,\\n 575 | \\t\\t} = await this.getState()\\n 576 | \\n 577 | \\t\\tconst cline = new Task({\\n 578 | \\t\\t\\tprovider: this,\\n 579 | \\t\\t\\tapiConfiguration,\\n 580 | \\t\\t\\tenableDiff,\\n 581 | \\t\\t\\tenableCheckpoints,\\n 582 | \\t\\t\\tfuzzyMatchThreshold,\\n 583 | \\t\\t\\thistoryItem,\\n 584 | \\t\\t\\texperiments,\\n 585 | \\t\\t\\trootTask: historyItem.rootTask,\\n 586 | \\t\\t\\tparentTask: historyItem.parentTask,\\n 587 | \\t\\t\\ttaskNumber: historyItem.number,\\n 588 | \\t\\t\\tonCreated: (cline) => this.emit(\\\"clineCreated\\\", cline),\\n 589 | \\t\\t})\\n 590 | \\n 591 | \\t\\tawait this.addClineToStack(cline)\\n 592 | \\t\\tthis.log(\\n 593 | \\t\\t\\t`[subtasks] ${cline.parentTask ? \\\"child\\\" : \\\"parent\\\"} task ${cline.taskId}.${cline.instanceId} instantiated`,\\n 594 | \\t\\t)\\n 595 | \\t\\treturn cline\\n 596 | \\t}\\n 597 | \\n 598 | \\tpublic async postMessageToWebview(message: ExtensionMessage) {\\n 599 | \\t\\tawait this.view?.webview.postMessage(message)\\n 600 | \\t}\\n 601 | \\n 602 | \\tprivate async getHMRHtmlContent(webview: vscode.Webview): Promise<string> {\\n 603 | \\t\\t// Try to read the port from the file\\n 604 | \\t\\tlet localPort = \\\"5173\\\" // Default fallback\\n 605 | \\t\\ttry {\\n 606 | \\t\\t\\tconst fs = require(\\\"fs\\\")\\n 607 | \\t\\t\\tconst path = require(\\\"path\\\")\\n 608 | \\t\\t\\tconst portFilePath = path.resolve(__dirname, \\\"../../.vite-port\\\")\\n 609 | \\n 610 | \\t\\t\\tif (fs.existsSync(portFilePath)) {\\n 611 | \\t\\t\\t\\tlocalPort = fs.readFileSync(portFilePath, \\\"utf8\\\").trim()\\n 612 | \\t\\t\\t\\tconsole.log(`[ClineProvider:Vite] Using Vite server port from ${portFilePath}: ${localPort}`)\\n 613 | \\t\\t\\t} else {\\n 614 | \\t\\t\\t\\tconsole.log(\\n 615 | \\t\\t\\t\\t\\t`[ClineProvider:Vite] Port file not found at ${portFilePath}, using default port: ${localPort}`,\\n 616 | \\t\\t\\t\\t)\\n 617 | \\t\\t\\t}\\n 618 | \\t\\t} catch (err) {\\n 619 | \\t\\t\\tconsole.error(\\\"[ClineProvider:Vite] Failed to read Vite port file:\\\", err)\\n 620 | \\t\\t\\t// Continue with default port if file reading fails\\n 621 | \\t\\t}\\n 622 | \\n 623 | \\t\\tconst localServerUrl = `localhost:${localPort}`\\n 624 | \\n 625 | \\t\\t// Check if local dev server is running.\\n 626 | \\t\\ttry {\\n 627 | \\t\\t\\tawait axios.get(`http://${localServerUrl}`)\\n 628 | \\t\\t} catch (error) {\\n 629 | \\t\\t\\tvscode.window.showErrorMessage(t(\\\"common:errors.hmr_not_running\\\"))\\n 630 | \\n 631 | \\t\\t\\treturn this.getHtmlContent(webview)\\n 632 | \\t\\t}\\n 633 | \\n 634 | \\t\\tconst nonce = getNonce()\\n 635 | \\n 636 | \\t\\tconst stylesUri = getUri(webview, this.contextProxy.extensionUri, [\\n 637 | \\t\\t\\t\\\"webview-ui\\\",\\n 638 | \\t\\t\\t\\\"build\\\",\\n 639 | \\t\\t\\t\\\"assets\\\",\\n 640 | \\t\\t\\t\\\"index.css\\\",\\n 641 | \\t\\t])\\n 642 | \\n 643 | \\t\\tconst codiconsUri = getUri(webview, this.contextProxy.extensionUri, [\\\"assets\\\", \\\"codicons\\\", \\\"codicon.css\\\"])\\n 644 | \\t\\tconst materialIconsUri = getUri(webview, this.contextProxy.extensionUri, [\\n 645 | \\t\\t\\t\\\"assets\\\",\\n 646 | \\t\\t\\t\\\"vscode-material-icons\\\",\\n 647 | \\t\\t\\t\\\"icons\\\",\\n 648 | \\t\\t])\\n 649 | \\t\\tconst imagesUri = getUri(webview, this.contextProxy.extensionUri, [\\\"assets\\\", \\\"images\\\"])\\n 650 | \\t\\tconst audioUri = getUri(webview, this.contextProxy.extensionUri, [\\\"webview-ui\\\", \\\"audio\\\"])\\n 651 | \\n 652 | \\t\\tconst file = \\\"src/index.tsx\\\"\\n 653 | \\t\\tconst scriptUri = `http://${localServerUrl}/${file}`\\n 654 | \\n 655 | \\t\\tconst reactRefresh = /*html*/ `\\n 656 | \\t\\t\\t<script nonce=\\\"${nonce}\\\" type=\\\"module\\\">\\n 657 | \\t\\t\\t\\timport RefreshRuntime from \\\"http://localhost:${localPort}/@react-refresh\\\"\\n 658 | \\t\\t\\t\\tRefreshRuntime.injectIntoGlobalHook(window)\\n 659 | \\t\\t\\t\\twindow.$RefreshReg$ = () => {}\\n 660 | \\t\\t\\t\\twindow.$RefreshSig$ = () => (type) => type\\n 661 | \\t\\t\\t\\twindow.__vite_plugin_react_preamble_installed__ = true\\n 662 | \\t\\t\\t</script>\\n 663 | \\t\\t`\\n 664 | \\n 665 | \\t\\tconst csp = [\\n 666 | \\t\\t\\t\\\"default-src 'none'\\\",\\n 667 | \\t\\t\\t`font-src ${webview.cspSource}`,\\n 668 | \\t\\t\\t`style-src ${webview.cspSource} 'unsafe-inline' https://* http://${localServerUrl} http://0.0.0.0:${localPort}`,\\n 669 | \\t\\t\\t`img-src ${webview.cspSource} https://storage.googleapis.com https://img.clerk.com data:`,\\n 670 | \\t\\t\\t`media-src ${webview.cspSource}`,\\n 671 | \\t\\t\\t`script-src 'unsafe-eval' ${webview.cspSource} https://* https://*.posthog.com http://${localServerUrl} http://0.0.0.0:${localPort} 'nonce-${nonce}'`,\\n 672 | \\t\\t\\t`connect-src https://* https://*.posthog.com ws://${localServerUrl} ws://0.0.0.0:${localPort} http://${localServerUrl} http://0.0.0.0:${localPort}`,\\n 673 | \\t\\t]\\n 674 | \\n 675 | \\t\\treturn /*html*/ `\\n 676 | \\t\\t\\t<!DOCTYPE html>\\n 677 | \\t\\t\\t<html lang=\\\"en\\\">\\n 678 | \\t\\t\\t\\t<head>\\n 679 | \\t\\t\\t\\t\\t<meta charset=\\\"utf-8\\\">\\n 680 | \\t\\t\\t\\t\\t<meta name=\\\"viewport\\\" content=\\\"width=device-width,initial-scale=1,shrink-to-fit=no\\\">\\n 681 | \\t\\t\\t\\t\\t<meta http-equiv=\\\"Content-Security-Policy\\\" content=\\\"${csp.join(\\\"; \\\")}\\\">\\n 682 | \\t\\t\\t\\t\\t<link rel=\\\"stylesheet\\\" type=\\\"text/css\\\" href=\\\"${stylesUri}\\\">\\n 683 | \\t\\t\\t\\t\\t<link href=\\\"${codiconsUri}\\\" rel=\\\"stylesheet\\\" />\\n 684 | \\t\\t\\t\\t\\t<script nonce=\\\"${nonce}\\\">\\n 685 | \\t\\t\\t\\t\\t\\twindow.IMAGES_BASE_URI = \\\"${imagesUri}\\\"\\n 686 | \\t\\t\\t\\t\\t\\twindow.AUDIO_BASE_URI = \\\"${audioUri}\\\"\\n 687 | \\t\\t\\t\\t\\t\\twindow.MATERIAL_ICONS_BASE_URI = \\\"${materialIconsUri}\\\"\\n 688 | \\t\\t\\t\\t\\t</script>\\n 689 | \\t\\t\\t\\t\\t<title>Roo Code</title>\\n 690 | \\t\\t\\t\\t</head>\\n 691 | \\t\\t\\t\\t<body>\\n 692 | \\t\\t\\t\\t\\t<div id=\\\"root\\\"></div>\\n 693 | \\t\\t\\t\\t\\t${reactRefresh}\\n 694 | \\t\\t\\t\\t\\t<script type=\\\"module\\\" src=\\\"${scriptUri}\\\"></script>\\n 695 | \\t\\t\\t\\t</body>\\n 696 | \\t\\t\\t</html>\\n 697 | \\t\\t`\\n 698 | \\t}\\n 699 | \\n 700 | \\t/**\\n 701 | \\t * Defines and returns the HTML that should be rendered within the webview panel.\\n 702 | \\t *\\n 703 | \\t * @remarks This is also the place where references to the React webview build files\\n 704 | \\t * are created and inserted into the webview HTML.\\n 705 | \\t *\\n 706 | \\t * @param webview A reference to the extension webview\\n 707 | \\t * @param extensionUri The URI of the directory containing the extension\\n 708 | \\t * @returns A template string literal containing the HTML that should be\\n 709 | \\t * rendered within the webview panel\\n 710 | \\t */\\n 711 | \\tprivate getHtmlContent(webview: vscode.Webview): string {\\n 712 | \\t\\t// Get the local path to main script run in the webview,\\n 713 | \\t\\t// then convert it to a uri we can use in the webview.\\n 714 | \\n 715 | \\t\\t// The CSS file from the React build output\\n 716 | \\t\\tconst stylesUri = getUri(webview, this.contextProxy.extensionUri, [\\n 717 | \\t\\t\\t\\\"webview-ui\\\",\\n 718 | \\t\\t\\t\\\"build\\\",\\n 719 | \\t\\t\\t\\\"assets\\\",\\n 720 | \\t\\t\\t\\\"index.css\\\",\\n 721 | \\t\\t])\\n 722 | \\n 723 | \\t\\tconst scriptUri = getUri(webview, this.contextProxy.extensionUri, [\\\"webview-ui\\\", \\\"build\\\", \\\"assets\\\", \\\"index.js\\\"])\\n 724 | \\t\\tconst codiconsUri = getUri(webview, this.contextProxy.extensionUri, [\\\"assets\\\", \\\"codicons\\\", \\\"codicon.css\\\"])\\n 725 | \\t\\tconst materialIconsUri = getUri(webview, this.contextProxy.extensionUri, [\\n 726 | \\t\\t\\t\\\"assets\\\",\\n 727 | \\t\\t\\t\\\"vscode-material-icons\\\",\\n 728 | \\t\\t\\t\\\"icons\\\",\\n 729 | \\t\\t])\\n 730 | \\t\\tconst imagesUri = getUri(webview, this.contextProxy.extensionUri, [\\\"assets\\\", \\\"images\\\"])\\n 731 | \\t\\tconst audioUri = getUri(webview, this.contextProxy.extensionUri, [\\\"webview-ui\\\", \\\"audio\\\"])\\n 732 | \\n 733 | \\t\\t// Use a nonce to only allow a specific script to be run.\\n 734 | \\t\\t/*\\n 735 | \\t\\tcontent security policy of your webview to only allow scripts that have a specific nonce\\n 736 | \\t\\tcreate a content security policy meta tag so that only loading scripts with a nonce is allowed\\n 737 | \\t\\tAs your extension grows you will likely want to add custom styles, fonts, and/or images to your webview. If you do, you will need to update the content security policy meta tag to explicitly allow for these resources. E.g.\\n 738 | \\t\\t\\t\\t<meta http-equiv=\\\"Content-Security-Policy\\\" content=\\\"default-src 'none'; style-src ${webview.cspSource}; font-src ${webview.cspSource}; img-src ${webview.cspSource} https:; script-src 'nonce-${nonce}';\\\">\\n 739 | \\t\\t- 'unsafe-inline' is required for styles due to vscode-webview-toolkit's dynamic style injection\\n 740 | \\t\\t- since we pass base64 images to the webview, we need to specify img-src ${webview.cspSource} data:;\\n 741 | \\n 742 | \\t\\tin meta tag we add nonce attribute: A cryptographic nonce (only used once) to allow scripts. The server must generate a unique nonce value each time it transmits a policy. It is critical to provide a nonce that cannot be guessed as bypassing a resource's policy is otherwise trivial.\\n 743 | \\t\\t*/\\n 744 | \\t\\tconst nonce = getNonce()\\n 745 | \\n 746 | \\t\\t// Tip: Install the es6-string-html VS Code extension to enable code highlighting below\\n 747 | \\t\\treturn /*html*/ `\\n 748 |         <!DOCTYPE html>\\n 749 |         <html lang=\\\"en\\\">\\n 750 |           <head>\\n 751 |             <meta charset=\\\"utf-8\\\">\\n 752 |             <meta name=\\\"viewport\\\" content=\\\"width=device-width,initial-scale=1,shrink-to-fit=no\\\">\\n 753 |             <meta name=\\\"theme-color\\\" content=\\\"#000000\\\">\\n 754 |             <meta http-equiv=\\\"Content-Security-Policy\\\" content=\\\"default-src 'none'; font-src ${webview.cspSource}; style-src ${webview.cspSource} 'unsafe-inline'; img-src ${webview.cspSource} https://storage.googleapis.com https://img.clerk.com data:; media-src ${webview.cspSource}; script-src ${webview.cspSource} 'wasm-unsafe-eval' 'nonce-${nonce}' https://us-assets.i.posthog.com 'strict-dynamic'; connect-src https://openrouter.ai https://api.requesty.ai https://us.i.posthog.com https://us-assets.i.posthog.com;\\\">\\n 755 |             <link rel=\\\"stylesheet\\\" type=\\\"text/css\\\" href=\\\"${stylesUri}\\\">\\n 756 | \\t\\t\\t<link href=\\\"${codiconsUri}\\\" rel=\\\"stylesheet\\\" />\\n 757 | \\t\\t\\t<script nonce=\\\"${nonce}\\\">\\n 758 | \\t\\t\\t\\twindow.IMAGES_BASE_URI = \\\"${imagesUri}\\\"\\n 759 | \\t\\t\\t\\twindow.AUDIO_BASE_URI = \\\"${audioUri}\\\"\\n 760 | \\t\\t\\t\\twindow.MATERIAL_ICONS_BASE_URI = \\\"${materialIconsUri}\\\"\\n 761 | \\t\\t\\t</script>\\n 762 |             <title>Roo Code</title>\\n 763 |           </head>\\n 764 |           <body>\\n 765 |             <noscript>You need to enable JavaScript to run this app.</noscript>\\n 766 |             <div id=\\\"root\\\"></div>\\n 767 |             <script nonce=\\\"${nonce}\\\" type=\\\"module\\\" src=\\\"${scriptUri}\\\"></script>\\n 768 |           </body>\\n 769 |         </html>\\n 770 |       `\\n 771 | \\t}\\n 772 | \\n 773 | \\t/**\\n 774 | \\t * Sets up an event listener to listen for messages passed from the webview context and\\n 775 | \\t * executes code based on the message that is received.\\n 776 | \\t *\\n 777 | \\t * @param webview A reference to the extension webview\\n 778 | \\t */\\n 779 | \\tprivate setWebviewMessageListener(webview: vscode.Webview) {\\n 780 | \\t\\tconst onReceiveMessage = async (message: WebviewMessage) =>\\n 781 | \\t\\t\\twebviewMessageHandler(this, message, this.marketplaceManager)\\n 782 | \\n 783 | \\t\\tconst messageDisposable = webview.onDidReceiveMessage(onReceiveMessage)\\n 784 | \\t\\tthis.webviewDisposables.push(messageDisposable)\\n 785 | \\t}\\n 786 | \\n 787 | \\t/**\\n 788 | \\t * Handle switching to a new mode, including updating the associated API configuration\\n 789 | \\t * @param newMode The mode to switch to\\n 790 | \\t */\\n 791 | \\tpublic async handleModeSwitch(newMode: Mode) {\\n 792 | \\t\\tconst cline = this.getCurrentCline()\\n 793 | \\n 794 | \\t\\tif (cline) {\\n 795 | \\t\\t\\tTelemetryService.instance.captureModeSwitch(cline.taskId, newMode)\\n 796 | \\t\\t\\tcline.emit(\\\"taskModeSwitched\\\", cline.taskId, newMode)\\n 797 | \\t\\t}\\n 798 | \\n 799 | \\t\\tawait this.updateGlobalState(\\\"mode\\\", newMode)\\n 800 | \\n 801 | \\t\\t// Load the saved API config for the new mode if it exists\\n 802 | \\t\\tconst savedConfigId = await this.providerSettingsManager.getModeConfigId(newMode)\\n 803 | \\t\\tconst listApiConfig = await this.providerSettingsManager.listConfig()\\n 804 | \\n 805 | \\t\\t// Update listApiConfigMeta first to ensure UI has latest data\\n 806 | \\t\\tawait this.updateGlobalState(\\\"listApiConfigMeta\\\", listApiConfig)\\n 807 | \\n 808 | \\t\\t// If this mode has a saved config, use it.\\n 809 | \\t\\tif (savedConfigId) {\\n 810 | \\t\\t\\tconst profile = listApiConfig.find(({ id }) => id === savedConfigId)\\n 811 | \\n 812 | \\t\\t\\tif (profile?.name) {\\n 813 | \\t\\t\\t\\tawait this.activateProviderProfile({ name: profile.name })\\n 814 | \\t\\t\\t}\\n 815 | \\t\\t} else {\\n 816 | \\t\\t\\t// If no saved config for this mode, save current config as default.\\n 817 | \\t\\t\\tconst currentApiConfigName = this.getGlobalState(\\\"currentApiConfigName\\\")\\n 818 | \\n 819 | \\t\\t\\tif (currentApiConfigName) {\\n 820 | \\t\\t\\t\\tconst config = listApiConfig.find((c) => c.name === currentApiConfigName)\\n 821 | \\n 822 | \\t\\t\\t\\tif (config?.id) {\\n 823 | \\t\\t\\t\\t\\tawait this.providerSettingsManager.setModeConfig(newMode, config.id)\\n 824 | \\t\\t\\t\\t}\\n 825 | \\t\\t\\t}\\n 826 | \\t\\t}\\n 827 | \\n 828 | \\t\\tawait this.postStateToWebview()\\n 829 | \\t}\\n 830 | \\n 831 | \\t// Provider Profile Management\\n 832 | \\n 833 | \\tgetProviderProfileEntries(): ProviderSettingsEntry[] {\\n 834 | \\t\\treturn this.contextProxy.getValues().listApiConfigMeta || []\\n 835 | \\t}\\n 836 | \\n 837 | \\tgetProviderProfileEntry(name: string): ProviderSettingsEntry | undefined {\\n 838 | \\t\\treturn this.getProviderProfileEntries().find((profile) => profile.name === name)\\n 839 | \\t}\\n 840 | \\n 841 | \\tpublic hasProviderProfileEntry(name: string): boolean {\\n 842 | \\t\\treturn !!this.getProviderProfileEntry(name)\\n 843 | \\t}\\n 844 | \\n 845 | \\tasync upsertProviderProfile(\\n 846 | \\t\\tname: string,\\n 847 | \\t\\tproviderSettings: ProviderSettings,\\n 848 | \\t\\tactivate: boolean = true,\\n 849 | \\t): Promise<string | undefined> {\\n 850 | \\t\\ttry {\\n 851 | \\t\\t\\t// TODO: Do we need to be calling `activateProfile`? It's not\\n 852 | \\t\\t\\t// clear to me what the source of truth should be; in some cases\\n 853 | \\t\\t\\t// we rely on the `ContextProxy`'s data store and in other cases\\n 854 | \\t\\t\\t// we rely on the `ProviderSettingsManager`'s data store. It might\\n 855 | \\t\\t\\t// be simpler to unify these two.\\n 856 | \\t\\t\\tconst id = await this.providerSettingsManager.saveConfig(name, providerSettings)\\n 857 | \\n 858 | \\t\\t\\tif (activate) {\\n 859 | \\t\\t\\t\\tconst { mode } = await this.getState()\\n 860 | \\n 861 | \\t\\t\\t\\t// These promises do the following:\\n 862 | \\t\\t\\t\\t// 1. Adds or updates the list of provider profiles.\\n 863 | \\t\\t\\t\\t// 2. Sets the current provider profile.\\n 864 | \\t\\t\\t\\t// 3. Sets the current mode's provider profile.\\n 865 | \\t\\t\\t\\t// 4. Copies the provider settings to the context.\\n 866 | \\t\\t\\t\\t//\\n 867 | \\t\\t\\t\\t// Note: 1, 2, and 4 can be done in one `ContextProxy` call:\\n 868 | \\t\\t\\t\\t// this.contextProxy.setValues({ ...providerSettings, listApiConfigMeta: ..., currentApiConfigName: ... })\\n 869 | \\t\\t\\t\\t// We should probably switch to that and verify that it works.\\n 870 | \\t\\t\\t\\t// I left the original implementation in just to be safe.\\n 871 | \\t\\t\\t\\tawait Promise.all([\\n 872 | \\t\\t\\t\\t\\tthis.updateGlobalState(\\\"listApiConfigMeta\\\", await this.providerSettingsManager.listConfig()),\\n 873 | \\t\\t\\t\\t\\tthis.updateGlobalState(\\\"currentApiConfigName\\\", name),\\n 874 | \\t\\t\\t\\t\\tthis.providerSettingsManager.setModeConfig(mode, id),\\n 875 | \\t\\t\\t\\t\\tthis.contextProxy.setProviderSettings(providerSettings),\\n 876 | \\t\\t\\t\\t])\\n 877 | \\n 878 | \\t\\t\\t\\t// Notify CodeIndexManager about the settings change\\n 879 | \\t\\t\\t\\tif (this.codeIndexManager) {\\n 880 | \\t\\t\\t\\t\\tawait this.codeIndexManager.handleExternalSettingsChange()\\n 881 | \\t\\t\\t\\t}\\n 882 | \\n 883 | \\t\\t\\t\\t// Change the provider for the current task.\\n 884 | \\t\\t\\t\\t// TODO: We should rename `buildApiHandler` for clarity (e.g. `getProviderClient`).\\n 885 | \\t\\t\\t\\tconst task = this.getCurrentCline()\\n 886 | \\n 887 | \\t\\t\\t\\tif (task) {\\n 888 | \\t\\t\\t\\t\\ttask.api = buildApiHandler(providerSettings)\\n 889 | \\t\\t\\t\\t}\\n 890 | \\t\\t\\t} else {\\n 891 | \\t\\t\\t\\tawait this.updateGlobalState(\\\"listApiConfigMeta\\\", await this.providerSettingsManager.listConfig())\\n 892 | \\t\\t\\t}\\n 893 | \\n 894 | \\t\\t\\tawait this.postStateToWebview()\\n 895 | \\t\\t\\treturn id\\n 896 | \\t\\t} catch (error) {\\n 897 | \\t\\t\\tthis.log(\\n 898 | \\t\\t\\t\\t`Error create new api configuration: ${JSON.stringify(error, Object.getOwnPropertyNames(error), 2)}`,\\n 899 | \\t\\t\\t)\\n 900 | \\n 901 | \\t\\t\\tvscode.window.showErrorMessage(t(\\\"common:errors.create_api_config\\\"))\\n 902 | \\t\\t\\treturn undefined\\n 903 | \\t\\t}\\n 904 | \\t}\\n 905 | \\n 906 | \\tasync deleteProviderProfile(profileToDelete: ProviderSettingsEntry) {\\n 907 | \\t\\tconst globalSettings = this.contextProxy.getValues()\\n 908 | \\t\\tlet profileToActivate: string | undefined = globalSettings.currentApiConfigName\\n 909 | \\n 910 | \\t\\tif (profileToDelete.name === profileToActivate) {\\n 911 | \\t\\t\\tprofileToActivate = this.getProviderProfileEntries().find(({ name }) => name !== profileToDelete.name)?.name\\n 912 | \\t\\t}\\n 913 | \\n 914 | \\t\\tif (!profileToActivate) {\\n 915 | \\t\\t\\tthrow new Error(\\\"You cannot delete the last profile\\\")\\n 916 | \\t\\t}\\n 917 | \\n 918 | \\t\\tconst entries = this.getProviderProfileEntries().filter(({ name }) => name !== profileToDelete.name)\\n 919 | \\n 920 | \\t\\tawait this.contextProxy.setValues({\\n 921 | \\t\\t\\t...globalSettings,\\n 922 | \\t\\t\\tcurrentApiConfigName: profileToActivate,\\n 923 | \\t\\t\\tlistApiConfigMeta: entries,\\n 924 | \\t\\t})\\n 925 | \\n 926 | \\t\\tawait this.postStateToWebview()\\n 927 | \\t}\\n 928 | \\n 929 | \\tasync activateProviderProfile(args: { name: string } | { id: string }) {\\n 930 | \\t\\tconst { name, id, ...providerSettings } = await this.providerSettingsManager.activateProfile(args)\\n 931 | \\n 932 | \\t\\t// See `upsertProviderProfile` for a description of what this is doing.\\n 933 | \\t\\tawait Promise.all([\\n 934 | \\t\\t\\tthis.contextProxy.setValue(\\\"listApiConfigMeta\\\", await this.providerSettingsManager.listConfig()),\\n 935 | \\t\\t\\tthis.contextProxy.setValue(\\\"currentApiConfigName\\\", name),\\n 936 | \\t\\t\\tthis.contextProxy.setProviderSettings(providerSettings),\\n 937 | \\t\\t])\\n 938 | \\n 939 | \\t\\tconst { mode } = await this.getState()\\n 940 | \\n 941 | \\t\\tif (id) {\\n 942 | \\t\\t\\tawait this.providerSettingsManager.setModeConfig(mode, id)\\n 943 | \\t\\t}\\n 944 | \\n 945 | \\t\\t// Change the provider for the current task.\\n 946 | \\t\\tconst task = this.getCurrentCline()\\n 947 | \\n 948 | \\t\\tif (task) {\\n 949 | \\t\\t\\ttask.api = buildApiHandler(providerSettings)\\n 950 | \\t\\t}\\n 951 | \\n 952 | \\t\\tawait this.postStateToWebview()\\n 953 | \\t}\\n 954 | \\n 955 | \\t// Task Management\\n 956 | \\n 957 | \\tasync cancelTask() {\\n 958 | \\t\\tconst cline = this.getCurrentCline()\\n 959 | \\n 960 | \\t\\tif (!cline) {\\n 961 | \\t\\t\\treturn\\n 962 | \\t\\t}\\n 963 | \\n 964 | \\t\\tconsole.log(`[subtasks] cancelling task ${cline.taskId}.${cline.instanceId}`)\\n 965 | \\n 966 | \\t\\tconst { historyItem } = await this.getTaskWithId(cline.taskId)\\n 967 | \\t\\t// Preserve parent and root task information for history item.\\n 968 | \\t\\tconst rootTask = cline.rootTask\\n 969 | \\t\\tconst parentTask = cline.parentTask\\n 970 | \\n 971 | \\t\\tcline.abortTask()\\n 972 | \\n 973 | \\t\\tawait pWaitFor(\\n 974 | \\t\\t\\t() =>\\n 975 | \\t\\t\\t\\tthis.getCurrentCline()! === undefined ||\\n 976 | \\t\\t\\t\\tthis.getCurrentCline()!.isStreaming === false ||\\n 977 | \\t\\t\\t\\tthis.getCurrentCline()!.didFinishAbortingStream ||\\n 978 | \\t\\t\\t\\t// If only the first chunk is processed, then there's no\\n 979 | \\t\\t\\t\\t// need to wait for graceful abort (closes edits, browser,\\n 980 | \\t\\t\\t\\t// etc).\\n 981 | \\t\\t\\t\\tthis.getCurrentCline()!.isWaitingForFirstChunk,\\n 982 | \\t\\t\\t{\\n 983 | \\t\\t\\t\\ttimeout: 3_000,\\n 984 | \\t\\t\\t},\\n 985 | \\t\\t).catch(() => {\\n 986 | \\t\\t\\tconsole.error(\\\"Failed to abort task\\\")\\n 987 | \\t\\t})\\n 988 | \\n 989 | \\t\\tif (this.getCurrentCline()) {\\n 990 | \\t\\t\\t// 'abandoned' will prevent this Cline instance from affecting\\n 991 | \\t\\t\\t// future Cline instances. This may happen if its hanging on a\\n 992 | \\t\\t\\t// streaming request.\\n 993 | \\t\\t\\tthis.getCurrentCline()!.abandoned = true\\n 994 | \\t\\t}\\n 995 | \\n 996 | \\t\\t// Clears task again, so we need to abortTask manually above.\\n 997 | \\t\\tawait this.initClineWithHistoryItem({ ...historyItem, rootTask, parentTask })\\n 998 | \\t}\\n 999 | \\n1000 | \\tasync updateCustomInstructions(instructions?: string) {\\n1001 | \\t\\t// User may be clearing the field.\\n1002 | \\t\\tawait this.updateGlobalState(\\\"customInstructions\\\", instructions || undefined)\\n1003 | \\t\\tawait this.postStateToWebview()\\n1004 | \\t}\\n1005 | \\n1006 | \\t// MCP\\n1007 | \\n1008 | \\tasync ensureMcpServersDirectoryExists(): Promise<string> {\\n1009 | \\t\\t// Get platform-specific application data directory\\n1010 | \\t\\tlet mcpServersDir: string\\n1011 | \\t\\tif (process.platform === \\\"win32\\\") {\\n1012 | \\t\\t\\t// Windows: %APPDATA%\\\\Roo-Code\\\\MCP\\n1013 | \\t\\t\\tmcpServersDir = path.join(os.homedir(), \\\"AppData\\\", \\\"Roaming\\\", \\\"Roo-Code\\\", \\\"MCP\\\")\\n1014 | \\t\\t} else if (process.platform === \\\"darwin\\\") {\\n1015 | \\t\\t\\t// macOS: ~/Documents/Cline/MCP\\n1016 | \\t\\t\\tmcpServersDir = path.join(os.homedir(), \\\"Documents\\\", \\\"Cline\\\", \\\"MCP\\\")\\n1017 | \\t\\t} else {\\n1018 | \\t\\t\\t// Linux: ~/.local/share/Cline/MCP\\n1019 | \\t\\t\\tmcpServersDir = path.join(os.homedir(), \\\".local\\\", \\\"share\\\", \\\"Roo-Code\\\", \\\"MCP\\\")\\n1020 | \\t\\t}\\n1021 | \\n1022 | \\t\\ttry {\\n1023 | \\t\\t\\tawait fs.mkdir(mcpServersDir, { recursive: true })\\n1024 | \\t\\t} catch (error) {\\n1025 | \\t\\t\\t// Fallback to a relative path if directory creation fails\\n1026 | \\t\\t\\treturn path.join(os.homedir(), \\\".roo-code\\\", \\\"mcp\\\")\\n1027 | \\t\\t}\\n1028 | \\t\\treturn mcpServersDir\\n1029 | \\t}\\n1030 | \\n1031 | \\tasync ensureSettingsDirectoryExists(): Promise<string> {\\n1032 | \\t\\tconst { getSettingsDirectoryPath } = await import(\\\"../../utils/storage\\\")\\n1033 | \\t\\tconst globalStoragePath = this.contextProxy.globalStorageUri.fsPath\\n1034 | \\t\\treturn getSettingsDirectoryPath(globalStoragePath)\\n1035 | \\t}\\n1036 | \\n1037 | \\t// OpenRouter\\n1038 | \\n1039 | \\tasync handleOpenRouterCallback(code: string) {\\n1040 | \\t\\tlet { apiConfiguration, currentApiConfigName } = await this.getState()\\n1041 | \\n1042 | \\t\\tlet apiKey: string\\n1043 | \\t\\ttry {\\n1044 | \\t\\t\\tconst baseUrl = apiConfiguration.openRouterBaseUrl || \\\"https://openrouter.ai/api/v1\\\"\\n1045 | \\t\\t\\t// Extract the base domain for the auth endpoint\\n1046 | \\t\\t\\tconst baseUrlDomain = baseUrl.match(/^(https?:\\\\/\\\\/[^\\\\/]+)/)?.[1] || \\\"https://openrouter.ai\\\"\\n1047 | \\t\\t\\tconst response = await axios.post(`${baseUrlDomain}/api/v1/auth/keys`, { code })\\n1048 | \\t\\t\\tif (response.data && response.data.key) {\\n1049 | \\t\\t\\t\\tapiKey = response.data.key\\n1050 | \\t\\t\\t} else {\\n1051 | \\t\\t\\t\\tthrow new Error(\\\"Invalid response from OpenRouter API\\\")\\n1052 | \\t\\t\\t}\\n1053 | \\t\\t} catch (error) {\\n1054 | \\t\\t\\tthis.log(\\n1055 | \\t\\t\\t\\t`Error exchanging code for API key: ${JSON.stringify(error, Object.getOwnPropertyNames(error), 2)}`,\\n1056 | \\t\\t\\t)\\n1057 | \\t\\t\\tthrow error\\n1058 | \\t\\t}\\n1059 | \\n1060 | \\t\\tconst newConfiguration: ProviderSettings = {\\n1061 | \\t\\t\\t...apiConfiguration,\\n1062 | \\t\\t\\tapiProvider: \\\"openrouter\\\",\\n1063 | \\t\\t\\topenRouterApiKey: apiKey,\\n1064 | \\t\\t\\topenRouterModelId: apiConfiguration?.openRouterModelId || openRouterDefaultModelId,\\n1065 | \\t\\t}\\n1066 | \\n1067 | \\t\\tawait this.upsertProviderProfile(currentApiConfigName, newConfiguration)\\n1068 | \\t}\\n1069 | \\n1070 | \\t// Glama\\n1071 | \\n1072 | \\tasync handleGlamaCallback(code: string) {\\n1073 | \\t\\tlet apiKey: string\\n1074 | \\t\\ttry {\\n1075 | \\t\\t\\tconst response = await axios.post(\\\"https://glama.ai/api/gateway/v1/auth/exchange-code\\\", { code })\\n1076 | \\t\\t\\tif (response.data && response.data.apiKey) {\\n1077 | \\t\\t\\t\\tapiKey = response.data.apiKey\\n1078 | \\t\\t\\t} else {\\n1079 | \\t\\t\\t\\tthrow new Error(\\\"Invalid response from Glama API\\\")\\n1080 | \\t\\t\\t}\\n1081 | \\t\\t} catch (error) {\\n1082 | \\t\\t\\tthis.log(\\n1083 | \\t\\t\\t\\t`Error exchanging code for API key: ${JSON.stringify(error, Object.getOwnPropertyNames(error), 2)}`,\\n1084 | \\t\\t\\t)\\n1085 | \\t\\t\\tthrow error\\n1086 | \\t\\t}\\n1087 | \\n1088 | \\t\\tconst { apiConfiguration, currentApiConfigName } = await this.getState()\\n1089 | \\n1090 | \\t\\tconst newConfiguration: ProviderSettings = {\\n1091 | \\t\\t\\t...apiConfiguration,\\n1092 | \\t\\t\\tapiProvider: \\\"glama\\\",\\n1093 | \\t\\t\\tglamaApiKey: apiKey,\\n1094 | \\t\\t\\tglamaModelId: apiConfiguration?.glamaModelId || glamaDefaultModelId,\\n1095 | \\t\\t}\\n1096 | \\n1097 | \\t\\tawait this.upsertProviderProfile(currentApiConfigName, newConfiguration)\\n1098 | \\t}\\n1099 | \\n1100 | \\t// Requesty\\n1101 | \\n1102 | \\tasync handleRequestyCallback(code: string) {\\n1103 | \\t\\tlet { apiConfiguration, currentApiConfigName } = await this.getState()\\n1104 | \\n1105 | \\t\\tconst newConfiguration: ProviderSettings = {\\n1106 | \\t\\t\\t...apiConfiguration,\\n1107 | \\t\\t\\tapiProvider: \\\"requesty\\\",\\n1108 | \\t\\t\\trequestyApiKey: code,\\n1109 | \\t\\t\\trequestyModelId: apiConfiguration?.requestyModelId || requestyDefaultModelId,\\n1110 | \\t\\t}\\n1111 | \\n1112 | \\t\\tawait this.upsertProviderProfile(currentApiConfigName, newConfiguration)\\n1113 | \\t}\\n1114 | \\n1115 | \\t// Task history\\n1116 | \\n1117 | \\tasync getTaskWithId(id: string): Promise<{\\n1118 | \\t\\thistoryItem: HistoryItem\\n1119 | \\t\\ttaskDirPath: string\\n1120 | \\t\\tapiConversationHistoryFilePath: string\\n1121 | \\t\\tuiMessagesFilePath: string\\n1122 | \\t\\tapiConversationHistory: Anthropic.MessageParam[]\\n1123 | \\t}> {\\n1124 | \\t\\tconst history = this.getGlobalState(\\\"taskHistory\\\") ?? []\\n1125 | \\t\\tconst historyItem = history.find((item) => item.id === id)\\n1126 | \\n1127 | \\t\\tif (historyItem) {\\n1128 | \\t\\t\\tconst { getTaskDirectoryPath } = await import(\\\"../../utils/storage\\\")\\n1129 | \\t\\t\\tconst globalStoragePath = this.contextProxy.globalStorageUri.fsPath\\n1130 | \\t\\t\\tconst taskDirPath = await getTaskDirectoryPath(globalStoragePath, id)\\n1131 | \\t\\t\\tconst apiConversationHistoryFilePath = path.join(taskDirPath, GlobalFileNames.apiConversationHistory)\\n1132 | \\t\\t\\tconst uiMessagesFilePath = path.join(taskDirPath, GlobalFileNames.uiMessages)\\n1133 | \\t\\t\\tconst fileExists = await fileExistsAtPath(apiConversationHistoryFilePath)\\n1134 | \\n1135 | \\t\\t\\tif (fileExists) {\\n1136 | \\t\\t\\t\\tconst apiConversationHistory = JSON.parse(await fs.readFile(apiConversationHistoryFilePath, \\\"utf8\\\"))\\n1137 | \\n1138 | \\t\\t\\t\\treturn {\\n1139 | \\t\\t\\t\\t\\thistoryItem,\\n1140 | \\t\\t\\t\\t\\ttaskDirPath,\\n1141 | \\t\\t\\t\\t\\tapiConversationHistoryFilePath,\\n1142 | \\t\\t\\t\\t\\tuiMessagesFilePath,\\n1143 | \\t\\t\\t\\t\\tapiConversationHistory,\\n1144 | \\t\\t\\t\\t}\\n1145 | \\t\\t\\t}\\n1146 | \\t\\t}\\n1147 | \\n1148 | \\t\\t// if we tried to get a task that doesn't exist, remove it from state\\n1149 | \\t\\t// FIXME: this seems to happen sometimes when the json file doesnt save to disk for some reason\\n1150 | \\t\\tawait this.deleteTaskFromState(id)\\n1151 | \\t\\tthrow new Error(\\\"Task not found\\\")\\n1152 | \\t}\\n1153 | \\n1154 | \\tasync showTaskWithId(id: string) {\\n1155 | \\t\\tif (id !== this.getCurrentCline()?.taskId) {\\n1156 | \\t\\t\\t// Non-current task.\\n1157 | \\t\\t\\tconst { historyItem } = await this.getTaskWithId(id)\\n1158 | \\t\\t\\tawait this.initClineWithHistoryItem(historyItem) // Clears existing task.\\n1159 | \\t\\t}\\n1160 | \\n1161 | \\t\\tawait this.postMessageToWebview({ type: \\\"action\\\", action: \\\"chatButtonClicked\\\" })\\n1162 | \\t}\\n1163 | \\n1164 | \\tasync exportTaskWithId(id: string) {\\n1165 | \\t\\tconst { historyItem, apiConversationHistory } = await this.getTaskWithId(id)\\n1166 | \\t\\tawait downloadTask(historyItem.ts, apiConversationHistory)\\n1167 | \\t}\\n1168 | \\n1169 | \\t/* Condenses a task's message history to use fewer tokens. */\\n1170 | \\tasync condenseTaskContext(taskId: string) {\\n1171 | \\t\\tlet task: Task | undefined\\n1172 | \\t\\tfor (let i = this.clineStack.length - 1; i >= 0; i--) {\\n1173 | \\t\\t\\tif (this.clineStack[i].taskId === taskId) {\\n1174 | \\t\\t\\t\\ttask = this.clineStack[i]\\n1175 | \\t\\t\\t\\tbreak\\n1176 | \\t\\t\\t}\\n1177 | \\t\\t}\\n1178 | \\t\\tif (!task) {\\n1179 | \\t\\t\\tthrow new Error(`Task with id ${taskId} not found in stack`)\\n1180 | \\t\\t}\\n1181 | \\t\\tawait task.condenseContext()\\n1182 | \\t\\tawait this.postMessageToWebview({ type: \\\"condenseTaskContextResponse\\\", text: taskId })\\n1183 | \\t}\\n1184 | \\n1185 | \\t// this function deletes a task from task hidtory, and deletes it's checkpoints and delete the task folder\\n1186 | \\tasync deleteTaskWithId(id: string) {\\n1187 | \\t\\ttry {\\n1188 | \\t\\t\\t// get the task directory full path\\n1189 | \\t\\t\\tconst { taskDirPath } = await this.getTaskWithId(id)\\n1190 | \\n1191 | \\t\\t\\t// remove task from stack if it's the current task\\n1192 | \\t\\t\\tif (id === this.getCurrentCline()?.taskId) {\\n1193 | \\t\\t\\t\\t// if we found the taskid to delete - call finish to abort this task and allow a new task to be started,\\n1194 | \\t\\t\\t\\t// if we are deleting a subtask and parent task is still waiting for subtask to finish - it allows the parent to resume (this case should neve exist)\\n1195 | \\t\\t\\t\\tawait this.finishSubTask(t(\\\"common:tasks.deleted\\\"))\\n1196 | \\t\\t\\t}\\n1197 | \\n1198 | \\t\\t\\t// delete task from the task history state\\n1199 | \\t\\t\\tawait this.deleteTaskFromState(id)\\n1200 | \\n1201 | \\t\\t\\t// Delete associated shadow repository or branch.\\n1202 | \\t\\t\\t// TODO: Store `workspaceDir` in the `HistoryItem` object.\\n1203 | \\t\\t\\tconst globalStorageDir = this.contextProxy.globalStorageUri.fsPath\\n1204 | \\t\\t\\tconst workspaceDir = this.cwd\\n1205 | \\n1206 | \\t\\t\\ttry {\\n1207 | \\t\\t\\t\\tawait ShadowCheckpointService.deleteTask({ taskId: id, globalStorageDir, workspaceDir })\\n1208 | \\t\\t\\t} catch (error) {\\n1209 | \\t\\t\\t\\tconsole.error(\\n1210 | \\t\\t\\t\\t\\t`[deleteTaskWithId${id}] failed to delete associated shadow repository or branch: ${error instanceof Error ? error.message : String(error)}`,\\n1211 | \\t\\t\\t\\t)\\n1212 | \\t\\t\\t}\\n1213 | \\n1214 | \\t\\t\\t// delete the entire task directory including checkpoints and all content\\n1215 | \\t\\t\\ttry {\\n1216 | \\t\\t\\t\\tawait fs.rm(taskDirPath, { recursive: true, force: true })\\n1217 | \\t\\t\\t\\tconsole.log(`[deleteTaskWithId${id}] removed task directory`)\\n1218 | \\t\\t\\t} catch (error) {\\n1219 | \\t\\t\\t\\tconsole.error(\\n1220 | \\t\\t\\t\\t\\t`[deleteTaskWithId${id}] failed to remove task directory: ${error instanceof Error ? error.message : String(error)}`,\\n1221 | \\t\\t\\t\\t)\\n1222 | \\t\\t\\t}\\n1223 | \\t\\t} catch (error) {\\n1224 | \\t\\t\\t// If task is not found, just remove it from state\\n1225 | \\t\\t\\tif (error instanceof Error && error.message === \\\"Task not found\\\") {\\n1226 | \\t\\t\\t\\tawait this.deleteTaskFromState(id)\\n1227 | \\t\\t\\t\\treturn\\n1228 | \\t\\t\\t}\\n1229 | \\t\\t\\tthrow error\\n1230 | \\t\\t}\\n1231 | \\t}\\n1232 | \\n1233 | \\tasync deleteTaskFromState(id: string) {\\n1234 | \\t\\tconst taskHistory = this.getGlobalState(\\\"taskHistory\\\") ?? []\\n1235 | \\t\\tconst updatedTaskHistory = taskHistory.filter((task) => task.id !== id)\\n1236 | \\t\\tawait this.updateGlobalState(\\\"taskHistory\\\", updatedTaskHistory)\\n1237 | \\t\\tawait this.postStateToWebview()\\n1238 | \\t}\\n1239 | \\n1240 | \\tasync postStateToWebview() {\\n1241 | \\t\\tconst state = await this.getStateToPostToWebview()\\n1242 | \\t\\tthis.postMessageToWebview({ type: \\\"state\\\", state })\\n1243 | \\n1244 | \\t\\t// Update VSCode context for experiments\\n1245 | \\t\\tawait this.updateVSCodeContext()\\n1246 | \\t}\\n1247 | \\n1248 | \\t/**\\n1249 | \\t * Updates VSCode context variables for experiments so they can be used in when clauses\\n1250 | \\t */\\n1251 | \\tprivate async updateVSCodeContext() {\\n1252 | \\t\\tconst { experiments } = await this.getState()\\n1253 | \\n1254 | \\t\\t// Set context for marketplace experiment\\n1255 | \\t\\tawait vscode.commands.executeCommand(\\n1256 | \\t\\t\\t\\\"setContext\\\",\\n1257 | \\t\\t\\t`${Package.name}.marketplaceEnabled`,\\n1258 | \\t\\t\\texperiments.marketplace ?? false,\\n1259 | \\t\\t)\\n1260 | \\t}\\n1261 | \\n1262 | \\t/**\\n1263 | \\t * Checks if there is a file-based system prompt override for the given mode\\n1264 | \\t */\\n1265 | \\tasync hasFileBasedSystemPromptOverride(mode: Mode): Promise<boolean> {\\n1266 | \\t\\tconst promptFilePath = getSystemPromptFilePath(this.cwd, mode)\\n1267 | \\t\\treturn await fileExistsAtPath(promptFilePath)\\n1268 | \\t}\\n1269 | \\n1270 | \\tasync getStateToPostToWebview() {\\n1271 | \\t\\tconst {\\n1272 | \\t\\t\\tapiConfiguration,\\n1273 | \\t\\t\\tlastShownAnnouncementId,\\n1274 | \\t\\t\\tcustomInstructions,\\n1275 | \\t\\t\\talwaysAllowReadOnly,\\n1276 | \\t\\t\\talwaysAllowReadOnlyOutsideWorkspace,\\n1277 | \\t\\t\\talwaysAllowWrite,\\n1278 | \\t\\t\\talwaysAllowWriteOutsideWorkspace,\\n1279 | \\t\\t\\talwaysAllowExecute,\\n1280 | \\t\\t\\talwaysAllowBrowser,\\n1281 | \\t\\t\\talwaysAllowMcp,\\n1282 | \\t\\t\\talwaysAllowModeSwitch,\\n1283 | \\t\\t\\talwaysAllowSubtasks,\\n1284 | \\t\\t\\tallowedMaxRequests,\\n1285 | \\t\\t\\tautoCondenseContext,\\n1286 | \\t\\t\\tautoCondenseContextPercent,\\n1287 | \\t\\t\\tsoundEnabled,\\n1288 | \\t\\t\\tttsEnabled,\\n1289 | \\t\\t\\tttsSpeed,\\n1290 | \\t\\t\\tdiffEnabled,\\n1291 | \\t\\t\\tenableCheckpoints,\\n1292 | \\t\\t\\ttaskHistory,\\n1293 | \\t\\t\\tsoundVolume,\\n1294 | \\t\\t\\tbrowserViewportSize,\\n1295 | \\t\\t\\tscreenshotQuality,\\n1296 | \\t\\t\\tremoteBrowserHost,\\n1297 | \\t\\t\\tremoteBrowserEnabled,\\n1298 | \\t\\t\\tcachedChromeHostUrl,\\n1299 | \\t\\t\\twriteDelayMs,\\n1300 | \\t\\t\\tterminalOutputLineLimit,\\n1301 | \\t\\t\\tterminalShellIntegrationTimeout,\\n1302 | \\t\\t\\tterminalShellIntegrationDisabled,\\n1303 | \\t\\t\\tterminalCommandDelay,\\n1304 | \\t\\t\\tterminalPowershellCounter,\\n1305 | \\t\\t\\tterminalZshClearEolMark,\\n1306 | \\t\\t\\tterminalZshOhMy,\\n1307 | \\t\\t\\tterminalZshP10k,\\n1308 | \\t\\t\\tterminalZdotdir,\\n1309 | \\t\\t\\tfuzzyMatchThreshold,\\n1310 | \\t\\t\\tmcpEnabled,\\n1311 | \\t\\t\\tenableMcpServerCreation,\\n1312 | \\t\\t\\talwaysApproveResubmit,\\n1313 | \\t\\t\\trequestDelaySeconds,\\n1314 | \\t\\t\\tcurrentApiConfigName,\\n1315 | \\t\\t\\tlistApiConfigMeta,\\n1316 | \\t\\t\\tpinnedApiConfigs,\\n1317 | \\t\\t\\tmode,\\n1318 | \\t\\t\\tcustomModePrompts,\\n1319 | \\t\\t\\tcustomSupportPrompts,\\n1320 | \\t\\t\\tenhancementApiConfigId,\\n1321 | \\t\\t\\tautoApprovalEnabled,\\n1322 | \\t\\t\\tcustomModes,\\n1323 | \\t\\t\\texperiments,\\n1324 | \\t\\t\\tmaxOpenTabsContext,\\n1325 | \\t\\t\\tmaxWorkspaceFiles,\\n1326 | \\t\\t\\tbrowserToolEnabled,\\n1327 | \\t\\t\\ttelemetrySetting,\\n1328 | \\t\\t\\tshowRooIgnoredFiles,\\n1329 | \\t\\t\\tlanguage,\\n1330 | \\t\\t\\tmaxReadFileLine,\\n1331 | \\t\\t\\tterminalCompressProgressBar,\\n1332 | \\t\\t\\thistoryPreviewCollapsed,\\n1333 | \\t\\t\\tcloudUserInfo,\\n1334 | \\t\\t\\tcloudIsAuthenticated,\\n1335 | \\t\\t\\tsharingEnabled,\\n1336 | \\t\\t\\torganizationAllowList,\\n1337 | \\t\\t\\tmaxConcurrentFileReads,\\n1338 | \\t\\t\\tcondensingApiConfigId,\\n1339 | \\t\\t\\tcustomCondensingPrompt,\\n1340 | \\t\\t\\tcodebaseIndexConfig,\\n1341 | \\t\\t\\tcodebaseIndexModels,\\n1342 | \\t\\t} = await this.getState()\\n1343 | \\n1344 | \\t\\tconst telemetryKey = process.env.POSTHOG_API_KEY\\n1345 | \\t\\tconst machineId = vscode.env.machineId\\n1346 | \\t\\tconst allowedCommands = vscode.workspace.getConfiguration(Package.name).get<string[]>(\\\"allowedCommands\\\") || []\\n1347 | \\t\\tconst cwd = this.cwd\\n1348 | \\n1349 | \\t\\t// Only fetch marketplace data if the feature is enabled\\n1350 | \\t\\tlet marketplaceItems: any[] = []\\n1351 | \\t\\tlet marketplaceInstalledMetadata: any = { project: {}, global: {} }\\n1352 | \\n1353 | \\t\\tif (experiments.marketplace) {\\n1354 | \\t\\t\\tmarketplaceItems = (await this.marketplaceManager.getCurrentItems()) || []\\n1355 | \\t\\t\\tmarketplaceInstalledMetadata = await this.marketplaceManager.getInstallationMetadata()\\n1356 | \\t\\t}\\n1357 | \\n1358 | \\t\\t// Check if there's a system prompt override for the current mode\\n1359 | \\t\\tconst currentMode = mode ?? defaultModeSlug\\n1360 | \\t\\tconst hasSystemPromptOverride = await this.hasFileBasedSystemPromptOverride(currentMode)\\n1361 | \\n1362 | \\t\\treturn {\\n1363 | \\t\\t\\tversion: this.context.extension?.packageJSON?.version ?? \\\"\\\",\\n1364 | \\t\\t\\tmarketplaceItems,\\n1365 | \\t\\t\\tmarketplaceInstalledMetadata,\\n1366 | \\t\\t\\tapiConfiguration,\\n1367 | \\t\\t\\tcustomInstructions,\\n1368 | \\t\\t\\talwaysAllowReadOnly: alwaysAllowReadOnly ?? false,\\n1369 | \\t\\t\\talwaysAllowReadOnlyOutsideWorkspace: alwaysAllowReadOnlyOutsideWorkspace ?? false,\\n1370 | \\t\\t\\talwaysAllowWrite: alwaysAllowWrite ?? false,\\n1371 | \\t\\t\\talwaysAllowWriteOutsideWorkspace: alwaysAllowWriteOutsideWorkspace ?? false,\\n1372 | \\t\\t\\talwaysAllowExecute: alwaysAllowExecute ?? false,\\n1373 | \\t\\t\\talwaysAllowBrowser: alwaysAllowBrowser ?? false,\\n1374 | \\t\\t\\talwaysAllowMcp: alwaysAllowMcp ?? false,\\n1375 | \\t\\t\\talwaysAllowModeSwitch: alwaysAllowModeSwitch ?? false,\\n1376 | \\t\\t\\talwaysAllowSubtasks: alwaysAllowSubtasks ?? false,\\n1377 | \\t\\t\\tallowedMaxRequests,\\n1378 | \\t\\t\\tautoCondenseContext: autoCondenseContext ?? true,\\n1379 | \\t\\t\\tautoCondenseContextPercent: autoCondenseContextPercent ?? 100,\\n1380 | \\t\\t\\turiScheme: vscode.env.uriScheme,\\n1381 | \\t\\t\\tcurrentTaskItem: this.getCurrentCline()?.taskId\\n1382 | \\t\\t\\t\\t? (taskHistory || []).find((item: HistoryItem) => item.id === this.getCurrentCline()?.taskId)\\n1383 | \\t\\t\\t\\t: undefined,\\n1384 | \\t\\t\\tclineMessages: this.getCurrentCline()?.clineMessages || [],\\n1385 | \\t\\t\\ttaskHistory: (taskHistory || [])\\n1386 | \\t\\t\\t\\t.filter((item: HistoryItem) => item.ts && item.task)\\n1387 | \\t\\t\\t\\t.sort((a: HistoryItem, b: HistoryItem) => b.ts - a.ts),\\n1388 | \\t\\t\\tsoundEnabled: soundEnabled ?? false,\\n1389 | \\t\\t\\tttsEnabled: ttsEnabled ?? false,\\n1390 | \\t\\t\\tttsSpeed: ttsSpeed ?? 1.0,\\n1391 | \\t\\t\\tdiffEnabled: diffEnabled ?? true,\\n1392 | \\t\\t\\tenableCheckpoints: enableCheckpoints ?? true,\\n1393 | \\t\\t\\tshouldShowAnnouncement:\\n1394 | \\t\\t\\t\\ttelemetrySetting !== \\\"unset\\\" && lastShownAnnouncementId !== this.latestAnnouncementId,\\n1395 | \\t\\t\\tallowedCommands,\\n1396 | \\t\\t\\tsoundVolume: soundVolume ?? 0.5,\\n1397 | \\t\\t\\tbrowserViewportSize: browserViewportSize ?? \\\"900x600\\\",\\n1398 | \\t\\t\\tscreenshotQuality: screenshotQuality ?? 75,\\n1399 | \\t\\t\\tremoteBrowserHost,\\n1400 | \\t\\t\\tremoteBrowserEnabled: remoteBrowserEnabled ?? false,\\n1401 | \\t\\t\\tcachedChromeHostUrl: cachedChromeHostUrl,\\n1402 | \\t\\t\\twriteDelayMs: writeDelayMs ?? 1000,\\n1403 | \\t\\t\\tterminalOutputLineLimit: terminalOutputLineLimit ?? 500,\\n1404 | \\t\\t\\tterminalShellIntegrationTimeout: terminalShellIntegrationTimeout ?? Terminal.defaultShellIntegrationTimeout,\\n1405 | \\t\\t\\tterminalShellIntegrationDisabled: terminalShellIntegrationDisabled ?? false,\\n1406 | \\t\\t\\tterminalCommandDelay: terminalCommandDelay ?? 0,\\n1407 | \\t\\t\\tterminalPowershellCounter: terminalPowershellCounter ?? false,\\n1408 | \\t\\t\\tterminalZshClearEolMark: terminalZshClearEolMark ?? true,\\n1409 | \\t\\t\\tterminalZshOhMy: terminalZshOhMy ?? false,\\n1410 | \\t\\t\\tterminalZshP10k: terminalZshP10k ?? false,\\n1411 | \\t\\t\\tterminalZdotdir: terminalZdotdir ?? false,\\n1412 | \\t\\t\\tfuzzyMatchThreshold: fuzzyMatchThreshold ?? 1.0,\\n1413 | \\t\\t\\tmcpEnabled: mcpEnabled ?? true,\\n1414 | \\t\\t\\tenableMcpServerCreation: enableMcpServerCreation ?? true,\\n1415 | \\t\\t\\talwaysApproveResubmit: alwaysApproveResubmit ?? false,\\n1416 | \\t\\t\\trequestDelaySeconds: requestDelaySeconds ?? 10,\\n1417 | \\t\\t\\tcurrentApiConfigName: currentApiConfigName ?? \\\"default\\\",\\n1418 | \\t\\t\\tlistApiConfigMeta: listApiConfigMeta ?? [],\\n1419 | \\t\\t\\tpinnedApiConfigs: pinnedApiConfigs ?? {},\\n1420 | \\t\\t\\tmode: mode ?? defaultModeSlug,\\n1421 | \\t\\t\\tcustomModePrompts: customModePrompts ?? {},\\n1422 | \\t\\t\\tcustomSupportPrompts: customSupportPrompts ?? {},\\n1423 | \\t\\t\\tenhancementApiConfigId,\\n1424 | \\t\\t\\tautoApprovalEnabled: autoApprovalEnabled ?? false,\\n1425 | \\t\\t\\tcustomModes,\\n1426 | \\t\\t\\texperiments: experiments ?? experimentDefault,\\n1427 | \\t\\t\\tmcpServers: this.mcpHub?.getAllServers() ?? [],\\n1428 | \\t\\t\\tmaxOpenTabsContext: maxOpenTabsContext ?? 20,\\n1429 | \\t\\t\\tmaxWorkspaceFiles: maxWorkspaceFiles ?? 200,\\n1430 | \\t\\t\\tcwd,\\n1431 | \\t\\t\\tbrowserToolEnabled: browserToolEnabled ?? true,\\n1432 | \\t\\t\\ttelemetrySetting,\\n1433 | \\t\\t\\ttelemetryKey,\\n1434 | \\t\\t\\tmachineId,\\n1435 | \\t\\t\\tshowRooIgnoredFiles: showRooIgnoredFiles ?? true,\\n1436 | \\t\\t\\tlanguage: language ?? formatLanguage(vscode.env.language),\\n1437 | \\t\\t\\trenderContext: this.renderContext,\\n1438 | \\t\\t\\tmaxReadFileLine: maxReadFileLine ?? -1,\\n1439 | \\t\\t\\tmaxConcurrentFileReads: maxConcurrentFileReads ?? 5,\\n1440 | \\t\\t\\tsettingsImportedAt: this.settingsImportedAt,\\n1441 | \\t\\t\\tterminalCompressProgressBar: terminalCompressProgressBar ?? true,\\n1442 | \\t\\t\\thasSystemPromptOverride,\\n1443 | \\t\\t\\thistoryPreviewCollapsed: historyPreviewCollapsed ?? false,\\n1444 | \\t\\t\\tcloudUserInfo,\\n1445 | \\t\\t\\tcloudIsAuthenticated: cloudIsAuthenticated ?? false,\\n1446 | \\t\\t\\tsharingEnabled: sharingEnabled ?? false,\\n1447 | \\t\\t\\torganizationAllowList,\\n1448 | \\t\\t\\tcondensingApiConfigId,\\n1449 | \\t\\t\\tcustomCondensingPrompt,\\n1450 | \\t\\t\\tcodebaseIndexModels: codebaseIndexModels ?? EMBEDDING_MODEL_PROFILES,\\n1451 | \\t\\t\\tcodebaseIndexConfig: codebaseIndexConfig ?? {\\n1452 | \\t\\t\\t\\tcodebaseIndexEnabled: false,\\n1453 | \\t\\t\\t\\tcodebaseIndexQdrantUrl: \\\"http://localhost:6333\\\",\\n1454 | \\t\\t\\t\\tcodebaseIndexEmbedderProvider: \\\"openai\\\",\\n1455 | \\t\\t\\t\\tcodebaseIndexEmbedderBaseUrl: \\\"\\\",\\n1456 | \\t\\t\\t\\tcodebaseIndexEmbedderModelId: \\\"\\\",\\n1457 | \\t\\t\\t},\\n1458 | \\t\\t}\\n1459 | \\t}\\n1460 | \\n1461 | \\t/**\\n1462 | \\t * Storage\\n1463 | \\t * https://dev.to/kompotkot/how-to-use-secretstorage-in-your-vscode-extensions-2hco\\n1464 | \\t * https://www.eliostruyf.com/devhack-code-extension-storage-options/\\n1465 | \\t */\\n1466 | \\n1467 | \\tasync getState() {\\n1468 | \\t\\tconst stateValues = this.contextProxy.getValues()\\n1469 | \\t\\tconst customModes = await this.customModesManager.getCustomModes()\\n1470 | \\n1471 | \\t\\t// Determine apiProvider with the same logic as before.\\n1472 | \\t\\tconst apiProvider: ProviderName = stateValues.apiProvider ? stateValues.apiProvider : \\\"anthropic\\\"\\n1473 | \\n1474 | \\t\\t// Build the apiConfiguration object combining state values and secrets.\\n1475 | \\t\\tconst providerSettings = this.contextProxy.getProviderSettings()\\n1476 | \\n1477 | \\t\\t// Ensure apiProvider is set properly if not already in state\\n1478 | \\t\\tif (!providerSettings.apiProvider) {\\n1479 | \\t\\t\\tproviderSettings.apiProvider = apiProvider\\n1480 | \\t\\t}\\n1481 | \\n1482 | \\t\\tlet organizationAllowList = ORGANIZATION_ALLOW_ALL\\n1483 | \\n1484 | \\t\\ttry {\\n1485 | \\t\\t\\torganizationAllowList = await CloudService.instance.getAllowList()\\n1486 | \\t\\t} catch (error) {\\n1487 | \\t\\t\\tconsole.error(\\n1488 | \\t\\t\\t\\t`[getState] failed to get organization allow list: ${error instanceof Error ? error.message : String(error)}`,\\n1489 | \\t\\t\\t)\\n1490 | \\t\\t}\\n1491 | \\n1492 | \\t\\tlet cloudUserInfo: CloudUserInfo | null = null\\n1493 | \\n1494 | \\t\\ttry {\\n1495 | \\t\\t\\tcloudUserInfo = CloudService.instance.getUserInfo()\\n1496 | \\t\\t} catch (error) {\\n1497 | \\t\\t\\tconsole.error(\\n1498 | \\t\\t\\t\\t`[getState] failed to get cloud user info: ${error instanceof Error ? error.message : String(error)}`,\\n1499 | \\t\\t\\t)\\n1500 | \\t\\t}\\n1501 | \\n1502 | \\t\\tlet cloudIsAuthenticated: boolean = false\\n1503 | \\n1504 | \\t\\ttry {\\n1505 | \\t\\t\\tcloudIsAuthenticated = CloudService.instance.isAuthenticated()\\n1506 | \\t\\t} catch (error) {\\n1507 | \\t\\t\\tconsole.error(\\n1508 | \\t\\t\\t\\t`[getState] failed to get cloud authentication state: ${error instanceof Error ? error.message : String(error)}`,\\n1509 | \\t\\t\\t)\\n1510 | \\t\\t}\\n1511 | \\n1512 | \\t\\tlet sharingEnabled: boolean = false\\n1513 | \\n1514 | \\t\\ttry {\\n1515 | \\t\\t\\tsharingEnabled = await CloudService.instance.canShareTask()\\n1516 | \\t\\t} catch (error) {\\n1517 | \\t\\t\\tconsole.error(\\n1518 | \\t\\t\\t\\t`[getState] failed to get sharing enabled state: ${error instanceof Error ? error.message : String(error)}`,\\n1519 | \\t\\t\\t)\\n1520 | \\t\\t}\\n1521 | \\n1522 | \\t\\t// Return the same structure as before\\n1523 | \\t\\treturn {\\n1524 | \\t\\t\\tapiConfiguration: providerSettings,\\n1525 | \\t\\t\\tlastShownAnnouncementId: stateValues.lastShownAnnouncementId,\\n1526 | \\t\\t\\tcustomInstructions: stateValues.customInstructions,\\n1527 | \\t\\t\\tapiModelId: stateValues.apiModelId,\\n1528 | \\t\\t\\talwaysAllowReadOnly: stateValues.alwaysAllowReadOnly ?? false,\\n1529 | \\t\\t\\talwaysAllowReadOnlyOutsideWorkspace: stateValues.alwaysAllowReadOnlyOutsideWorkspace ?? false,\\n1530 | \\t\\t\\talwaysAllowWrite: stateValues.alwaysAllowWrite ?? false,\\n1531 | \\t\\t\\talwaysAllowWriteOutsideWorkspace: stateValues.alwaysAllowWriteOutsideWorkspace ?? false,\\n1532 | \\t\\t\\talwaysAllowExecute: stateValues.alwaysAllowExecute ?? false,\\n1533 | \\t\\t\\talwaysAllowBrowser: stateValues.alwaysAllowBrowser ?? false,\\n1534 | \\t\\t\\talwaysAllowMcp: stateValues.alwaysAllowMcp ?? false,\\n1535 | \\t\\t\\talwaysAllowModeSwitch: stateValues.alwaysAllowModeSwitch ?? false,\\n1536 | \\t\\t\\talwaysAllowSubtasks: stateValues.alwaysAllowSubtasks ?? false,\\n1537 | \\t\\t\\tallowedMaxRequests: stateValues.allowedMaxRequests,\\n1538 | \\t\\t\\tautoCondenseContext: stateValues.autoCondenseContext ?? true,\\n1539 | \\t\\t\\tautoCondenseContextPercent: stateValues.autoCondenseContextPercent ?? 100,\\n1540 | \\t\\t\\ttaskHistory: stateValues.taskHistory,\\n1541 | \\t\\t\\tallowedCommands: stateValues.allowedCommands,\\n1542 | \\t\\t\\tsoundEnabled: stateValues.soundEnabled ?? false,\\n1543 | \\t\\t\\tttsEnabled: stateValues.ttsEnabled ?? false,\\n1544 | \\t\\t\\tttsSpeed: stateValues.ttsSpeed ?? 1.0,\\n1545 | \\t\\t\\tdiffEnabled: stateValues.diffEnabled ?? true,\\n1546 | \\t\\t\\tenableCheckpoints: stateValues.enableCheckpoints ?? true,\\n1547 | \\t\\t\\tsoundVolume: stateValues.soundVolume,\\n1548 | \\t\\t\\tbrowserViewportSize: stateValues.browserViewportSize ?? \\\"900x600\\\",\\n1549 | \\t\\t\\tscreenshotQuality: stateValues.screenshotQuality ?? 75,\\n1550 | \\t\\t\\tremoteBrowserHost: stateValues.remoteBrowserHost,\\n1551 | \\t\\t\\tremoteBrowserEnabled: stateValues.remoteBrowserEnabled ?? false,\\n1552 | \\t\\t\\tcachedChromeHostUrl: stateValues.cachedChromeHostUrl as string | undefined,\\n1553 | \\t\\t\\tfuzzyMatchThreshold: stateValues.fuzzyMatchThreshold ?? 1.0,\\n1554 | \\t\\t\\twriteDelayMs: stateValues.writeDelayMs ?? 1000,\\n1555 | \\t\\t\\tterminalOutputLineLimit: stateValues.terminalOutputLineLimit ?? 500,\\n1556 | \\t\\t\\tterminalShellIntegrationTimeout:\\n1557 | \\t\\t\\t\\tstateValues.terminalShellIntegrationTimeout ?? Terminal.defaultShellIntegrationTimeout,\\n1558 | \\t\\t\\tterminalShellIntegrationDisabled: stateValues.terminalShellIntegrationDisabled ?? false,\\n1559 | \\t\\t\\tterminalCommandDelay: stateValues.terminalCommandDelay ?? 0,\\n1560 | \\t\\t\\tterminalPowershellCounter: stateValues.terminalPowershellCounter ?? false,\\n1561 | \\t\\t\\tterminalZshClearEolMark: stateValues.terminalZshClearEolMark ?? true,\\n1562 | \\t\\t\\tterminalZshOhMy: stateValues.terminalZshOhMy ?? false,\\n1563 | \\t\\t\\tterminalZshP10k: stateValues.terminalZshP10k ?? false,\\n1564 | \\t\\t\\tterminalZdotdir: stateValues.terminalZdotdir ?? false,\\n1565 | \\t\\t\\tterminalCompressProgressBar: stateValues.terminalCompressProgressBar ?? true,\\n1566 | \\t\\t\\tmode: stateValues.mode ?? defaultModeSlug,\\n1567 | \\t\\t\\tlanguage: stateValues.language ?? formatLanguage(vscode.env.language),\\n1568 | \\t\\t\\tmcpEnabled: stateValues.mcpEnabled ?? true,\\n1569 | \\t\\t\\tenableMcpServerCreation: stateValues.enableMcpServerCreation ?? true,\\n1570 | \\t\\t\\talwaysApproveResubmit: stateValues.alwaysApproveResubmit ?? false,\\n1571 | \\t\\t\\trequestDelaySeconds: Math.max(5, stateValues.requestDelaySeconds ?? 10),\\n1572 | \\t\\t\\tcurrentApiConfigName: stateValues.currentApiConfigName ?? \\\"default\\\",\\n1573 | \\t\\t\\tlistApiConfigMeta: stateValues.listApiConfigMeta ?? [],\\n1574 | \\t\\t\\tpinnedApiConfigs: stateValues.pinnedApiConfigs ?? {},\\n1575 | \\t\\t\\tmodeApiConfigs: stateValues.modeApiConfigs ?? ({} as Record<Mode, string>),\\n1576 | \\t\\t\\tcustomModePrompts: stateValues.customModePrompts ?? {},\\n1577 | \\t\\t\\tcustomSupportPrompts: stateValues.customSupportPrompts ?? {},\\n1578 | \\t\\t\\tenhancementApiConfigId: stateValues.enhancementApiConfigId,\\n1579 | \\t\\t\\texperiments: stateValues.experiments ?? experimentDefault,\\n1580 | \\t\\t\\tautoApprovalEnabled: stateValues.autoApprovalEnabled ?? false,\\n1581 | \\t\\t\\tcustomModes,\\n1582 | \\t\\t\\tmaxOpenTabsContext: stateValues.maxOpenTabsContext ?? 20,\\n1583 | \\t\\t\\tmaxWorkspaceFiles: stateValues.maxWorkspaceFiles ?? 200,\\n1584 | \\t\\t\\topenRouterUseMiddleOutTransform: stateValues.openRouterUseMiddleOutTransform ?? true,\\n1585 | \\t\\t\\tbrowserToolEnabled: stateValues.browserToolEnabled ?? true,\\n1586 | \\t\\t\\ttelemetrySetting: stateValues.telemetrySetting || \\\"unset\\\",\\n1587 | \\t\\t\\tshowRooIgnoredFiles: stateValues.showRooIgnoredFiles ?? true,\\n1588 | \\t\\t\\tmaxReadFileLine: stateValues.maxReadFileLine ?? -1,\\n1589 | \\t\\t\\tmaxConcurrentFileReads: stateValues.maxConcurrentFileReads ?? 5,\\n1590 | \\t\\t\\thistoryPreviewCollapsed: stateValues.historyPreviewCollapsed ?? false,\\n1591 | \\t\\t\\tcloudUserInfo,\\n1592 | \\t\\t\\tcloudIsAuthenticated,\\n1593 | \\t\\t\\tsharingEnabled,\\n1594 | \\t\\t\\torganizationAllowList,\\n1595 | \\t\\t\\t// Explicitly add condensing settings\\n1596 | \\t\\t\\tcondensingApiConfigId: stateValues.condensingApiConfigId,\\n1597 | \\t\\t\\tcustomCondensingPrompt: stateValues.customCondensingPrompt,\\n1598 | \\t\\t\\tcodebaseIndexModels: stateValues.codebaseIndexModels ?? EMBEDDING_MODEL_PROFILES,\\n1599 | \\t\\t\\tcodebaseIndexConfig: stateValues.codebaseIndexConfig ?? {\\n1600 | \\t\\t\\t\\tcodebaseIndexEnabled: false,\\n1601 | \\t\\t\\t\\tcodebaseIndexQdrantUrl: \\\"http://localhost:6333\\\",\\n1602 | \\t\\t\\t\\tcodebaseIndexEmbedderProvider: \\\"openai\\\",\\n1603 | \\t\\t\\t\\tcodebaseIndexEmbedderBaseUrl: \\\"\\\",\\n1604 | \\t\\t\\t\\tcodebaseIndexEmbedderModelId: \\\"\\\",\\n1605 | \\t\\t\\t},\\n1606 | \\t\\t}\\n1607 | \\t}\\n1608 | \\n1609 | \\tasync updateTaskHistory(item: HistoryItem): Promise<HistoryItem[]> {\\n1610 | \\t\\tconst history = (this.getGlobalState(\\\"taskHistory\\\") as HistoryItem[] | undefined) || []\\n1611 | \\t\\tconst existingItemIndex = history.findIndex((h) => h.id === item.id)\\n1612 | \\n1613 | \\t\\tif (existingItemIndex !== -1) {\\n1614 | \\t\\t\\thistory[existingItemIndex] = item\\n1615 | \\t\\t} else {\\n1616 | \\t\\t\\thistory.push(item)\\n1617 | \\t\\t}\\n1618 | \\n1619 | \\t\\tawait this.updateGlobalState(\\\"taskHistory\\\", history)\\n1620 | \\t\\treturn history\\n1621 | \\t}\\n1622 | \\n1623 | \\t// ContextProxy\\n1624 | \\n1625 | \\t// @deprecated - Use `ContextProxy#setValue` instead.\\n1626 | \\tprivate async updateGlobalState<K extends keyof GlobalState>(key: K, value: GlobalState[K]) {\\n1627 | \\t\\tawait this.contextProxy.setValue(key, value)\\n1628 | \\t}\\n1629 | \\n1630 | \\t// @deprecated - Use `ContextProxy#getValue` instead.\\n1631 | \\tprivate getGlobalState<K extends keyof GlobalState>(key: K) {\\n1632 | \\t\\treturn this.contextProxy.getValue(key)\\n1633 | \\t}\\n1634 | \\n1635 | \\tpublic async setValue<K extends keyof RooCodeSettings>(key: K, value: RooCodeSettings[K]) {\\n1636 | \\t\\tawait this.contextProxy.setValue(key, value)\\n1637 | \\t}\\n1638 | \\n1639 | \\tpublic getValue<K extends keyof RooCodeSettings>(key: K) {\\n1640 | \\t\\treturn this.contextProxy.getValue(key)\\n1641 | \\t}\\n1642 | \\n1643 | \\tpublic getValues() {\\n1644 | \\t\\treturn this.contextProxy.getValues()\\n1645 | \\t}\\n1646 | \\n1647 | \\tpublic async setValues(values: RooCodeSettings) {\\n1648 | \\t\\tawait this.contextProxy.setValues(values)\\n1649 | \\t}\\n1650 | \\n1651 | \\t// cwd\\n1652 | \\n1653 | \\tget cwd() {\\n1654 | \\t\\treturn getWorkspacePath()\\n1655 | \\t}\\n1656 | \\n1657 | \\t// dev\\n1658 | \\n1659 | \\tasync resetState() {\\n1660 | \\t\\tconst answer = await vscode.window.showInformationMessage(\\n1661 | \\t\\t\\tt(\\\"common:confirmation.reset_state\\\"),\\n1662 | \\t\\t\\t{ modal: true },\\n1663 | \\t\\t\\tt(\\\"common:answers.yes\\\"),\\n1664 | \\t\\t)\\n1665 | \\n1666 | \\t\\tif (answer !== t(\\\"common:answers.yes\\\")) {\\n1667 | \\t\\t\\treturn\\n1668 | \\t\\t}\\n1669 | \\n1670 | \\t\\tawait this.contextProxy.resetAllState()\\n1671 | \\t\\tawait this.providerSettingsManager.resetAllConfigs()\\n1672 | \\t\\tawait this.customModesManager.resetCustomModes()\\n1673 | \\t\\tawait this.removeClineFromStack()\\n1674 | \\t\\tawait this.postStateToWebview()\\n1675 | \\t\\tawait this.postMessageToWebview({ type: \\\"action\\\", action: \\\"chatButtonClicked\\\" })\\n1676 | \\t}\\n1677 | \\n1678 | \\t// logging\\n1679 | \\n1680 | \\tpublic log(message: string) {\\n1681 | \\t\\tthis.outputChannel.appendLine(message)\\n1682 | \\t\\tconsole.log(message)\\n1683 | \\t}\\n1684 | \\n1685 | \\t// integration tests\\n1686 | \\n1687 | \\tget viewLaunched() {\\n1688 | \\t\\treturn this.isViewLaunched\\n1689 | \\t}\\n1690 | \\n1691 | \\tget messages() {\\n1692 | \\t\\treturn this.getCurrentCline()?.clineMessages || []\\n1693 | \\t}\\n1694 | \\n1695 | \\t// Add public getter\\n1696 | \\tpublic getMcpHub(): McpHub | undefined {\\n1697 | \\t\\treturn this.mcpHub\\n1698 | \\t}\\n1699 | \\n1700 | \\t/**\\n1701 | \\t * Returns properties to be included in every telemetry event\\n1702 | \\t * This method is called by the telemetry service to get context information\\n1703 | \\t * like the current mode, API provider, etc.\\n1704 | \\t */\\n1705 | \\tpublic async getTelemetryProperties(): Promise<TelemetryProperties> {\\n1706 | \\t\\tconst { mode, apiConfiguration, language } = await this.getState()\\n1707 | \\t\\tconst task = this.getCurrentCline()\\n1708 | \\n1709 | \\t\\tconst packageJSON = this.context.extension?.packageJSON\\n1710 | \\n1711 | \\t\\treturn {\\n1712 | \\t\\t\\tappName: packageJSON?.name ?? Package.name,\\n1713 | \\t\\t\\tappVersion: packageJSON?.version ?? Package.version,\\n1714 | \\t\\t\\tvscodeVersion: vscode.version,\\n1715 | \\t\\t\\tplatform: process.platform,\\n1716 | \\t\\t\\teditorName: vscode.env.appName,\\n1717 | \\t\\t\\tlanguage,\\n1718 | \\t\\t\\tmode,\\n1719 | \\t\\t\\tapiProvider: apiConfiguration?.apiProvider,\\n1720 | \\t\\t\\tmodelId: task?.api?.getModel().id,\\n1721 | \\t\\t\\tdiffStrategy: task?.diffStrategy?.getName(),\\n1722 | \\t\\t\\tisSubtask: task ? !!task.parentTask : undefined,\\n1723 | \\t\\t}\\n1724 | \\t}\\n1725 | }\\n</content>\\n</file>\\n<file><path>src/core/webview/webviewMessageHandler.ts</path>\\n<content lines=\\\"1-1594\\\">\\n   1 | import * as path from \\\"path\\\"\\n   2 | import fs from \\\"fs/promises\\\"\\n   3 | import pWaitFor from \\\"p-wait-for\\\"\\n   4 | import * as vscode from \\\"vscode\\\"\\n   5 | \\n   6 | import { type Language, type ProviderSettings, type GlobalState, TelemetryEventName } from \\\"@roo-code/types\\\"\\n   7 | import { CloudService } from \\\"@roo-code/cloud\\\"\\n   8 | import { TelemetryService } from \\\"@roo-code/telemetry\\\"\\n   9 | \\n  10 | import { ClineProvider } from \\\"./ClineProvider\\\"\\n  11 | import { changeLanguage, t } from \\\"../../i18n\\\"\\n  12 | import { Package } from \\\"../../shared/package\\\"\\n  13 | import { RouterName, toRouterName, ModelRecord } from \\\"../../shared/api\\\"\\n  14 | import { supportPrompt } from \\\"../../shared/support-prompt\\\"\\n  15 | \\n  16 | import { checkoutDiffPayloadSchema, checkoutRestorePayloadSchema, WebviewMessage } from \\\"../../shared/WebviewMessage\\\"\\n  17 | import { checkExistKey } from \\\"../../shared/checkExistApiConfig\\\"\\n  18 | import { experimentDefault } from \\\"../../shared/experiments\\\"\\n  19 | import { Terminal } from \\\"../../integrations/terminal/Terminal\\\"\\n  20 | import { openFile } from \\\"../../integrations/misc/open-file\\\"\\n  21 | import { openImage, saveImage } from \\\"../../integrations/misc/image-handler\\\"\\n  22 | import { selectImages } from \\\"../../integrations/misc/process-images\\\"\\n  23 | import { getTheme } from \\\"../../integrations/theme/getTheme\\\"\\n  24 | import { discoverChromeHostUrl, tryChromeHostUrl } from \\\"../../services/browser/browserDiscovery\\\"\\n  25 | import { searchWorkspaceFiles } from \\\"../../services/search/file-search\\\"\\n  26 | import { fileExistsAtPath } from \\\"../../utils/fs\\\"\\n  27 | import { playTts, setTtsEnabled, setTtsSpeed, stopTts } from \\\"../../utils/tts\\\"\\n  28 | import { singleCompletionHandler } from \\\"../../utils/single-completion-handler\\\"\\n  29 | import { searchCommits } from \\\"../../utils/git\\\"\\n  30 | import { exportSettings, importSettings } from \\\"../config/importExport\\\"\\n  31 | import { getOpenAiModels } from \\\"../../api/providers/openai\\\"\\n  32 | import { getOllamaModels } from \\\"../../api/providers/ollama\\\"\\n  33 | import { getVsCodeLmModels } from \\\"../../api/providers/vscode-lm\\\"\\n  34 | import { getLmStudioModels } from \\\"../../api/providers/lm-studio\\\"\\n  35 | import { openMention } from \\\"../mentions\\\"\\n  36 | import { TelemetrySetting } from \\\"../../shared/TelemetrySetting\\\"\\n  37 | import { getWorkspacePath } from \\\"../../utils/path\\\"\\n  38 | import { Mode, defaultModeSlug } from \\\"../../shared/modes\\\"\\n  39 | import { getModels, flushModels } from \\\"../../api/providers/fetchers/modelCache\\\"\\n  40 | import { GetModelsOptions } from \\\"../../shared/api\\\"\\n  41 | import { generateSystemPrompt } from \\\"./generateSystemPrompt\\\"\\n  42 | import { getCommand } from \\\"../../utils/commands\\\"\\n  43 | \\n  44 | const ALLOWED_VSCODE_SETTINGS = new Set([\\\"terminal.integrated.inheritEnv\\\"])\\n  45 | \\n  46 | import { MarketplaceManager, MarketplaceItemType } from \\\"../../services/marketplace\\\"\\n  47 | \\n  48 | export const webviewMessageHandler = async (\\n  49 | \\tprovider: ClineProvider,\\n  50 | \\tmessage: WebviewMessage,\\n  51 | \\tmarketplaceManager?: MarketplaceManager,\\n  52 | ) => {\\n  53 | \\t// Utility functions provided for concise get/update of global state via contextProxy API.\\n  54 | \\tconst getGlobalState = <K extends keyof GlobalState>(key: K) => provider.contextProxy.getValue(key)\\n  55 | \\tconst updateGlobalState = async <K extends keyof GlobalState>(key: K, value: GlobalState[K]) =>\\n  56 | \\t\\tawait provider.contextProxy.setValue(key, value)\\n  57 | \\n  58 | \\tswitch (message.type) {\\n  59 | \\t\\tcase \\\"webviewDidLaunch\\\":\\n  60 | \\t\\t\\t// Load custom modes first\\n  61 | \\t\\t\\tconst customModes = await provider.customModesManager.getCustomModes()\\n  62 | \\t\\t\\tawait updateGlobalState(\\\"customModes\\\", customModes)\\n  63 | \\n  64 | \\t\\t\\tprovider.postStateToWebview()\\n  65 | \\t\\t\\tprovider.workspaceTracker?.initializeFilePaths() // Don't await.\\n  66 | \\n  67 | \\t\\t\\tgetTheme().then((theme) => provider.postMessageToWebview({ type: \\\"theme\\\", text: JSON.stringify(theme) }))\\n  68 | \\n  69 | \\t\\t\\t// If MCP Hub is already initialized, update the webview with\\n  70 | \\t\\t\\t// current server list.\\n  71 | \\t\\t\\tconst mcpHub = provider.getMcpHub()\\n  72 | \\n  73 | \\t\\t\\tif (mcpHub) {\\n  74 | \\t\\t\\t\\tprovider.postMessageToWebview({ type: \\\"mcpServers\\\", mcpServers: mcpHub.getAllServers() })\\n  75 | \\t\\t\\t}\\n  76 | \\n  77 | \\t\\t\\tprovider.providerSettingsManager\\n  78 | \\t\\t\\t\\t.listConfig()\\n  79 | \\t\\t\\t\\t.then(async (listApiConfig) => {\\n  80 | \\t\\t\\t\\t\\tif (!listApiConfig) {\\n  81 | \\t\\t\\t\\t\\t\\treturn\\n  82 | \\t\\t\\t\\t\\t}\\n  83 | \\n  84 | \\t\\t\\t\\t\\tif (listApiConfig.length === 1) {\\n  85 | \\t\\t\\t\\t\\t\\t// Check if first time init then sync with exist config.\\n  86 | \\t\\t\\t\\t\\t\\tif (!checkExistKey(listApiConfig[0])) {\\n  87 | \\t\\t\\t\\t\\t\\t\\tconst { apiConfiguration } = await provider.getState()\\n  88 | \\n  89 | \\t\\t\\t\\t\\t\\t\\tawait provider.providerSettingsManager.saveConfig(\\n  90 | \\t\\t\\t\\t\\t\\t\\t\\tlistApiConfig[0].name ?? \\\"default\\\",\\n  91 | \\t\\t\\t\\t\\t\\t\\t\\tapiConfiguration,\\n  92 | \\t\\t\\t\\t\\t\\t\\t)\\n  93 | \\n  94 | \\t\\t\\t\\t\\t\\t\\tlistApiConfig[0].apiProvider = apiConfiguration.apiProvider\\n  95 | \\t\\t\\t\\t\\t\\t}\\n  96 | \\t\\t\\t\\t\\t}\\n  97 | \\n  98 | \\t\\t\\t\\t\\tconst currentConfigName = getGlobalState(\\\"currentApiConfigName\\\")\\n  99 | \\n 100 | \\t\\t\\t\\t\\tif (currentConfigName) {\\n 101 | \\t\\t\\t\\t\\t\\tif (!(await provider.providerSettingsManager.hasConfig(currentConfigName))) {\\n 102 | \\t\\t\\t\\t\\t\\t\\t// Current config name not valid, get first config in list.\\n 103 | \\t\\t\\t\\t\\t\\t\\tconst name = listApiConfig[0]?.name\\n 104 | \\t\\t\\t\\t\\t\\t\\tawait updateGlobalState(\\\"currentApiConfigName\\\", name)\\n 105 | \\n 106 | \\t\\t\\t\\t\\t\\t\\tif (name) {\\n 107 | \\t\\t\\t\\t\\t\\t\\t\\tawait provider.activateProviderProfile({ name })\\n 108 | \\t\\t\\t\\t\\t\\t\\t\\treturn\\n 109 | \\t\\t\\t\\t\\t\\t\\t}\\n 110 | \\t\\t\\t\\t\\t\\t}\\n 111 | \\t\\t\\t\\t\\t}\\n 112 | \\n 113 | \\t\\t\\t\\t\\tawait Promise.all([\\n 114 | \\t\\t\\t\\t\\t\\tawait updateGlobalState(\\\"listApiConfigMeta\\\", listApiConfig),\\n 115 | \\t\\t\\t\\t\\t\\tawait provider.postMessageToWebview({ type: \\\"listApiConfig\\\", listApiConfig }),\\n 116 | \\t\\t\\t\\t\\t])\\n 117 | \\t\\t\\t\\t})\\n 118 | \\t\\t\\t\\t.catch((error) =>\\n 119 | \\t\\t\\t\\t\\tprovider.log(\\n 120 | \\t\\t\\t\\t\\t\\t`Error list api configuration: ${JSON.stringify(error, Object.getOwnPropertyNames(error), 2)}`,\\n 121 | \\t\\t\\t\\t\\t),\\n 122 | \\t\\t\\t\\t)\\n 123 | \\n 124 | \\t\\t\\t// If user already opted in to telemetry, enable telemetry service\\n 125 | \\t\\t\\tprovider.getStateToPostToWebview().then((state) => {\\n 126 | \\t\\t\\t\\tconst { telemetrySetting } = state\\n 127 | \\t\\t\\t\\tconst isOptedIn = telemetrySetting === \\\"enabled\\\"\\n 128 | \\t\\t\\t\\tTelemetryService.instance.updateTelemetryState(isOptedIn)\\n 129 | \\t\\t\\t})\\n 130 | \\n 131 | \\t\\t\\tprovider.isViewLaunched = true\\n 132 | \\t\\t\\tbreak\\n 133 | \\t\\tcase \\\"newTask\\\":\\n 134 | \\t\\t\\t// Initializing new instance of Cline will make sure that any\\n 135 | \\t\\t\\t// agentically running promises in old instance don't affect our new\\n 136 | \\t\\t\\t// task. This essentially creates a fresh slate for the new task.\\n 137 | \\t\\t\\tawait provider.initClineWithTask(message.text, message.images)\\n 138 | \\t\\t\\tbreak\\n 139 | \\t\\tcase \\\"customInstructions\\\":\\n 140 | \\t\\t\\tawait provider.updateCustomInstructions(message.text)\\n 141 | \\t\\t\\tbreak\\n 142 | \\t\\tcase \\\"alwaysAllowReadOnly\\\":\\n 143 | \\t\\t\\tawait updateGlobalState(\\\"alwaysAllowReadOnly\\\", message.bool ?? undefined)\\n 144 | \\t\\t\\tawait provider.postStateToWebview()\\n 145 | \\t\\t\\tbreak\\n 146 | \\t\\tcase \\\"alwaysAllowReadOnlyOutsideWorkspace\\\":\\n 147 | \\t\\t\\tawait updateGlobalState(\\\"alwaysAllowReadOnlyOutsideWorkspace\\\", message.bool ?? undefined)\\n 148 | \\t\\t\\tawait provider.postStateToWebview()\\n 149 | \\t\\t\\tbreak\\n 150 | \\t\\tcase \\\"alwaysAllowWrite\\\":\\n 151 | \\t\\t\\tawait updateGlobalState(\\\"alwaysAllowWrite\\\", message.bool ?? undefined)\\n 152 | \\t\\t\\tawait provider.postStateToWebview()\\n 153 | \\t\\t\\tbreak\\n 154 | \\t\\tcase \\\"alwaysAllowWriteOutsideWorkspace\\\":\\n 155 | \\t\\t\\tawait updateGlobalState(\\\"alwaysAllowWriteOutsideWorkspace\\\", message.bool ?? undefined)\\n 156 | \\t\\t\\tawait provider.postStateToWebview()\\n 157 | \\t\\t\\tbreak\\n 158 | \\t\\tcase \\\"alwaysAllowExecute\\\":\\n 159 | \\t\\t\\tawait updateGlobalState(\\\"alwaysAllowExecute\\\", message.bool ?? undefined)\\n 160 | \\t\\t\\tawait provider.postStateToWebview()\\n 161 | \\t\\t\\tbreak\\n 162 | \\t\\tcase \\\"alwaysAllowBrowser\\\":\\n 163 | \\t\\t\\tawait updateGlobalState(\\\"alwaysAllowBrowser\\\", message.bool ?? undefined)\\n 164 | \\t\\t\\tawait provider.postStateToWebview()\\n 165 | \\t\\t\\tbreak\\n 166 | \\t\\tcase \\\"alwaysAllowMcp\\\":\\n 167 | \\t\\t\\tawait updateGlobalState(\\\"alwaysAllowMcp\\\", message.bool)\\n 168 | \\t\\t\\tawait provider.postStateToWebview()\\n 169 | \\t\\t\\tbreak\\n 170 | \\t\\tcase \\\"alwaysAllowModeSwitch\\\":\\n 171 | \\t\\t\\tawait updateGlobalState(\\\"alwaysAllowModeSwitch\\\", message.bool)\\n 172 | \\t\\t\\tawait provider.postStateToWebview()\\n 173 | \\t\\t\\tbreak\\n 174 | \\t\\tcase \\\"allowedMaxRequests\\\":\\n 175 | \\t\\t\\tawait updateGlobalState(\\\"allowedMaxRequests\\\", message.value)\\n 176 | \\t\\t\\tawait provider.postStateToWebview()\\n 177 | \\t\\t\\tbreak\\n 178 | \\t\\tcase \\\"alwaysAllowSubtasks\\\":\\n 179 | \\t\\t\\tawait updateGlobalState(\\\"alwaysAllowSubtasks\\\", message.bool)\\n 180 | \\t\\t\\tawait provider.postStateToWebview()\\n 181 | \\t\\t\\tbreak\\n 182 | \\t\\tcase \\\"askResponse\\\":\\n 183 | \\t\\t\\tprovider.getCurrentCline()?.handleWebviewAskResponse(message.askResponse!, message.text, message.images)\\n 184 | \\t\\t\\tbreak\\n 185 | \\t\\tcase \\\"autoCondenseContext\\\":\\n 186 | \\t\\t\\tawait updateGlobalState(\\\"autoCondenseContext\\\", message.bool)\\n 187 | \\t\\t\\tawait provider.postStateToWebview()\\n 188 | \\t\\t\\tbreak\\n 189 | \\t\\tcase \\\"autoCondenseContextPercent\\\":\\n 190 | \\t\\t\\tawait updateGlobalState(\\\"autoCondenseContextPercent\\\", message.value)\\n 191 | \\t\\t\\tawait provider.postStateToWebview()\\n 192 | \\t\\t\\tbreak\\n 193 | \\t\\tcase \\\"terminalOperation\\\":\\n 194 | \\t\\t\\tif (message.terminalOperation) {\\n 195 | \\t\\t\\t\\tprovider.getCurrentCline()?.handleTerminalOperation(message.terminalOperation)\\n 196 | \\t\\t\\t}\\n 197 | \\t\\t\\tbreak\\n 198 | \\t\\tcase \\\"clearTask\\\":\\n 199 | \\t\\t\\t// clear task resets the current session and allows for a new task to be started, if this session is a subtask - it allows the parent task to be resumed\\n 200 | \\t\\t\\tawait provider.finishSubTask(t(\\\"common:tasks.canceled\\\"))\\n 201 | \\t\\t\\tawait provider.postStateToWebview()\\n 202 | \\t\\t\\tbreak\\n 203 | \\t\\tcase \\\"didShowAnnouncement\\\":\\n 204 | \\t\\t\\tawait updateGlobalState(\\\"lastShownAnnouncementId\\\", provider.latestAnnouncementId)\\n 205 | \\t\\t\\tawait provider.postStateToWebview()\\n 206 | \\t\\t\\tbreak\\n 207 | \\t\\tcase \\\"selectImages\\\":\\n 208 | \\t\\t\\tconst images = await selectImages()\\n 209 | \\t\\t\\tawait provider.postMessageToWebview({ type: \\\"selectedImages\\\", images })\\n 210 | \\t\\t\\tbreak\\n 211 | \\t\\tcase \\\"exportCurrentTask\\\":\\n 212 | \\t\\t\\tconst currentTaskId = provider.getCurrentCline()?.taskId\\n 213 | \\t\\t\\tif (currentTaskId) {\\n 214 | \\t\\t\\t\\tprovider.exportTaskWithId(currentTaskId)\\n 215 | \\t\\t\\t}\\n 216 | \\t\\t\\tbreak\\n 217 | \\t\\tcase \\\"shareCurrentTask\\\":\\n 218 | \\t\\t\\tconst shareTaskId = provider.getCurrentCline()?.taskId\\n 219 | \\t\\t\\tif (!shareTaskId) {\\n 220 | \\t\\t\\t\\tvscode.window.showErrorMessage(t(\\\"common:errors.share_no_active_task\\\"))\\n 221 | \\t\\t\\t\\tbreak\\n 222 | \\t\\t\\t}\\n 223 | \\n 224 | \\t\\t\\ttry {\\n 225 | \\t\\t\\t\\tconst success = await CloudService.instance.shareTask(shareTaskId)\\n 226 | \\t\\t\\t\\tif (success) {\\n 227 | \\t\\t\\t\\t\\t// Show success message\\n 228 | \\t\\t\\t\\t\\tvscode.window.showInformationMessage(t(\\\"common:info.share_link_copied\\\"))\\n 229 | \\t\\t\\t\\t} else {\\n 230 | \\t\\t\\t\\t\\t// Show generic failure message\\n 231 | \\t\\t\\t\\t\\tvscode.window.showErrorMessage(t(\\\"common:errors.share_task_failed\\\"))\\n 232 | \\t\\t\\t\\t}\\n 233 | \\t\\t\\t} catch (error) {\\n 234 | \\t\\t\\t\\t// Show generic failure message\\n 235 | \\t\\t\\t\\tvscode.window.showErrorMessage(t(\\\"common:errors.share_task_failed\\\"))\\n 236 | \\t\\t\\t}\\n 237 | \\t\\t\\tbreak\\n 238 | \\t\\tcase \\\"showTaskWithId\\\":\\n 239 | \\t\\t\\tprovider.showTaskWithId(message.text!)\\n 240 | \\t\\t\\tbreak\\n 241 | \\t\\tcase \\\"condenseTaskContextRequest\\\":\\n 242 | \\t\\t\\tprovider.condenseTaskContext(message.text!)\\n 243 | \\t\\t\\tbreak\\n 244 | \\t\\tcase \\\"deleteTaskWithId\\\":\\n 245 | \\t\\t\\tprovider.deleteTaskWithId(message.text!)\\n 246 | \\t\\t\\tbreak\\n 247 | \\t\\tcase \\\"deleteMultipleTasksWithIds\\\": {\\n 248 | \\t\\t\\tconst ids = message.ids\\n 249 | \\n 250 | \\t\\t\\tif (Array.isArray(ids)) {\\n 251 | \\t\\t\\t\\t// Process in batches of 20 (or another reasonable number)\\n 252 | \\t\\t\\t\\tconst batchSize = 20\\n 253 | \\t\\t\\t\\tconst results = []\\n 254 | \\n 255 | \\t\\t\\t\\t// Only log start and end of the operation\\n 256 | \\t\\t\\t\\tconsole.log(`Batch deletion started: ${ids.length} tasks total`)\\n 257 | \\n 258 | \\t\\t\\t\\tfor (let i = 0; i < ids.length; i += batchSize) {\\n 259 | \\t\\t\\t\\t\\tconst batch = ids.slice(i, i + batchSize)\\n 260 | \\n 261 | \\t\\t\\t\\t\\tconst batchPromises = batch.map(async (id) => {\\n 262 | \\t\\t\\t\\t\\t\\ttry {\\n 263 | \\t\\t\\t\\t\\t\\t\\tawait provider.deleteTaskWithId(id)\\n 264 | \\t\\t\\t\\t\\t\\t\\treturn { id, success: true }\\n 265 | \\t\\t\\t\\t\\t\\t} catch (error) {\\n 266 | \\t\\t\\t\\t\\t\\t\\t// Keep error logging for debugging purposes\\n 267 | \\t\\t\\t\\t\\t\\t\\tconsole.log(\\n 268 | \\t\\t\\t\\t\\t\\t\\t\\t`Failed to delete task ${id}: ${error instanceof Error ? error.message : String(error)}`,\\n 269 | \\t\\t\\t\\t\\t\\t\\t)\\n 270 | \\t\\t\\t\\t\\t\\t\\treturn { id, success: false }\\n 271 | \\t\\t\\t\\t\\t\\t}\\n 272 | \\t\\t\\t\\t\\t})\\n 273 | \\n 274 | \\t\\t\\t\\t\\t// Process each batch in parallel but wait for completion before starting the next batch\\n 275 | \\t\\t\\t\\t\\tconst batchResults = await Promise.all(batchPromises)\\n 276 | \\t\\t\\t\\t\\tresults.push(...batchResults)\\n 277 | \\n 278 | \\t\\t\\t\\t\\t// Update the UI after each batch to show progress\\n 279 | \\t\\t\\t\\t\\tawait provider.postStateToWebview()\\n 280 | \\t\\t\\t\\t}\\n 281 | \\n 282 | \\t\\t\\t\\t// Log final results\\n 283 | \\t\\t\\t\\tconst successCount = results.filter((r) => r.success).length\\n 284 | \\t\\t\\t\\tconst failCount = results.length - successCount\\n 285 | \\t\\t\\t\\tconsole.log(\\n 286 | \\t\\t\\t\\t\\t`Batch deletion completed: ${successCount}/${ids.length} tasks successful, ${failCount} tasks failed`,\\n 287 | \\t\\t\\t\\t)\\n 288 | \\t\\t\\t}\\n 289 | \\t\\t\\tbreak\\n 290 | \\t\\t}\\n 291 | \\t\\tcase \\\"exportTaskWithId\\\":\\n 292 | \\t\\t\\tprovider.exportTaskWithId(message.text!)\\n 293 | \\t\\t\\tbreak\\n 294 | \\t\\tcase \\\"importSettings\\\": {\\n 295 | \\t\\t\\tconst result = await importSettings({\\n 296 | \\t\\t\\t\\tproviderSettingsManager: provider.providerSettingsManager,\\n 297 | \\t\\t\\t\\tcontextProxy: provider.contextProxy,\\n 298 | \\t\\t\\t\\tcustomModesManager: provider.customModesManager,\\n 299 | \\t\\t\\t})\\n 300 | \\n 301 | \\t\\t\\tif (result.success) {\\n 302 | \\t\\t\\t\\tprovider.settingsImportedAt = Date.now()\\n 303 | \\t\\t\\t\\tawait provider.postStateToWebview()\\n 304 | \\t\\t\\t\\tawait vscode.window.showInformationMessage(t(\\\"common:info.settings_imported\\\"))\\n 305 | \\t\\t\\t} else if (result.error) {\\n 306 | \\t\\t\\t\\tawait vscode.window.showErrorMessage(t(\\\"common:errors.settings_import_failed\\\", { error: result.error }))\\n 307 | \\t\\t\\t}\\n 308 | \\n 309 | \\t\\t\\tbreak\\n 310 | \\t\\t}\\n 311 | \\t\\tcase \\\"exportSettings\\\":\\n 312 | \\t\\t\\tawait exportSettings({\\n 313 | \\t\\t\\t\\tproviderSettingsManager: provider.providerSettingsManager,\\n 314 | \\t\\t\\t\\tcontextProxy: provider.contextProxy,\\n 315 | \\t\\t\\t})\\n 316 | \\n 317 | \\t\\t\\tbreak\\n 318 | \\t\\tcase \\\"resetState\\\":\\n 319 | \\t\\t\\tawait provider.resetState()\\n 320 | \\t\\t\\tbreak\\n 321 | \\t\\tcase \\\"flushRouterModels\\\":\\n 322 | \\t\\t\\tconst routerNameFlush: RouterName = toRouterName(message.text)\\n 323 | \\t\\t\\tawait flushModels(routerNameFlush)\\n 324 | \\t\\t\\tbreak\\n 325 | \\t\\tcase \\\"requestRouterModels\\\":\\n 326 | \\t\\t\\tconst { apiConfiguration } = await provider.getState()\\n 327 | \\n 328 | \\t\\t\\tconst routerModels: Partial<Record<RouterName, ModelRecord>> = {\\n 329 | \\t\\t\\t\\topenrouter: {},\\n 330 | \\t\\t\\t\\trequesty: {},\\n 331 | \\t\\t\\t\\tglama: {},\\n 332 | \\t\\t\\t\\tunbound: {},\\n 333 | \\t\\t\\t\\tlitellm: {},\\n 334 | \\t\\t\\t}\\n 335 | \\n 336 | \\t\\t\\tconst safeGetModels = async (options: GetModelsOptions): Promise<ModelRecord> => {\\n 337 | \\t\\t\\t\\ttry {\\n 338 | \\t\\t\\t\\t\\treturn await getModels(options)\\n 339 | \\t\\t\\t\\t} catch (error) {\\n 340 | \\t\\t\\t\\t\\tconsole.error(\\n 341 | \\t\\t\\t\\t\\t\\t`Failed to fetch models in webviewMessageHandler requestRouterModels for ${options.provider}:`,\\n 342 | \\t\\t\\t\\t\\t\\terror,\\n 343 | \\t\\t\\t\\t\\t)\\n 344 | \\t\\t\\t\\t\\tthrow error // Re-throw to be caught by Promise.allSettled\\n 345 | \\t\\t\\t\\t}\\n 346 | \\t\\t\\t}\\n 347 | \\n 348 | \\t\\t\\tconst modelFetchPromises: Array<{ key: RouterName; options: GetModelsOptions }> = [\\n 349 | \\t\\t\\t\\t{ key: \\\"openrouter\\\", options: { provider: \\\"openrouter\\\" } },\\n 350 | \\t\\t\\t\\t{ key: \\\"requesty\\\", options: { provider: \\\"requesty\\\", apiKey: apiConfiguration.requestyApiKey } },\\n 351 | \\t\\t\\t\\t{ key: \\\"glama\\\", options: { provider: \\\"glama\\\" } },\\n 352 | \\t\\t\\t\\t{ key: \\\"unbound\\\", options: { provider: \\\"unbound\\\", apiKey: apiConfiguration.unboundApiKey } },\\n 353 | \\t\\t\\t]\\n 354 | \\n 355 | \\t\\t\\tconst litellmApiKey = apiConfiguration.litellmApiKey || message?.values?.litellmApiKey\\n 356 | \\t\\t\\tconst litellmBaseUrl = apiConfiguration.litellmBaseUrl || message?.values?.litellmBaseUrl\\n 357 | \\t\\t\\tif (litellmApiKey && litellmBaseUrl) {\\n 358 | \\t\\t\\t\\tmodelFetchPromises.push({\\n 359 | \\t\\t\\t\\t\\tkey: \\\"litellm\\\",\\n 360 | \\t\\t\\t\\t\\toptions: { provider: \\\"litellm\\\", apiKey: litellmApiKey, baseUrl: litellmBaseUrl },\\n 361 | \\t\\t\\t\\t})\\n 362 | \\t\\t\\t}\\n 363 | \\n 364 | \\t\\t\\tconst results = await Promise.allSettled(\\n 365 | \\t\\t\\t\\tmodelFetchPromises.map(async ({ key, options }) => {\\n 366 | \\t\\t\\t\\t\\tconst models = await safeGetModels(options)\\n 367 | \\t\\t\\t\\t\\treturn { key, models } // key is RouterName here\\n 368 | \\t\\t\\t\\t}),\\n 369 | \\t\\t\\t)\\n 370 | \\n 371 | \\t\\t\\tconst fetchedRouterModels: Partial<Record<RouterName, ModelRecord>> = { ...routerModels }\\n 372 | \\n 373 | \\t\\t\\tresults.forEach((result, index) => {\\n 374 | \\t\\t\\t\\tconst routerName = modelFetchPromises[index].key // Get RouterName using index\\n 375 | \\n 376 | \\t\\t\\t\\tif (result.status === \\\"fulfilled\\\") {\\n 377 | \\t\\t\\t\\t\\tfetchedRouterModels[routerName] = result.value.models\\n 378 | \\t\\t\\t\\t} else {\\n 379 | \\t\\t\\t\\t\\t// Handle rejection: Post a specific error message for this provider\\n 380 | \\t\\t\\t\\t\\tconst errorMessage = result.reason instanceof Error ? result.reason.message : String(result.reason)\\n 381 | \\t\\t\\t\\t\\tconsole.error(`Error fetching models for ${routerName}:`, result.reason)\\n 382 | \\n 383 | \\t\\t\\t\\t\\tfetchedRouterModels[routerName] = {} // Ensure it's an empty object in the main routerModels message\\n 384 | \\n 385 | \\t\\t\\t\\t\\tprovider.postMessageToWebview({\\n 386 | \\t\\t\\t\\t\\t\\ttype: \\\"singleRouterModelFetchResponse\\\",\\n 387 | \\t\\t\\t\\t\\t\\tsuccess: false,\\n 388 | \\t\\t\\t\\t\\t\\terror: errorMessage,\\n 389 | \\t\\t\\t\\t\\t\\tvalues: { provider: routerName },\\n 390 | \\t\\t\\t\\t\\t})\\n 391 | \\t\\t\\t\\t}\\n 392 | \\t\\t\\t})\\n 393 | \\n 394 | \\t\\t\\tprovider.postMessageToWebview({\\n 395 | \\t\\t\\t\\ttype: \\\"routerModels\\\",\\n 396 | \\t\\t\\t\\trouterModels: fetchedRouterModels as Record<RouterName, ModelRecord>,\\n 397 | \\t\\t\\t})\\n 398 | \\t\\t\\tbreak\\n 399 | \\t\\tcase \\\"requestOpenAiModels\\\":\\n 400 | \\t\\t\\tif (message?.values?.baseUrl && message?.values?.apiKey) {\\n 401 | \\t\\t\\t\\tconst openAiModels = await getOpenAiModels(\\n 402 | \\t\\t\\t\\t\\tmessage?.values?.baseUrl,\\n 403 | \\t\\t\\t\\t\\tmessage?.values?.apiKey,\\n 404 | \\t\\t\\t\\t\\tmessage?.values?.openAiHeaders,\\n 405 | \\t\\t\\t\\t)\\n 406 | \\n 407 | \\t\\t\\t\\tprovider.postMessageToWebview({ type: \\\"openAiModels\\\", openAiModels })\\n 408 | \\t\\t\\t}\\n 409 | \\n 410 | \\t\\t\\tbreak\\n 411 | \\t\\tcase \\\"requestOllamaModels\\\":\\n 412 | \\t\\t\\tconst ollamaModels = await getOllamaModels(message.text)\\n 413 | \\t\\t\\t// TODO: Cache like we do for OpenRouter, etc?\\n 414 | \\t\\t\\tprovider.postMessageToWebview({ type: \\\"ollamaModels\\\", ollamaModels })\\n 415 | \\t\\t\\tbreak\\n 416 | \\t\\tcase \\\"requestLmStudioModels\\\":\\n 417 | \\t\\t\\tconst lmStudioModels = await getLmStudioModels(message.text)\\n 418 | \\t\\t\\t// TODO: Cache like we do for OpenRouter, etc?\\n 419 | \\t\\t\\tprovider.postMessageToWebview({ type: \\\"lmStudioModels\\\", lmStudioModels })\\n 420 | \\t\\t\\tbreak\\n 421 | \\t\\tcase \\\"requestVsCodeLmModels\\\":\\n 422 | \\t\\t\\tconst vsCodeLmModels = await getVsCodeLmModels()\\n 423 | \\t\\t\\t// TODO: Cache like we do for OpenRouter, etc?\\n 424 | \\t\\t\\tprovider.postMessageToWebview({ type: \\\"vsCodeLmModels\\\", vsCodeLmModels })\\n 425 | \\t\\t\\tbreak\\n 426 | \\t\\tcase \\\"openImage\\\":\\n 427 | \\t\\t\\topenImage(message.text!, { values: message.values })\\n 428 | \\t\\t\\tbreak\\n 429 | \\t\\tcase \\\"saveImage\\\":\\n 430 | \\t\\t\\tsaveImage(message.dataUri!)\\n 431 | \\t\\t\\tbreak\\n 432 | \\t\\tcase \\\"openFile\\\":\\n 433 | \\t\\t\\topenFile(message.text!, message.values as { create?: boolean; content?: string; line?: number })\\n 434 | \\t\\t\\tbreak\\n 435 | \\t\\tcase \\\"openMention\\\":\\n 436 | \\t\\t\\topenMention(message.text)\\n 437 | \\t\\t\\tbreak\\n 438 | \\t\\tcase \\\"openExternal\\\":\\n 439 | \\t\\t\\tif (message.url) {\\n 440 | \\t\\t\\t\\tvscode.env.openExternal(vscode.Uri.parse(message.url))\\n 441 | \\t\\t\\t}\\n 442 | \\t\\t\\tbreak\\n 443 | \\t\\tcase \\\"checkpointDiff\\\":\\n 444 | \\t\\t\\tconst result = checkoutDiffPayloadSchema.safeParse(message.payload)\\n 445 | \\n 446 | \\t\\t\\tif (result.success) {\\n 447 | \\t\\t\\t\\tawait provider.getCurrentCline()?.checkpointDiff(result.data)\\n 448 | \\t\\t\\t}\\n 449 | \\n 450 | \\t\\t\\tbreak\\n 451 | \\t\\tcase \\\"checkpointRestore\\\": {\\n 452 | \\t\\t\\tconst result = checkoutRestorePayloadSchema.safeParse(message.payload)\\n 453 | \\n 454 | \\t\\t\\tif (result.success) {\\n 455 | \\t\\t\\t\\tawait provider.cancelTask()\\n 456 | \\n 457 | \\t\\t\\t\\ttry {\\n 458 | \\t\\t\\t\\t\\tawait pWaitFor(() => provider.getCurrentCline()?.isInitialized === true, { timeout: 3_000 })\\n 459 | \\t\\t\\t\\t} catch (error) {\\n 460 | \\t\\t\\t\\t\\tvscode.window.showErrorMessage(t(\\\"common:errors.checkpoint_timeout\\\"))\\n 461 | \\t\\t\\t\\t}\\n 462 | \\n 463 | \\t\\t\\t\\ttry {\\n 464 | \\t\\t\\t\\t\\tawait provider.getCurrentCline()?.checkpointRestore(result.data)\\n 465 | \\t\\t\\t\\t} catch (error) {\\n 466 | \\t\\t\\t\\t\\tvscode.window.showErrorMessage(t(\\\"common:errors.checkpoint_failed\\\"))\\n 467 | \\t\\t\\t\\t}\\n 468 | \\t\\t\\t}\\n 469 | \\n 470 | \\t\\t\\tbreak\\n 471 | \\t\\t}\\n 472 | \\t\\tcase \\\"cancelTask\\\":\\n 473 | \\t\\t\\tawait provider.cancelTask()\\n 474 | \\t\\t\\tbreak\\n 475 | \\t\\tcase \\\"allowedCommands\\\":\\n 476 | \\t\\t\\tawait provider.context.globalState.update(\\\"allowedCommands\\\", message.commands)\\n 477 | \\n 478 | \\t\\t\\t// Also update workspace settings.\\n 479 | \\t\\t\\tawait vscode.workspace\\n 480 | \\t\\t\\t\\t.getConfiguration(Package.name)\\n 481 | \\t\\t\\t\\t.update(\\\"allowedCommands\\\", message.commands, vscode.ConfigurationTarget.Global)\\n 482 | \\n 483 | \\t\\t\\tbreak\\n 484 | \\t\\tcase \\\"openCustomModesSettings\\\": {\\n 485 | \\t\\t\\tconst customModesFilePath = await provider.customModesManager.getCustomModesFilePath()\\n 486 | \\n 487 | \\t\\t\\tif (customModesFilePath) {\\n 488 | \\t\\t\\t\\topenFile(customModesFilePath)\\n 489 | \\t\\t\\t}\\n 490 | \\n 491 | \\t\\t\\tbreak\\n 492 | \\t\\t}\\n 493 | \\t\\tcase \\\"openMcpSettings\\\": {\\n 494 | \\t\\t\\tconst mcpSettingsFilePath = await provider.getMcpHub()?.getMcpSettingsFilePath()\\n 495 | \\n 496 | \\t\\t\\tif (mcpSettingsFilePath) {\\n 497 | \\t\\t\\t\\topenFile(mcpSettingsFilePath)\\n 498 | \\t\\t\\t}\\n 499 | \\n 500 | \\t\\t\\tbreak\\n 501 | \\t\\t}\\n 502 | \\t\\tcase \\\"openProjectMcpSettings\\\": {\\n 503 | \\t\\t\\tif (!vscode.workspace.workspaceFolders?.length) {\\n 504 | \\t\\t\\t\\tvscode.window.showErrorMessage(t(\\\"common:errors.no_workspace\\\"))\\n 505 | \\t\\t\\t\\treturn\\n 506 | \\t\\t\\t}\\n 507 | \\n 508 | \\t\\t\\tconst workspaceFolder = vscode.workspace.workspaceFolders[0]\\n 509 | \\t\\t\\tconst rooDir = path.join(workspaceFolder.uri.fsPath, \\\".roo\\\")\\n 510 | \\t\\t\\tconst mcpPath = path.join(rooDir, \\\"mcp.json\\\")\\n 511 | \\n 512 | \\t\\t\\ttry {\\n 513 | \\t\\t\\t\\tawait fs.mkdir(rooDir, { recursive: true })\\n 514 | \\t\\t\\t\\tconst exists = await fileExistsAtPath(mcpPath)\\n 515 | \\n 516 | \\t\\t\\t\\tif (!exists) {\\n 517 | \\t\\t\\t\\t\\tawait fs.writeFile(mcpPath, JSON.stringify({ mcpServers: {} }, null, 2))\\n 518 | \\t\\t\\t\\t}\\n 519 | \\n 520 | \\t\\t\\t\\tawait openFile(mcpPath)\\n 521 | \\t\\t\\t} catch (error) {\\n 522 | \\t\\t\\t\\tvscode.window.showErrorMessage(t(\\\"mcp:errors.create_json\\\", { error: `${error}` }))\\n 523 | \\t\\t\\t}\\n 524 | \\n 525 | \\t\\t\\tbreak\\n 526 | \\t\\t}\\n 527 | \\t\\tcase \\\"deleteMcpServer\\\": {\\n 528 | \\t\\t\\tif (!message.serverName) {\\n 529 | \\t\\t\\t\\tbreak\\n 530 | \\t\\t\\t}\\n 531 | \\n 532 | \\t\\t\\ttry {\\n 533 | \\t\\t\\t\\tprovider.log(`Attempting to delete MCP server: ${message.serverName}`)\\n 534 | \\t\\t\\t\\tawait provider.getMcpHub()?.deleteServer(message.serverName, message.source as \\\"global\\\" | \\\"project\\\")\\n 535 | \\t\\t\\t\\tprovider.log(`Successfully deleted MCP server: ${message.serverName}`)\\n 536 | \\n 537 | \\t\\t\\t\\t// Refresh the webview state\\n 538 | \\t\\t\\t\\tawait provider.postStateToWebview()\\n 539 | \\t\\t\\t} catch (error) {\\n 540 | \\t\\t\\t\\tconst errorMessage = error instanceof Error ? error.message : String(error)\\n 541 | \\t\\t\\t\\tprovider.log(`Failed to delete MCP server: ${errorMessage}`)\\n 542 | \\t\\t\\t\\t// Error messages are already handled by McpHub.deleteServer\\n 543 | \\t\\t\\t}\\n 544 | \\t\\t\\tbreak\\n 545 | \\t\\t}\\n 546 | \\t\\tcase \\\"restartMcpServer\\\": {\\n 547 | \\t\\t\\ttry {\\n 548 | \\t\\t\\t\\tawait provider.getMcpHub()?.restartConnection(message.text!, message.source as \\\"global\\\" | \\\"project\\\")\\n 549 | \\t\\t\\t} catch (error) {\\n 550 | \\t\\t\\t\\tprovider.log(\\n 551 | \\t\\t\\t\\t\\t`Failed to retry connection for ${message.text}: ${JSON.stringify(error, Object.getOwnPropertyNames(error), 2)}`,\\n 552 | \\t\\t\\t\\t)\\n 553 | \\t\\t\\t}\\n 554 | \\t\\t\\tbreak\\n 555 | \\t\\t}\\n 556 | \\t\\tcase \\\"toggleToolAlwaysAllow\\\": {\\n 557 | \\t\\t\\ttry {\\n 558 | \\t\\t\\t\\tawait provider\\n 559 | \\t\\t\\t\\t\\t.getMcpHub()\\n 560 | \\t\\t\\t\\t\\t?.toggleToolAlwaysAllow(\\n 561 | \\t\\t\\t\\t\\t\\tmessage.serverName!,\\n 562 | \\t\\t\\t\\t\\t\\tmessage.source as \\\"global\\\" | \\\"project\\\",\\n 563 | \\t\\t\\t\\t\\t\\tmessage.toolName!,\\n 564 | \\t\\t\\t\\t\\t\\tBoolean(message.alwaysAllow),\\n 565 | \\t\\t\\t\\t\\t)\\n 566 | \\t\\t\\t} catch (error) {\\n 567 | \\t\\t\\t\\tprovider.log(\\n 568 | \\t\\t\\t\\t\\t`Failed to toggle auto-approve for tool ${message.toolName}: ${JSON.stringify(error, Object.getOwnPropertyNames(error), 2)}`,\\n 569 | \\t\\t\\t\\t)\\n 570 | \\t\\t\\t}\\n 571 | \\t\\t\\tbreak\\n 572 | \\t\\t}\\n 573 | \\t\\tcase \\\"toggleMcpServer\\\": {\\n 574 | \\t\\t\\ttry {\\n 575 | \\t\\t\\t\\tawait provider\\n 576 | \\t\\t\\t\\t\\t.getMcpHub()\\n 577 | \\t\\t\\t\\t\\t?.toggleServerDisabled(\\n 578 | \\t\\t\\t\\t\\t\\tmessage.serverName!,\\n 579 | \\t\\t\\t\\t\\t\\tmessage.disabled!,\\n 580 | \\t\\t\\t\\t\\t\\tmessage.source as \\\"global\\\" | \\\"project\\\",\\n 581 | \\t\\t\\t\\t\\t)\\n 582 | \\t\\t\\t} catch (error) {\\n 583 | \\t\\t\\t\\tprovider.log(\\n 584 | \\t\\t\\t\\t\\t`Failed to toggle MCP server ${message.serverName}: ${JSON.stringify(error, Object.getOwnPropertyNames(error), 2)}`,\\n 585 | \\t\\t\\t\\t)\\n 586 | \\t\\t\\t}\\n 587 | \\t\\t\\tbreak\\n 588 | \\t\\t}\\n 589 | \\t\\tcase \\\"mcpEnabled\\\":\\n 590 | \\t\\t\\tconst mcpEnabled = message.bool ?? true\\n 591 | \\t\\t\\tawait updateGlobalState(\\\"mcpEnabled\\\", mcpEnabled)\\n 592 | \\t\\t\\tawait provider.postStateToWebview()\\n 593 | \\t\\t\\tbreak\\n 594 | \\t\\tcase \\\"enableMcpServerCreation\\\":\\n 595 | \\t\\t\\tawait updateGlobalState(\\\"enableMcpServerCreation\\\", message.bool ?? true)\\n 596 | \\t\\t\\tawait provider.postStateToWebview()\\n 597 | \\t\\t\\tbreak\\n 598 | \\t\\tcase \\\"refreshAllMcpServers\\\": {\\n 599 | \\t\\t\\tconst mcpHub = provider.getMcpHub()\\n 600 | \\t\\t\\tif (mcpHub) {\\n 601 | \\t\\t\\t\\tawait mcpHub.refreshAllConnections()\\n 602 | \\t\\t\\t}\\n 603 | \\t\\t\\tbreak\\n 604 | \\t\\t}\\n 605 | \\t\\t// playSound handler removed - now handled directly in the webview\\n 606 | \\t\\tcase \\\"soundEnabled\\\":\\n 607 | \\t\\t\\tconst soundEnabled = message.bool ?? true\\n 608 | \\t\\t\\tawait updateGlobalState(\\\"soundEnabled\\\", soundEnabled)\\n 609 | \\t\\t\\tawait provider.postStateToWebview()\\n 610 | \\t\\t\\tbreak\\n 611 | \\t\\tcase \\\"soundVolume\\\":\\n 612 | \\t\\t\\tconst soundVolume = message.value ?? 0.5\\n 613 | \\t\\t\\tawait updateGlobalState(\\\"soundVolume\\\", soundVolume)\\n 614 | \\t\\t\\tawait provider.postStateToWebview()\\n 615 | \\t\\t\\tbreak\\n 616 | \\t\\tcase \\\"ttsEnabled\\\":\\n 617 | \\t\\t\\tconst ttsEnabled = message.bool ?? true\\n 618 | \\t\\t\\tawait updateGlobalState(\\\"ttsEnabled\\\", ttsEnabled)\\n 619 | \\t\\t\\tsetTtsEnabled(ttsEnabled) // Add this line to update the tts utility\\n 620 | \\t\\t\\tawait provider.postStateToWebview()\\n 621 | \\t\\t\\tbreak\\n 622 | \\t\\tcase \\\"ttsSpeed\\\":\\n 623 | \\t\\t\\tconst ttsSpeed = message.value ?? 1.0\\n 624 | \\t\\t\\tawait updateGlobalState(\\\"ttsSpeed\\\", ttsSpeed)\\n 625 | \\t\\t\\tsetTtsSpeed(ttsSpeed)\\n 626 | \\t\\t\\tawait provider.postStateToWebview()\\n 627 | \\t\\t\\tbreak\\n 628 | \\t\\tcase \\\"playTts\\\":\\n 629 | \\t\\t\\tif (message.text) {\\n 630 | \\t\\t\\t\\tplayTts(message.text, {\\n 631 | \\t\\t\\t\\t\\tonStart: () => provider.postMessageToWebview({ type: \\\"ttsStart\\\", text: message.text }),\\n 632 | \\t\\t\\t\\t\\tonStop: () => provider.postMessageToWebview({ type: \\\"ttsStop\\\", text: message.text }),\\n 633 | \\t\\t\\t\\t})\\n 634 | \\t\\t\\t}\\n 635 | \\t\\t\\tbreak\\n 636 | \\t\\tcase \\\"stopTts\\\":\\n 637 | \\t\\t\\tstopTts()\\n 638 | \\t\\t\\tbreak\\n 639 | \\t\\tcase \\\"diffEnabled\\\":\\n 640 | \\t\\t\\tconst diffEnabled = message.bool ?? true\\n 641 | \\t\\t\\tawait updateGlobalState(\\\"diffEnabled\\\", diffEnabled)\\n 642 | \\t\\t\\tawait provider.postStateToWebview()\\n 643 | \\t\\t\\tbreak\\n 644 | \\t\\tcase \\\"enableCheckpoints\\\":\\n 645 | \\t\\t\\tconst enableCheckpoints = message.bool ?? true\\n 646 | \\t\\t\\tawait updateGlobalState(\\\"enableCheckpoints\\\", enableCheckpoints)\\n 647 | \\t\\t\\tawait provider.postStateToWebview()\\n 648 | \\t\\t\\tbreak\\n 649 | \\t\\tcase \\\"browserViewportSize\\\":\\n 650 | \\t\\t\\tconst browserViewportSize = message.text ?? \\\"900x600\\\"\\n 651 | \\t\\t\\tawait updateGlobalState(\\\"browserViewportSize\\\", browserViewportSize)\\n 652 | \\t\\t\\tawait provider.postStateToWebview()\\n 653 | \\t\\t\\tbreak\\n 654 | \\t\\tcase \\\"remoteBrowserHost\\\":\\n 655 | \\t\\t\\tawait updateGlobalState(\\\"remoteBrowserHost\\\", message.text)\\n 656 | \\t\\t\\tawait provider.postStateToWebview()\\n 657 | \\t\\t\\tbreak\\n 658 | \\t\\tcase \\\"remoteBrowserEnabled\\\":\\n 659 | \\t\\t\\t// Store the preference in global state\\n 660 | \\t\\t\\t// remoteBrowserEnabled now means \\\"enable remote browser connection\\\"\\n 661 | \\t\\t\\tawait updateGlobalState(\\\"remoteBrowserEnabled\\\", message.bool ?? false)\\n 662 | \\t\\t\\t// If disabling remote browser connection, clear the remoteBrowserHost\\n 663 | \\t\\t\\tif (!message.bool) {\\n 664 | \\t\\t\\t\\tawait updateGlobalState(\\\"remoteBrowserHost\\\", undefined)\\n 665 | \\t\\t\\t}\\n 666 | \\t\\t\\tawait provider.postStateToWebview()\\n 667 | \\t\\t\\tbreak\\n 668 | \\t\\tcase \\\"testBrowserConnection\\\":\\n 669 | \\t\\t\\t// If no text is provided, try auto-discovery\\n 670 | \\t\\t\\tif (!message.text) {\\n 671 | \\t\\t\\t\\t// Use testBrowserConnection for auto-discovery\\n 672 | \\t\\t\\t\\tconst chromeHostUrl = await discoverChromeHostUrl()\\n 673 | \\n 674 | \\t\\t\\t\\tif (chromeHostUrl) {\\n 675 | \\t\\t\\t\\t\\t// Send the result back to the webview\\n 676 | \\t\\t\\t\\t\\tawait provider.postMessageToWebview({\\n 677 | \\t\\t\\t\\t\\t\\ttype: \\\"browserConnectionResult\\\",\\n 678 | \\t\\t\\t\\t\\t\\tsuccess: !!chromeHostUrl,\\n 679 | \\t\\t\\t\\t\\t\\ttext: `Auto-discovered and tested connection to Chrome: ${chromeHostUrl}`,\\n 680 | \\t\\t\\t\\t\\t\\tvalues: { endpoint: chromeHostUrl },\\n 681 | \\t\\t\\t\\t\\t})\\n 682 | \\t\\t\\t\\t} else {\\n 683 | \\t\\t\\t\\t\\tawait provider.postMessageToWebview({\\n 684 | \\t\\t\\t\\t\\t\\ttype: \\\"browserConnectionResult\\\",\\n 685 | \\t\\t\\t\\t\\t\\tsuccess: false,\\n 686 | \\t\\t\\t\\t\\t\\ttext: \\\"No Chrome instances found on the network. Make sure Chrome is running with remote debugging enabled (--remote-debugging-port=9222).\\\",\\n 687 | \\t\\t\\t\\t\\t})\\n 688 | \\t\\t\\t\\t}\\n 689 | \\t\\t\\t} else {\\n 690 | \\t\\t\\t\\t// Test the provided URL\\n 691 | \\t\\t\\t\\tconst customHostUrl = message.text\\n 692 | \\t\\t\\t\\tconst hostIsValid = await tryChromeHostUrl(message.text)\\n 693 | \\n 694 | \\t\\t\\t\\t// Send the result back to the webview\\n 695 | \\t\\t\\t\\tawait provider.postMessageToWebview({\\n 696 | \\t\\t\\t\\t\\ttype: \\\"browserConnectionResult\\\",\\n 697 | \\t\\t\\t\\t\\tsuccess: hostIsValid,\\n 698 | \\t\\t\\t\\t\\ttext: hostIsValid\\n 699 | \\t\\t\\t\\t\\t\\t? `Successfully connected to Chrome: ${customHostUrl}`\\n 700 | \\t\\t\\t\\t\\t\\t: \\\"Failed to connect to Chrome\\\",\\n 701 | \\t\\t\\t\\t})\\n 702 | \\t\\t\\t}\\n 703 | \\t\\t\\tbreak\\n 704 | \\t\\tcase \\\"fuzzyMatchThreshold\\\":\\n 705 | \\t\\t\\tawait updateGlobalState(\\\"fuzzyMatchThreshold\\\", message.value)\\n 706 | \\t\\t\\tawait provider.postStateToWebview()\\n 707 | \\t\\t\\tbreak\\n 708 | \\t\\tcase \\\"updateVSCodeSetting\\\": {\\n 709 | \\t\\t\\tconst { setting, value } = message\\n 710 | \\n 711 | \\t\\t\\tif (setting !== undefined && value !== undefined) {\\n 712 | \\t\\t\\t\\tif (ALLOWED_VSCODE_SETTINGS.has(setting)) {\\n 713 | \\t\\t\\t\\t\\tawait vscode.workspace.getConfiguration().update(setting, value, true)\\n 714 | \\t\\t\\t\\t} else {\\n 715 | \\t\\t\\t\\t\\tvscode.window.showErrorMessage(`Cannot update restricted VSCode setting: ${setting}`)\\n 716 | \\t\\t\\t\\t}\\n 717 | \\t\\t\\t}\\n 718 | \\n 719 | \\t\\t\\tbreak\\n 720 | \\t\\t}\\n 721 | \\t\\tcase \\\"getVSCodeSetting\\\":\\n 722 | \\t\\t\\tconst { setting } = message\\n 723 | \\n 724 | \\t\\t\\tif (setting) {\\n 725 | \\t\\t\\t\\ttry {\\n 726 | \\t\\t\\t\\t\\tawait provider.postMessageToWebview({\\n 727 | \\t\\t\\t\\t\\t\\ttype: \\\"vsCodeSetting\\\",\\n 728 | \\t\\t\\t\\t\\t\\tsetting,\\n 729 | \\t\\t\\t\\t\\t\\tvalue: vscode.workspace.getConfiguration().get(setting),\\n 730 | \\t\\t\\t\\t\\t})\\n 731 | \\t\\t\\t\\t} catch (error) {\\n 732 | \\t\\t\\t\\t\\tconsole.error(`Failed to get VSCode setting ${message.setting}:`, error)\\n 733 | \\n 734 | \\t\\t\\t\\t\\tawait provider.postMessageToWebview({\\n 735 | \\t\\t\\t\\t\\t\\ttype: \\\"vsCodeSetting\\\",\\n 736 | \\t\\t\\t\\t\\t\\tsetting,\\n 737 | \\t\\t\\t\\t\\t\\terror: `Failed to get setting: ${error.message}`,\\n 738 | \\t\\t\\t\\t\\t\\tvalue: undefined,\\n 739 | \\t\\t\\t\\t\\t})\\n 740 | \\t\\t\\t\\t}\\n 741 | \\t\\t\\t}\\n 742 | \\n 743 | \\t\\t\\tbreak\\n 744 | \\t\\tcase \\\"alwaysApproveResubmit\\\":\\n 745 | \\t\\t\\tawait updateGlobalState(\\\"alwaysApproveResubmit\\\", message.bool ?? false)\\n 746 | \\t\\t\\tawait provider.postStateToWebview()\\n 747 | \\t\\t\\tbreak\\n 748 | \\t\\tcase \\\"requestDelaySeconds\\\":\\n 749 | \\t\\t\\tawait updateGlobalState(\\\"requestDelaySeconds\\\", message.value ?? 5)\\n 750 | \\t\\t\\tawait provider.postStateToWebview()\\n 751 | \\t\\t\\tbreak\\n 752 | \\t\\tcase \\\"writeDelayMs\\\":\\n 753 | \\t\\t\\tawait updateGlobalState(\\\"writeDelayMs\\\", message.value)\\n 754 | \\t\\t\\tawait provider.postStateToWebview()\\n 755 | \\t\\t\\tbreak\\n 756 | \\t\\tcase \\\"terminalOutputLineLimit\\\":\\n 757 | \\t\\t\\tawait updateGlobalState(\\\"terminalOutputLineLimit\\\", message.value)\\n 758 | \\t\\t\\tawait provider.postStateToWebview()\\n 759 | \\t\\t\\tbreak\\n 760 | \\t\\tcase \\\"terminalShellIntegrationTimeout\\\":\\n 761 | \\t\\t\\tawait updateGlobalState(\\\"terminalShellIntegrationTimeout\\\", message.value)\\n 762 | \\t\\t\\tawait provider.postStateToWebview()\\n 763 | \\t\\t\\tif (message.value !== undefined) {\\n 764 | \\t\\t\\t\\tTerminal.setShellIntegrationTimeout(message.value)\\n 765 | \\t\\t\\t}\\n 766 | \\t\\t\\tbreak\\n 767 | \\t\\tcase \\\"terminalShellIntegrationDisabled\\\":\\n 768 | \\t\\t\\tawait updateGlobalState(\\\"terminalShellIntegrationDisabled\\\", message.bool)\\n 769 | \\t\\t\\tawait provider.postStateToWebview()\\n 770 | \\t\\t\\tif (message.bool !== undefined) {\\n 771 | \\t\\t\\t\\tTerminal.setShellIntegrationDisabled(message.bool)\\n 772 | \\t\\t\\t}\\n 773 | \\t\\t\\tbreak\\n 774 | \\t\\tcase \\\"terminalCommandDelay\\\":\\n 775 | \\t\\t\\tawait updateGlobalState(\\\"terminalCommandDelay\\\", message.value)\\n 776 | \\t\\t\\tawait provider.postStateToWebview()\\n 777 | \\t\\t\\tif (message.value !== undefined) {\\n 778 | \\t\\t\\t\\tTerminal.setCommandDelay(message.value)\\n 779 | \\t\\t\\t}\\n 780 | \\t\\t\\tbreak\\n 781 | \\t\\tcase \\\"terminalPowershellCounter\\\":\\n 782 | \\t\\t\\tawait updateGlobalState(\\\"terminalPowershellCounter\\\", message.bool)\\n 783 | \\t\\t\\tawait provider.postStateToWebview()\\n 784 | \\t\\t\\tif (message.bool !== undefined) {\\n 785 | \\t\\t\\t\\tTerminal.setPowershellCounter(message.bool)\\n 786 | \\t\\t\\t}\\n 787 | \\t\\t\\tbreak\\n 788 | \\t\\tcase \\\"terminalZshClearEolMark\\\":\\n 789 | \\t\\t\\tawait updateGlobalState(\\\"terminalZshClearEolMark\\\", message.bool)\\n 790 | \\t\\t\\tawait provider.postStateToWebview()\\n 791 | \\t\\t\\tif (message.bool !== undefined) {\\n 792 | \\t\\t\\t\\tTerminal.setTerminalZshClearEolMark(message.bool)\\n 793 | \\t\\t\\t}\\n 794 | \\t\\t\\tbreak\\n 795 | \\t\\tcase \\\"terminalZshOhMy\\\":\\n 796 | \\t\\t\\tawait updateGlobalState(\\\"terminalZshOhMy\\\", message.bool)\\n 797 | \\t\\t\\tawait provider.postStateToWebview()\\n 798 | \\t\\t\\tif (message.bool !== undefined) {\\n 799 | \\t\\t\\t\\tTerminal.setTerminalZshOhMy(message.bool)\\n 800 | \\t\\t\\t}\\n 801 | \\t\\t\\tbreak\\n 802 | \\t\\tcase \\\"terminalZshP10k\\\":\\n 803 | \\t\\t\\tawait updateGlobalState(\\\"terminalZshP10k\\\", message.bool)\\n 804 | \\t\\t\\tawait provider.postStateToWebview()\\n 805 | \\t\\t\\tif (message.bool !== undefined) {\\n 806 | \\t\\t\\t\\tTerminal.setTerminalZshP10k(message.bool)\\n 807 | \\t\\t\\t}\\n 808 | \\t\\t\\tbreak\\n 809 | \\t\\tcase \\\"terminalZdotdir\\\":\\n 810 | \\t\\t\\tawait updateGlobalState(\\\"terminalZdotdir\\\", message.bool)\\n 811 | \\t\\t\\tawait provider.postStateToWebview()\\n 812 | \\t\\t\\tif (message.bool !== undefined) {\\n 813 | \\t\\t\\t\\tTerminal.setTerminalZdotdir(message.bool)\\n 814 | \\t\\t\\t}\\n 815 | \\t\\t\\tbreak\\n 816 | \\t\\tcase \\\"terminalCompressProgressBar\\\":\\n 817 | \\t\\t\\tawait updateGlobalState(\\\"terminalCompressProgressBar\\\", message.bool)\\n 818 | \\t\\t\\tawait provider.postStateToWebview()\\n 819 | \\t\\t\\tif (message.bool !== undefined) {\\n 820 | \\t\\t\\t\\tTerminal.setCompressProgressBar(message.bool)\\n 821 | \\t\\t\\t}\\n 822 | \\t\\t\\tbreak\\n 823 | \\t\\tcase \\\"mode\\\":\\n 824 | \\t\\t\\tawait provider.handleModeSwitch(message.text as Mode)\\n 825 | \\t\\t\\tbreak\\n 826 | \\t\\tcase \\\"updateSupportPrompt\\\":\\n 827 | \\t\\t\\ttry {\\n 828 | \\t\\t\\t\\tif (Object.keys(message?.values ?? {}).length === 0) {\\n 829 | \\t\\t\\t\\t\\treturn\\n 830 | \\t\\t\\t\\t}\\n 831 | \\n 832 | \\t\\t\\t\\tconst existingPrompts = getGlobalState(\\\"customSupportPrompts\\\") ?? {}\\n 833 | \\t\\t\\t\\tconst updatedPrompts = { ...existingPrompts, ...message.values }\\n 834 | \\t\\t\\t\\tawait updateGlobalState(\\\"customSupportPrompts\\\", updatedPrompts)\\n 835 | \\t\\t\\t\\tawait provider.postStateToWebview()\\n 836 | \\t\\t\\t} catch (error) {\\n 837 | \\t\\t\\t\\tprovider.log(\\n 838 | \\t\\t\\t\\t\\t`Error update support prompt: ${JSON.stringify(error, Object.getOwnPropertyNames(error), 2)}`,\\n 839 | \\t\\t\\t\\t)\\n 840 | \\t\\t\\t\\tvscode.window.showErrorMessage(t(\\\"common:errors.update_support_prompt\\\"))\\n 841 | \\t\\t\\t}\\n 842 | \\t\\t\\tbreak\\n 843 | \\t\\tcase \\\"resetSupportPrompt\\\":\\n 844 | \\t\\t\\ttry {\\n 845 | \\t\\t\\t\\tif (!message?.text) {\\n 846 | \\t\\t\\t\\t\\treturn\\n 847 | \\t\\t\\t\\t}\\n 848 | \\n 849 | \\t\\t\\t\\tconst existingPrompts = getGlobalState(\\\"customSupportPrompts\\\") ?? {}\\n 850 | \\t\\t\\t\\tconst updatedPrompts = { ...existingPrompts }\\n 851 | \\t\\t\\t\\tupdatedPrompts[message.text] = undefined\\n 852 | \\t\\t\\t\\tawait updateGlobalState(\\\"customSupportPrompts\\\", updatedPrompts)\\n 853 | \\t\\t\\t\\tawait provider.postStateToWebview()\\n 854 | \\t\\t\\t} catch (error) {\\n 855 | \\t\\t\\t\\tprovider.log(\\n 856 | \\t\\t\\t\\t\\t`Error reset support prompt: ${JSON.stringify(error, Object.getOwnPropertyNames(error), 2)}`,\\n 857 | \\t\\t\\t\\t)\\n 858 | \\t\\t\\t\\tvscode.window.showErrorMessage(t(\\\"common:errors.reset_support_prompt\\\"))\\n 859 | \\t\\t\\t}\\n 860 | \\t\\t\\tbreak\\n 861 | \\t\\tcase \\\"updatePrompt\\\":\\n 862 | \\t\\t\\tif (message.promptMode && message.customPrompt !== undefined) {\\n 863 | \\t\\t\\t\\tconst existingPrompts = getGlobalState(\\\"customModePrompts\\\") ?? {}\\n 864 | \\t\\t\\t\\tconst updatedPrompts = { ...existingPrompts, [message.promptMode]: message.customPrompt }\\n 865 | \\t\\t\\t\\tawait updateGlobalState(\\\"customModePrompts\\\", updatedPrompts)\\n 866 | \\t\\t\\t\\tconst currentState = await provider.getStateToPostToWebview()\\n 867 | \\t\\t\\t\\tconst stateWithPrompts = { ...currentState, customModePrompts: updatedPrompts }\\n 868 | \\t\\t\\t\\tprovider.postMessageToWebview({ type: \\\"state\\\", state: stateWithPrompts })\\n 869 | \\t\\t\\t}\\n 870 | \\t\\t\\tbreak\\n 871 | \\t\\tcase \\\"deleteMessage\\\": {\\n 872 | \\t\\t\\tconst answer = await vscode.window.showInformationMessage(\\n 873 | \\t\\t\\t\\tt(\\\"common:confirmation.delete_message\\\"),\\n 874 | \\t\\t\\t\\t{ modal: true },\\n 875 | \\t\\t\\t\\tt(\\\"common:confirmation.just_this_message\\\"),\\n 876 | \\t\\t\\t\\tt(\\\"common:confirmation.this_and_subsequent\\\"),\\n 877 | \\t\\t\\t)\\n 878 | \\n 879 | \\t\\t\\tif (\\n 880 | \\t\\t\\t\\t(answer === t(\\\"common:confirmation.just_this_message\\\") ||\\n 881 | \\t\\t\\t\\t\\tanswer === t(\\\"common:confirmation.this_and_subsequent\\\")) &&\\n 882 | \\t\\t\\t\\tprovider.getCurrentCline() &&\\n 883 | \\t\\t\\t\\ttypeof message.value === \\\"number\\\" &&\\n 884 | \\t\\t\\t\\tmessage.value\\n 885 | \\t\\t\\t) {\\n 886 | \\t\\t\\t\\tconst timeCutoff = message.value - 1000 // 1 second buffer before the message to delete\\n 887 | \\n 888 | \\t\\t\\t\\tconst messageIndex = provider\\n 889 | \\t\\t\\t\\t\\t.getCurrentCline()!\\n 890 | \\t\\t\\t\\t\\t.clineMessages.findIndex((msg) => msg.ts && msg.ts >= timeCutoff)\\n 891 | \\n 892 | \\t\\t\\t\\tconst apiConversationHistoryIndex = provider\\n 893 | \\t\\t\\t\\t\\t.getCurrentCline()\\n 894 | \\t\\t\\t\\t\\t?.apiConversationHistory.findIndex((msg) => msg.ts && msg.ts >= timeCutoff)\\n 895 | \\n 896 | \\t\\t\\t\\tif (messageIndex !== -1) {\\n 897 | \\t\\t\\t\\t\\tconst { historyItem } = await provider.getTaskWithId(provider.getCurrentCline()!.taskId)\\n 898 | \\n 899 | \\t\\t\\t\\t\\tif (answer === t(\\\"common:confirmation.just_this_message\\\")) {\\n 900 | \\t\\t\\t\\t\\t\\t// Find the next user message first\\n 901 | \\t\\t\\t\\t\\t\\tconst nextUserMessage = provider\\n 902 | \\t\\t\\t\\t\\t\\t\\t.getCurrentCline()!\\n 903 | \\t\\t\\t\\t\\t\\t\\t.clineMessages.slice(messageIndex + 1)\\n 904 | \\t\\t\\t\\t\\t\\t\\t.find((msg) => msg.type === \\\"say\\\" && msg.say === \\\"user_feedback\\\")\\n 905 | \\n 906 | \\t\\t\\t\\t\\t\\t// Handle UI messages\\n 907 | \\t\\t\\t\\t\\t\\tif (nextUserMessage) {\\n 908 | \\t\\t\\t\\t\\t\\t\\t// Find absolute index of next user message\\n 909 | \\t\\t\\t\\t\\t\\t\\tconst nextUserMessageIndex = provider\\n 910 | \\t\\t\\t\\t\\t\\t\\t\\t.getCurrentCline()!\\n 911 | \\t\\t\\t\\t\\t\\t\\t\\t.clineMessages.findIndex((msg) => msg === nextUserMessage)\\n 912 | \\n 913 | \\t\\t\\t\\t\\t\\t\\t// Keep messages before current message and after next user message\\n 914 | \\t\\t\\t\\t\\t\\t\\tawait provider\\n 915 | \\t\\t\\t\\t\\t\\t\\t\\t.getCurrentCline()!\\n 916 | \\t\\t\\t\\t\\t\\t\\t\\t.overwriteClineMessages([\\n 917 | \\t\\t\\t\\t\\t\\t\\t\\t\\t...provider.getCurrentCline()!.clineMessages.slice(0, messageIndex),\\n 918 | \\t\\t\\t\\t\\t\\t\\t\\t\\t...provider.getCurrentCline()!.clineMessages.slice(nextUserMessageIndex),\\n 919 | \\t\\t\\t\\t\\t\\t\\t\\t])\\n 920 | \\t\\t\\t\\t\\t\\t} else {\\n 921 | \\t\\t\\t\\t\\t\\t\\t// If no next user message, keep only messages before current message\\n 922 | \\t\\t\\t\\t\\t\\t\\tawait provider\\n 923 | \\t\\t\\t\\t\\t\\t\\t\\t.getCurrentCline()!\\n 924 | \\t\\t\\t\\t\\t\\t\\t\\t.overwriteClineMessages(\\n 925 | \\t\\t\\t\\t\\t\\t\\t\\t\\tprovider.getCurrentCline()!.clineMessages.slice(0, messageIndex),\\n 926 | \\t\\t\\t\\t\\t\\t\\t\\t)\\n 927 | \\t\\t\\t\\t\\t\\t}\\n 928 | \\n 929 | \\t\\t\\t\\t\\t\\t// Handle API messages\\n 930 | \\t\\t\\t\\t\\t\\tif (apiConversationHistoryIndex !== -1) {\\n 931 | \\t\\t\\t\\t\\t\\t\\tif (nextUserMessage && nextUserMessage.ts) {\\n 932 | \\t\\t\\t\\t\\t\\t\\t\\t// Keep messages before current API message and after next user message\\n 933 | \\t\\t\\t\\t\\t\\t\\t\\tawait provider\\n 934 | \\t\\t\\t\\t\\t\\t\\t\\t\\t.getCurrentCline()!\\n 935 | \\t\\t\\t\\t\\t\\t\\t\\t\\t.overwriteApiConversationHistory([\\n 936 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t...provider\\n 937 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t.getCurrentCline()!\\n 938 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t.apiConversationHistory.slice(0, apiConversationHistoryIndex),\\n 939 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t...provider\\n 940 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t.getCurrentCline()!\\n 941 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t.apiConversationHistory.filter(\\n 942 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t(msg) => msg.ts && msg.ts >= nextUserMessage.ts,\\n 943 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t),\\n 944 | \\t\\t\\t\\t\\t\\t\\t\\t\\t])\\n 945 | \\t\\t\\t\\t\\t\\t\\t} else {\\n 946 | \\t\\t\\t\\t\\t\\t\\t\\t// If no next user message, keep only messages before current API message\\n 947 | \\t\\t\\t\\t\\t\\t\\t\\tawait provider\\n 948 | \\t\\t\\t\\t\\t\\t\\t\\t\\t.getCurrentCline()!\\n 949 | \\t\\t\\t\\t\\t\\t\\t\\t\\t.overwriteApiConversationHistory(\\n 950 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprovider\\n 951 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t.getCurrentCline()!\\n 952 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\t.apiConversationHistory.slice(0, apiConversationHistoryIndex),\\n 953 | \\t\\t\\t\\t\\t\\t\\t\\t\\t)\\n 954 | \\t\\t\\t\\t\\t\\t\\t}\\n 955 | \\t\\t\\t\\t\\t\\t}\\n 956 | \\t\\t\\t\\t\\t} else if (answer === t(\\\"common:confirmation.this_and_subsequent\\\")) {\\n 957 | \\t\\t\\t\\t\\t\\t// Delete this message and all that follow\\n 958 | \\t\\t\\t\\t\\t\\tawait provider\\n 959 | \\t\\t\\t\\t\\t\\t\\t.getCurrentCline()!\\n 960 | \\t\\t\\t\\t\\t\\t\\t.overwriteClineMessages(provider.getCurrentCline()!.clineMessages.slice(0, messageIndex))\\n 961 | \\t\\t\\t\\t\\t\\tif (apiConversationHistoryIndex !== -1) {\\n 962 | \\t\\t\\t\\t\\t\\t\\tawait provider\\n 963 | \\t\\t\\t\\t\\t\\t\\t\\t.getCurrentCline()!\\n 964 | \\t\\t\\t\\t\\t\\t\\t\\t.overwriteApiConversationHistory(\\n 965 | \\t\\t\\t\\t\\t\\t\\t\\t\\tprovider\\n 966 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t.getCurrentCline()!\\n 967 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t.apiConversationHistory.slice(0, apiConversationHistoryIndex),\\n 968 | \\t\\t\\t\\t\\t\\t\\t\\t)\\n 969 | \\t\\t\\t\\t\\t\\t}\\n 970 | \\t\\t\\t\\t\\t}\\n 971 | \\n 972 | \\t\\t\\t\\t\\tawait provider.initClineWithHistoryItem(historyItem)\\n 973 | \\t\\t\\t\\t}\\n 974 | \\t\\t\\t}\\n 975 | \\t\\t\\tbreak\\n 976 | \\t\\t}\\n 977 | \\t\\tcase \\\"screenshotQuality\\\":\\n 978 | \\t\\t\\tawait updateGlobalState(\\\"screenshotQuality\\\", message.value)\\n 979 | \\t\\t\\tawait provider.postStateToWebview()\\n 980 | \\t\\t\\tbreak\\n 981 | \\t\\tcase \\\"maxOpenTabsContext\\\":\\n 982 | \\t\\t\\tconst tabCount = Math.min(Math.max(0, message.value ?? 20), 500)\\n 983 | \\t\\t\\tawait updateGlobalState(\\\"maxOpenTabsContext\\\", tabCount)\\n 984 | \\t\\t\\tawait provider.postStateToWebview()\\n 985 | \\t\\t\\tbreak\\n 986 | \\t\\tcase \\\"maxWorkspaceFiles\\\":\\n 987 | \\t\\t\\tconst fileCount = Math.min(Math.max(0, message.value ?? 200), 500)\\n 988 | \\t\\t\\tawait updateGlobalState(\\\"maxWorkspaceFiles\\\", fileCount)\\n 989 | \\t\\t\\tawait provider.postStateToWebview()\\n 990 | \\t\\t\\tbreak\\n 991 | \\t\\tcase \\\"browserToolEnabled\\\":\\n 992 | \\t\\t\\tawait updateGlobalState(\\\"browserToolEnabled\\\", message.bool ?? true)\\n 993 | \\t\\t\\tawait provider.postStateToWebview()\\n 994 | \\t\\t\\tbreak\\n 995 | \\t\\tcase \\\"language\\\":\\n 996 | \\t\\t\\tchangeLanguage(message.text ?? \\\"en\\\")\\n 997 | \\t\\t\\tawait updateGlobalState(\\\"language\\\", message.text as Language)\\n 998 | \\t\\t\\tawait provider.postStateToWebview()\\n 999 | \\t\\t\\tbreak\\n1000 | \\t\\tcase \\\"showRooIgnoredFiles\\\":\\n1001 | \\t\\t\\tawait updateGlobalState(\\\"showRooIgnoredFiles\\\", message.bool ?? true)\\n1002 | \\t\\t\\tawait provider.postStateToWebview()\\n1003 | \\t\\t\\tbreak\\n1004 | \\t\\tcase \\\"maxReadFileLine\\\":\\n1005 | \\t\\t\\tawait updateGlobalState(\\\"maxReadFileLine\\\", message.value)\\n1006 | \\t\\t\\tawait provider.postStateToWebview()\\n1007 | \\t\\t\\tbreak\\n1008 | \\t\\tcase \\\"maxConcurrentFileReads\\\":\\n1009 | \\t\\t\\tconst valueToSave = message.value // Capture the value intended for saving\\n1010 | \\t\\t\\tawait updateGlobalState(\\\"maxConcurrentFileReads\\\", valueToSave)\\n1011 | \\t\\t\\tawait provider.postStateToWebview()\\n1012 | \\t\\t\\tbreak\\n1013 | \\t\\tcase \\\"setHistoryPreviewCollapsed\\\": // Add the new case handler\\n1014 | \\t\\t\\tawait updateGlobalState(\\\"historyPreviewCollapsed\\\", message.bool ?? false)\\n1015 | \\t\\t\\t// No need to call postStateToWebview here as the UI already updated optimistically\\n1016 | \\t\\t\\tbreak\\n1017 | \\t\\tcase \\\"toggleApiConfigPin\\\":\\n1018 | \\t\\t\\tif (message.text) {\\n1019 | \\t\\t\\t\\tconst currentPinned = getGlobalState(\\\"pinnedApiConfigs\\\") ?? {}\\n1020 | \\t\\t\\t\\tconst updatedPinned: Record<string, boolean> = { ...currentPinned }\\n1021 | \\n1022 | \\t\\t\\t\\tif (currentPinned[message.text]) {\\n1023 | \\t\\t\\t\\t\\tdelete updatedPinned[message.text]\\n1024 | \\t\\t\\t\\t} else {\\n1025 | \\t\\t\\t\\t\\tupdatedPinned[message.text] = true\\n1026 | \\t\\t\\t\\t}\\n1027 | \\n1028 | \\t\\t\\t\\tawait updateGlobalState(\\\"pinnedApiConfigs\\\", updatedPinned)\\n1029 | \\t\\t\\t\\tawait provider.postStateToWebview()\\n1030 | \\t\\t\\t}\\n1031 | \\t\\t\\tbreak\\n1032 | \\t\\tcase \\\"enhancementApiConfigId\\\":\\n1033 | \\t\\t\\tawait updateGlobalState(\\\"enhancementApiConfigId\\\", message.text)\\n1034 | \\t\\t\\tawait provider.postStateToWebview()\\n1035 | \\t\\t\\tbreak\\n1036 | \\t\\tcase \\\"condensingApiConfigId\\\":\\n1037 | \\t\\t\\tawait updateGlobalState(\\\"condensingApiConfigId\\\", message.text)\\n1038 | \\t\\t\\tawait provider.postStateToWebview()\\n1039 | \\t\\t\\tbreak\\n1040 | \\t\\tcase \\\"updateCondensingPrompt\\\":\\n1041 | \\t\\t\\tawait updateGlobalState(\\\"customCondensingPrompt\\\", message.text)\\n1042 | \\t\\t\\tawait provider.postStateToWebview()\\n1043 | \\t\\t\\tbreak\\n1044 | \\t\\tcase \\\"autoApprovalEnabled\\\":\\n1045 | \\t\\t\\tawait updateGlobalState(\\\"autoApprovalEnabled\\\", message.bool ?? false)\\n1046 | \\t\\t\\tawait provider.postStateToWebview()\\n1047 | \\t\\t\\tbreak\\n1048 | \\t\\tcase \\\"enhancePrompt\\\":\\n1049 | \\t\\t\\tif (message.text) {\\n1050 | \\t\\t\\t\\ttry {\\n1051 | \\t\\t\\t\\t\\tconst { apiConfiguration, customSupportPrompts, listApiConfigMeta, enhancementApiConfigId } =\\n1052 | \\t\\t\\t\\t\\t\\tawait provider.getState()\\n1053 | \\n1054 | \\t\\t\\t\\t\\t// Try to get enhancement config first, fall back to current config.\\n1055 | \\t\\t\\t\\t\\tlet configToUse: ProviderSettings = apiConfiguration\\n1056 | \\n1057 | \\t\\t\\t\\t\\tif (enhancementApiConfigId && !!listApiConfigMeta.find(({ id }) => id === enhancementApiConfigId)) {\\n1058 | \\t\\t\\t\\t\\t\\tconst { name: _, ...providerSettings } = await provider.providerSettingsManager.getProfile({\\n1059 | \\t\\t\\t\\t\\t\\t\\tid: enhancementApiConfigId,\\n1060 | \\t\\t\\t\\t\\t\\t})\\n1061 | \\n1062 | \\t\\t\\t\\t\\t\\tif (providerSettings.apiProvider) {\\n1063 | \\t\\t\\t\\t\\t\\t\\tconfigToUse = providerSettings\\n1064 | \\t\\t\\t\\t\\t\\t}\\n1065 | \\t\\t\\t\\t\\t}\\n1066 | \\n1067 | \\t\\t\\t\\t\\tconst enhancedPrompt = await singleCompletionHandler(\\n1068 | \\t\\t\\t\\t\\t\\tconfigToUse,\\n1069 | \\t\\t\\t\\t\\t\\tsupportPrompt.create(\\\"ENHANCE\\\", { userInput: message.text }, customSupportPrompts),\\n1070 | \\t\\t\\t\\t\\t)\\n1071 | \\n1072 | \\t\\t\\t\\t\\t// Capture telemetry for prompt enhancement.\\n1073 | \\t\\t\\t\\t\\tconst currentCline = provider.getCurrentCline()\\n1074 | \\t\\t\\t\\t\\tTelemetryService.instance.capturePromptEnhanced(currentCline?.taskId)\\n1075 | \\n1076 | \\t\\t\\t\\t\\tawait provider.postMessageToWebview({ type: \\\"enhancedPrompt\\\", text: enhancedPrompt })\\n1077 | \\t\\t\\t\\t} catch (error) {\\n1078 | \\t\\t\\t\\t\\tprovider.log(\\n1079 | \\t\\t\\t\\t\\t\\t`Error enhancing prompt: ${JSON.stringify(error, Object.getOwnPropertyNames(error), 2)}`,\\n1080 | \\t\\t\\t\\t\\t)\\n1081 | \\n1082 | \\t\\t\\t\\t\\tvscode.window.showErrorMessage(t(\\\"common:errors.enhance_prompt\\\"))\\n1083 | \\t\\t\\t\\t\\tawait provider.postMessageToWebview({ type: \\\"enhancedPrompt\\\" })\\n1084 | \\t\\t\\t\\t}\\n1085 | \\t\\t\\t}\\n1086 | \\t\\t\\tbreak\\n1087 | \\t\\tcase \\\"getSystemPrompt\\\":\\n1088 | \\t\\t\\ttry {\\n1089 | \\t\\t\\t\\tconst systemPrompt = await generateSystemPrompt(provider, message)\\n1090 | \\n1091 | \\t\\t\\t\\tawait provider.postMessageToWebview({\\n1092 | \\t\\t\\t\\t\\ttype: \\\"systemPrompt\\\",\\n1093 | \\t\\t\\t\\t\\ttext: systemPrompt,\\n1094 | \\t\\t\\t\\t\\tmode: message.mode,\\n1095 | \\t\\t\\t\\t})\\n1096 | \\t\\t\\t} catch (error) {\\n1097 | \\t\\t\\t\\tprovider.log(\\n1098 | \\t\\t\\t\\t\\t`Error getting system prompt:  ${JSON.stringify(error, Object.getOwnPropertyNames(error), 2)}`,\\n1099 | \\t\\t\\t\\t)\\n1100 | \\t\\t\\t\\tvscode.window.showErrorMessage(t(\\\"common:errors.get_system_prompt\\\"))\\n1101 | \\t\\t\\t}\\n1102 | \\t\\t\\tbreak\\n1103 | \\t\\tcase \\\"copySystemPrompt\\\":\\n1104 | \\t\\t\\ttry {\\n1105 | \\t\\t\\t\\tconst systemPrompt = await generateSystemPrompt(provider, message)\\n1106 | \\n1107 | \\t\\t\\t\\tawait vscode.env.clipboard.writeText(systemPrompt)\\n1108 | \\t\\t\\t\\tawait vscode.window.showInformationMessage(t(\\\"common:info.clipboard_copy\\\"))\\n1109 | \\t\\t\\t} catch (error) {\\n1110 | \\t\\t\\t\\tprovider.log(\\n1111 | \\t\\t\\t\\t\\t`Error getting system prompt:  ${JSON.stringify(error, Object.getOwnPropertyNames(error), 2)}`,\\n1112 | \\t\\t\\t\\t)\\n1113 | \\t\\t\\t\\tvscode.window.showErrorMessage(t(\\\"common:errors.get_system_prompt\\\"))\\n1114 | \\t\\t\\t}\\n1115 | \\t\\t\\tbreak\\n1116 | \\t\\tcase \\\"searchCommits\\\": {\\n1117 | \\t\\t\\tconst cwd = provider.cwd\\n1118 | \\t\\t\\tif (cwd) {\\n1119 | \\t\\t\\t\\ttry {\\n1120 | \\t\\t\\t\\t\\tconst commits = await searchCommits(message.query || \\\"\\\", cwd)\\n1121 | \\t\\t\\t\\t\\tawait provider.postMessageToWebview({\\n1122 | \\t\\t\\t\\t\\t\\ttype: \\\"commitSearchResults\\\",\\n1123 | \\t\\t\\t\\t\\t\\tcommits,\\n1124 | \\t\\t\\t\\t\\t})\\n1125 | \\t\\t\\t\\t} catch (error) {\\n1126 | \\t\\t\\t\\t\\tprovider.log(\\n1127 | \\t\\t\\t\\t\\t\\t`Error searching commits: ${JSON.stringify(error, Object.getOwnPropertyNames(error), 2)}`,\\n1128 | \\t\\t\\t\\t\\t)\\n1129 | \\t\\t\\t\\t\\tvscode.window.showErrorMessage(t(\\\"common:errors.search_commits\\\"))\\n1130 | \\t\\t\\t\\t}\\n1131 | \\t\\t\\t}\\n1132 | \\t\\t\\tbreak\\n1133 | \\t\\t}\\n1134 | \\t\\tcase \\\"searchFiles\\\": {\\n1135 | \\t\\t\\tconst workspacePath = getWorkspacePath()\\n1136 | \\n1137 | \\t\\t\\tif (!workspacePath) {\\n1138 | \\t\\t\\t\\t// Handle case where workspace path is not available\\n1139 | \\t\\t\\t\\tawait provider.postMessageToWebview({\\n1140 | \\t\\t\\t\\t\\ttype: \\\"fileSearchResults\\\",\\n1141 | \\t\\t\\t\\t\\tresults: [],\\n1142 | \\t\\t\\t\\t\\trequestId: message.requestId,\\n1143 | \\t\\t\\t\\t\\terror: \\\"No workspace path available\\\",\\n1144 | \\t\\t\\t\\t})\\n1145 | \\t\\t\\t\\tbreak\\n1146 | \\t\\t\\t}\\n1147 | \\t\\t\\ttry {\\n1148 | \\t\\t\\t\\t// Call file search service with query from message\\n1149 | \\t\\t\\t\\tconst results = await searchWorkspaceFiles(\\n1150 | \\t\\t\\t\\t\\tmessage.query || \\\"\\\",\\n1151 | \\t\\t\\t\\t\\tworkspacePath,\\n1152 | \\t\\t\\t\\t\\t20, // Use default limit, as filtering is now done in the backend\\n1153 | \\t\\t\\t\\t)\\n1154 | \\n1155 | \\t\\t\\t\\t// Send results back to webview\\n1156 | \\t\\t\\t\\tawait provider.postMessageToWebview({\\n1157 | \\t\\t\\t\\t\\ttype: \\\"fileSearchResults\\\",\\n1158 | \\t\\t\\t\\t\\tresults,\\n1159 | \\t\\t\\t\\t\\trequestId: message.requestId,\\n1160 | \\t\\t\\t\\t})\\n1161 | \\t\\t\\t} catch (error) {\\n1162 | \\t\\t\\t\\tconst errorMessage = error instanceof Error ? error.message : String(error)\\n1163 | \\n1164 | \\t\\t\\t\\t// Send error response to webview\\n1165 | \\t\\t\\t\\tawait provider.postMessageToWebview({\\n1166 | \\t\\t\\t\\t\\ttype: \\\"fileSearchResults\\\",\\n1167 | \\t\\t\\t\\t\\tresults: [],\\n1168 | \\t\\t\\t\\t\\terror: errorMessage,\\n1169 | \\t\\t\\t\\t\\trequestId: message.requestId,\\n1170 | \\t\\t\\t\\t})\\n1171 | \\t\\t\\t}\\n1172 | \\t\\t\\tbreak\\n1173 | \\t\\t}\\n1174 | \\t\\tcase \\\"saveApiConfiguration\\\":\\n1175 | \\t\\t\\tif (message.text && message.apiConfiguration) {\\n1176 | \\t\\t\\t\\ttry {\\n1177 | \\t\\t\\t\\t\\tawait provider.providerSettingsManager.saveConfig(message.text, message.apiConfiguration)\\n1178 | \\t\\t\\t\\t\\tconst listApiConfig = await provider.providerSettingsManager.listConfig()\\n1179 | \\t\\t\\t\\t\\tawait updateGlobalState(\\\"listApiConfigMeta\\\", listApiConfig)\\n1180 | \\t\\t\\t\\t} catch (error) {\\n1181 | \\t\\t\\t\\t\\tprovider.log(\\n1182 | \\t\\t\\t\\t\\t\\t`Error save api configuration: ${JSON.stringify(error, Object.getOwnPropertyNames(error), 2)}`,\\n1183 | \\t\\t\\t\\t\\t)\\n1184 | \\t\\t\\t\\t\\tvscode.window.showErrorMessage(t(\\\"common:errors.save_api_config\\\"))\\n1185 | \\t\\t\\t\\t}\\n1186 | \\t\\t\\t}\\n1187 | \\t\\t\\tbreak\\n1188 | \\t\\tcase \\\"upsertApiConfiguration\\\":\\n1189 | \\t\\t\\tif (message.text && message.apiConfiguration) {\\n1190 | \\t\\t\\t\\tawait provider.upsertProviderProfile(message.text, message.apiConfiguration)\\n1191 | \\t\\t\\t}\\n1192 | \\t\\t\\tbreak\\n1193 | \\t\\tcase \\\"renameApiConfiguration\\\":\\n1194 | \\t\\t\\tif (message.values && message.apiConfiguration) {\\n1195 | \\t\\t\\t\\ttry {\\n1196 | \\t\\t\\t\\t\\tconst { oldName, newName } = message.values\\n1197 | \\n1198 | \\t\\t\\t\\t\\tif (oldName === newName) {\\n1199 | \\t\\t\\t\\t\\t\\tbreak\\n1200 | \\t\\t\\t\\t\\t}\\n1201 | \\n1202 | \\t\\t\\t\\t\\t// Load the old configuration to get its ID.\\n1203 | \\t\\t\\t\\t\\tconst { id } = await provider.providerSettingsManager.getProfile({ name: oldName })\\n1204 | \\n1205 | \\t\\t\\t\\t\\t// Create a new configuration with the new name and old ID.\\n1206 | \\t\\t\\t\\t\\tawait provider.providerSettingsManager.saveConfig(newName, { ...message.apiConfiguration, id })\\n1207 | \\n1208 | \\t\\t\\t\\t\\t// Delete the old configuration.\\n1209 | \\t\\t\\t\\t\\tawait provider.providerSettingsManager.deleteConfig(oldName)\\n1210 | \\n1211 | \\t\\t\\t\\t\\t// Re-activate to update the global settings related to the\\n1212 | \\t\\t\\t\\t\\t// currently activated provider profile.\\n1213 | \\t\\t\\t\\t\\tawait provider.activateProviderProfile({ name: newName })\\n1214 | \\t\\t\\t\\t} catch (error) {\\n1215 | \\t\\t\\t\\t\\tprovider.log(\\n1216 | \\t\\t\\t\\t\\t\\t`Error rename api configuration: ${JSON.stringify(error, Object.getOwnPropertyNames(error), 2)}`,\\n1217 | \\t\\t\\t\\t\\t)\\n1218 | \\n1219 | \\t\\t\\t\\t\\tvscode.window.showErrorMessage(t(\\\"common:errors.rename_api_config\\\"))\\n1220 | \\t\\t\\t\\t}\\n1221 | \\t\\t\\t}\\n1222 | \\t\\t\\tbreak\\n1223 | \\t\\tcase \\\"loadApiConfiguration\\\":\\n1224 | \\t\\t\\tif (message.text) {\\n1225 | \\t\\t\\t\\ttry {\\n1226 | \\t\\t\\t\\t\\tawait provider.activateProviderProfile({ name: message.text })\\n1227 | \\t\\t\\t\\t} catch (error) {\\n1228 | \\t\\t\\t\\t\\tprovider.log(\\n1229 | \\t\\t\\t\\t\\t\\t`Error load api configuration: ${JSON.stringify(error, Object.getOwnPropertyNames(error), 2)}`,\\n1230 | \\t\\t\\t\\t\\t)\\n1231 | \\t\\t\\t\\t\\tvscode.window.showErrorMessage(t(\\\"common:errors.load_api_config\\\"))\\n1232 | \\t\\t\\t\\t}\\n1233 | \\t\\t\\t}\\n1234 | \\t\\t\\tbreak\\n1235 | \\t\\tcase \\\"loadApiConfigurationById\\\":\\n1236 | \\t\\t\\tif (message.text) {\\n1237 | \\t\\t\\t\\ttry {\\n1238 | \\t\\t\\t\\t\\tawait provider.activateProviderProfile({ id: message.text })\\n1239 | \\t\\t\\t\\t} catch (error) {\\n1240 | \\t\\t\\t\\t\\tprovider.log(\\n1241 | \\t\\t\\t\\t\\t\\t`Error load api configuration by ID: ${JSON.stringify(error, Object.getOwnPropertyNames(error), 2)}`,\\n1242 | \\t\\t\\t\\t\\t)\\n1243 | \\t\\t\\t\\t\\tvscode.window.showErrorMessage(t(\\\"common:errors.load_api_config\\\"))\\n1244 | \\t\\t\\t\\t}\\n1245 | \\t\\t\\t}\\n1246 | \\t\\t\\tbreak\\n1247 | \\t\\tcase \\\"deleteApiConfiguration\\\":\\n1248 | \\t\\t\\tif (message.text) {\\n1249 | \\t\\t\\t\\tconst answer = await vscode.window.showInformationMessage(\\n1250 | \\t\\t\\t\\t\\tt(\\\"common:confirmation.delete_config_profile\\\"),\\n1251 | \\t\\t\\t\\t\\t{ modal: true },\\n1252 | \\t\\t\\t\\t\\tt(\\\"common:answers.yes\\\"),\\n1253 | \\t\\t\\t\\t)\\n1254 | \\n1255 | \\t\\t\\t\\tif (answer !== t(\\\"common:answers.yes\\\")) {\\n1256 | \\t\\t\\t\\t\\tbreak\\n1257 | \\t\\t\\t\\t}\\n1258 | \\n1259 | \\t\\t\\t\\tconst oldName = message.text\\n1260 | \\n1261 | \\t\\t\\t\\tconst newName = (await provider.providerSettingsManager.listConfig()).filter(\\n1262 | \\t\\t\\t\\t\\t(c) => c.name !== oldName,\\n1263 | \\t\\t\\t\\t)[0]?.name\\n1264 | \\n1265 | \\t\\t\\t\\tif (!newName) {\\n1266 | \\t\\t\\t\\t\\tvscode.window.showErrorMessage(t(\\\"common:errors.delete_api_config\\\"))\\n1267 | \\t\\t\\t\\t\\treturn\\n1268 | \\t\\t\\t\\t}\\n1269 | \\n1270 | \\t\\t\\t\\ttry {\\n1271 | \\t\\t\\t\\t\\tawait provider.providerSettingsManager.deleteConfig(oldName)\\n1272 | \\t\\t\\t\\t\\tawait provider.activateProviderProfile({ name: newName })\\n1273 | \\t\\t\\t\\t} catch (error) {\\n1274 | \\t\\t\\t\\t\\tprovider.log(\\n1275 | \\t\\t\\t\\t\\t\\t`Error delete api configuration: ${JSON.stringify(error, Object.getOwnPropertyNames(error), 2)}`,\\n1276 | \\t\\t\\t\\t\\t)\\n1277 | \\n1278 | \\t\\t\\t\\t\\tvscode.window.showErrorMessage(t(\\\"common:errors.delete_api_config\\\"))\\n1279 | \\t\\t\\t\\t}\\n1280 | \\t\\t\\t}\\n1281 | \\t\\t\\tbreak\\n1282 | \\t\\tcase \\\"getListApiConfiguration\\\":\\n1283 | \\t\\t\\ttry {\\n1284 | \\t\\t\\t\\tconst listApiConfig = await provider.providerSettingsManager.listConfig()\\n1285 | \\t\\t\\t\\tawait updateGlobalState(\\\"listApiConfigMeta\\\", listApiConfig)\\n1286 | \\t\\t\\t\\tprovider.postMessageToWebview({ type: \\\"listApiConfig\\\", listApiConfig })\\n1287 | \\t\\t\\t} catch (error) {\\n1288 | \\t\\t\\t\\tprovider.log(\\n1289 | \\t\\t\\t\\t\\t`Error get list api configuration: ${JSON.stringify(error, Object.getOwnPropertyNames(error), 2)}`,\\n1290 | \\t\\t\\t\\t)\\n1291 | \\t\\t\\t\\tvscode.window.showErrorMessage(t(\\\"common:errors.list_api_config\\\"))\\n1292 | \\t\\t\\t}\\n1293 | \\t\\t\\tbreak\\n1294 | \\t\\tcase \\\"updateExperimental\\\": {\\n1295 | \\t\\t\\tif (!message.values) {\\n1296 | \\t\\t\\t\\tbreak\\n1297 | \\t\\t\\t}\\n1298 | \\n1299 | \\t\\t\\tconst updatedExperiments = {\\n1300 | \\t\\t\\t\\t...(getGlobalState(\\\"experiments\\\") ?? experimentDefault),\\n1301 | \\t\\t\\t\\t...message.values,\\n1302 | \\t\\t\\t}\\n1303 | \\n1304 | \\t\\t\\tawait updateGlobalState(\\\"experiments\\\", updatedExperiments)\\n1305 | \\n1306 | \\t\\t\\tawait provider.postStateToWebview()\\n1307 | \\t\\t\\tbreak\\n1308 | \\t\\t}\\n1309 | \\t\\tcase \\\"updateMcpTimeout\\\":\\n1310 | \\t\\t\\tif (message.serverName && typeof message.timeout === \\\"number\\\") {\\n1311 | \\t\\t\\t\\ttry {\\n1312 | \\t\\t\\t\\t\\tawait provider\\n1313 | \\t\\t\\t\\t\\t\\t.getMcpHub()\\n1314 | \\t\\t\\t\\t\\t\\t?.updateServerTimeout(\\n1315 | \\t\\t\\t\\t\\t\\t\\tmessage.serverName,\\n1316 | \\t\\t\\t\\t\\t\\t\\tmessage.timeout,\\n1317 | \\t\\t\\t\\t\\t\\t\\tmessage.source as \\\"global\\\" | \\\"project\\\",\\n1318 | \\t\\t\\t\\t\\t\\t)\\n1319 | \\t\\t\\t\\t} catch (error) {\\n1320 | \\t\\t\\t\\t\\tprovider.log(\\n1321 | \\t\\t\\t\\t\\t\\t`Failed to update timeout for ${message.serverName}: ${JSON.stringify(error, Object.getOwnPropertyNames(error), 2)}`,\\n1322 | \\t\\t\\t\\t\\t)\\n1323 | \\t\\t\\t\\t\\tvscode.window.showErrorMessage(t(\\\"common:errors.update_server_timeout\\\"))\\n1324 | \\t\\t\\t\\t}\\n1325 | \\t\\t\\t}\\n1326 | \\t\\t\\tbreak\\n1327 | \\t\\tcase \\\"updateCustomMode\\\":\\n1328 | \\t\\t\\tif (message.modeConfig) {\\n1329 | \\t\\t\\t\\tawait provider.customModesManager.updateCustomMode(message.modeConfig.slug, message.modeConfig)\\n1330 | \\t\\t\\t\\t// Update state after saving the mode\\n1331 | \\t\\t\\t\\tconst customModes = await provider.customModesManager.getCustomModes()\\n1332 | \\t\\t\\t\\tawait updateGlobalState(\\\"customModes\\\", customModes)\\n1333 | \\t\\t\\t\\tawait updateGlobalState(\\\"mode\\\", message.modeConfig.slug)\\n1334 | \\t\\t\\t\\tawait provider.postStateToWebview()\\n1335 | \\t\\t\\t}\\n1336 | \\t\\t\\tbreak\\n1337 | \\t\\tcase \\\"deleteCustomMode\\\":\\n1338 | \\t\\t\\tif (message.slug) {\\n1339 | \\t\\t\\t\\tconst answer = await vscode.window.showInformationMessage(\\n1340 | \\t\\t\\t\\t\\tt(\\\"common:confirmation.delete_custom_mode\\\"),\\n1341 | \\t\\t\\t\\t\\t{ modal: true },\\n1342 | \\t\\t\\t\\t\\tt(\\\"common:answers.yes\\\"),\\n1343 | \\t\\t\\t\\t)\\n1344 | \\n1345 | \\t\\t\\t\\tif (answer !== t(\\\"common:answers.yes\\\")) {\\n1346 | \\t\\t\\t\\t\\tbreak\\n1347 | \\t\\t\\t\\t}\\n1348 | \\n1349 | \\t\\t\\t\\tawait provider.customModesManager.deleteCustomMode(message.slug)\\n1350 | \\t\\t\\t\\t// Switch back to default mode after deletion\\n1351 | \\t\\t\\t\\tawait updateGlobalState(\\\"mode\\\", defaultModeSlug)\\n1352 | \\t\\t\\t\\tawait provider.postStateToWebview()\\n1353 | \\t\\t\\t}\\n1354 | \\t\\t\\tbreak\\n1355 | \\t\\tcase \\\"humanRelayResponse\\\":\\n1356 | \\t\\t\\tif (message.requestId && message.text) {\\n1357 | \\t\\t\\t\\tvscode.commands.executeCommand(getCommand(\\\"handleHumanRelayResponse\\\"), {\\n1358 | \\t\\t\\t\\t\\trequestId: message.requestId,\\n1359 | \\t\\t\\t\\t\\ttext: message.text,\\n1360 | \\t\\t\\t\\t\\tcancelled: false,\\n1361 | \\t\\t\\t\\t})\\n1362 | \\t\\t\\t}\\n1363 | \\t\\t\\tbreak\\n1364 | \\n1365 | \\t\\tcase \\\"humanRelayCancel\\\":\\n1366 | \\t\\t\\tif (message.requestId) {\\n1367 | \\t\\t\\t\\tvscode.commands.executeCommand(getCommand(\\\"handleHumanRelayResponse\\\"), {\\n1368 | \\t\\t\\t\\t\\trequestId: message.requestId,\\n1369 | \\t\\t\\t\\t\\tcancelled: true,\\n1370 | \\t\\t\\t\\t})\\n1371 | \\t\\t\\t}\\n1372 | \\t\\t\\tbreak\\n1373 | \\n1374 | \\t\\tcase \\\"telemetrySetting\\\": {\\n1375 | \\t\\t\\tconst telemetrySetting = message.text as TelemetrySetting\\n1376 | \\t\\t\\tawait updateGlobalState(\\\"telemetrySetting\\\", telemetrySetting)\\n1377 | \\t\\t\\tconst isOptedIn = telemetrySetting === \\\"enabled\\\"\\n1378 | \\t\\t\\tTelemetryService.instance.updateTelemetryState(isOptedIn)\\n1379 | \\t\\t\\tawait provider.postStateToWebview()\\n1380 | \\t\\t\\tbreak\\n1381 | \\t\\t}\\n1382 | \\t\\tcase \\\"accountButtonClicked\\\": {\\n1383 | \\t\\t\\t// Navigate to the account tab.\\n1384 | \\t\\t\\tprovider.postMessageToWebview({ type: \\\"action\\\", action: \\\"accountButtonClicked\\\" })\\n1385 | \\t\\t\\tbreak\\n1386 | \\t\\t}\\n1387 | \\t\\tcase \\\"rooCloudSignIn\\\": {\\n1388 | \\t\\t\\ttry {\\n1389 | \\t\\t\\t\\tTelemetryService.instance.captureEvent(TelemetryEventName.AUTHENTICATION_INITIATED)\\n1390 | \\t\\t\\t\\tawait CloudService.instance.login()\\n1391 | \\t\\t\\t} catch (error) {\\n1392 | \\t\\t\\t\\tprovider.log(`AuthService#login failed: ${error}`)\\n1393 | \\t\\t\\t\\tvscode.window.showErrorMessage(\\\"Sign in failed.\\\")\\n1394 | \\t\\t\\t}\\n1395 | \\n1396 | \\t\\t\\tbreak\\n1397 | \\t\\t}\\n1398 | \\t\\tcase \\\"rooCloudSignOut\\\": {\\n1399 | \\t\\t\\ttry {\\n1400 | \\t\\t\\t\\tawait CloudService.instance.logout()\\n1401 | \\t\\t\\t\\tawait provider.postStateToWebview()\\n1402 | \\t\\t\\t\\tprovider.postMessageToWebview({ type: \\\"authenticatedUser\\\", userInfo: undefined })\\n1403 | \\t\\t\\t} catch (error) {\\n1404 | \\t\\t\\t\\tprovider.log(`AuthService#logout failed: ${error}`)\\n1405 | \\t\\t\\t\\tvscode.window.showErrorMessage(\\\"Sign out failed.\\\")\\n1406 | \\t\\t\\t}\\n1407 | \\n1408 | \\t\\t\\tbreak\\n1409 | \\t\\t}\\n1410 | \\t\\tcase \\\"codebaseIndexConfig\\\": {\\n1411 | \\t\\t\\tconst codebaseIndexConfig = message.values ?? {\\n1412 | \\t\\t\\t\\tcodebaseIndexEnabled: false,\\n1413 | \\t\\t\\t\\tcodebaseIndexQdrantUrl: \\\"http://localhost:6333\\\",\\n1414 | \\t\\t\\t\\tcodebaseIndexEmbedderProvider: \\\"openai\\\",\\n1415 | \\t\\t\\t\\tcodebaseIndexEmbedderBaseUrl: \\\"\\\",\\n1416 | \\t\\t\\t\\tcodebaseIndexEmbedderModelId: \\\"\\\",\\n1417 | \\t\\t\\t}\\n1418 | \\t\\t\\tawait updateGlobalState(\\\"codebaseIndexConfig\\\", codebaseIndexConfig)\\n1419 | \\n1420 | \\t\\t\\ttry {\\n1421 | \\t\\t\\t\\tif (provider.codeIndexManager) {\\n1422 | \\t\\t\\t\\t\\tawait provider.codeIndexManager.handleExternalSettingsChange()\\n1423 | \\n1424 | \\t\\t\\t\\t\\t// If now configured and enabled, start indexing automatically\\n1425 | \\t\\t\\t\\t\\tif (provider.codeIndexManager.isFeatureEnabled && provider.codeIndexManager.isFeatureConfigured) {\\n1426 | \\t\\t\\t\\t\\t\\tif (!provider.codeIndexManager.isInitialized) {\\n1427 | \\t\\t\\t\\t\\t\\t\\tawait provider.codeIndexManager.initialize(provider.contextProxy)\\n1428 | \\t\\t\\t\\t\\t\\t}\\n1429 | \\t\\t\\t\\t\\t\\t// Start indexing in background (no await)\\n1430 | \\t\\t\\t\\t\\t\\tprovider.codeIndexManager.startIndexing()\\n1431 | \\t\\t\\t\\t\\t}\\n1432 | \\t\\t\\t\\t}\\n1433 | \\t\\t\\t} catch (error) {\\n1434 | \\t\\t\\t\\tprovider.log(\\n1435 | \\t\\t\\t\\t\\t`[CodeIndexManager] Error during background CodeIndexManager configuration/indexing: ${error.message || error}`,\\n1436 | \\t\\t\\t\\t)\\n1437 | \\t\\t\\t}\\n1438 | \\n1439 | \\t\\t\\tawait provider.postStateToWebview()\\n1440 | \\t\\t\\tbreak\\n1441 | \\t\\t}\\n1442 | \\t\\tcase \\\"requestIndexingStatus\\\": {\\n1443 | \\t\\t\\tconst status = provider.codeIndexManager!.getCurrentStatus()\\n1444 | \\t\\t\\tprovider.postMessageToWebview({\\n1445 | \\t\\t\\t\\ttype: \\\"indexingStatusUpdate\\\",\\n1446 | \\t\\t\\t\\tvalues: status,\\n1447 | \\t\\t\\t})\\n1448 | \\t\\t\\tbreak\\n1449 | \\t\\t}\\n1450 | \\t\\tcase \\\"startIndexing\\\": {\\n1451 | \\t\\t\\ttry {\\n1452 | \\t\\t\\t\\tconst manager = provider.codeIndexManager!\\n1453 | \\t\\t\\t\\tif (manager.isFeatureEnabled && manager.isFeatureConfigured) {\\n1454 | \\t\\t\\t\\t\\tif (!manager.isInitialized) {\\n1455 | \\t\\t\\t\\t\\t\\tawait manager.initialize(provider.contextProxy)\\n1456 | \\t\\t\\t\\t\\t}\\n1457 | \\n1458 | \\t\\t\\t\\t\\tmanager.startIndexing()\\n1459 | \\t\\t\\t\\t}\\n1460 | \\t\\t\\t} catch (error) {\\n1461 | \\t\\t\\t\\tprovider.log(`Error starting indexing: ${error instanceof Error ? error.message : String(error)}`)\\n1462 | \\t\\t\\t}\\n1463 | \\t\\t\\tbreak\\n1464 | \\t\\t}\\n1465 | \\t\\tcase \\\"clearIndexData\\\": {\\n1466 | \\t\\t\\ttry {\\n1467 | \\t\\t\\t\\tconst manager = provider.codeIndexManager!\\n1468 | \\t\\t\\t\\tawait manager.clearIndexData()\\n1469 | \\t\\t\\t\\tprovider.postMessageToWebview({ type: \\\"indexCleared\\\", values: { success: true } })\\n1470 | \\t\\t\\t} catch (error) {\\n1471 | \\t\\t\\t\\tprovider.log(`Error clearing index data: ${error instanceof Error ? error.message : String(error)}`)\\n1472 | \\t\\t\\t\\tprovider.postMessageToWebview({\\n1473 | \\t\\t\\t\\t\\ttype: \\\"indexCleared\\\",\\n1474 | \\t\\t\\t\\t\\tvalues: {\\n1475 | \\t\\t\\t\\t\\t\\tsuccess: false,\\n1476 | \\t\\t\\t\\t\\t\\terror: error instanceof Error ? error.message : String(error),\\n1477 | \\t\\t\\t\\t\\t},\\n1478 | \\t\\t\\t\\t})\\n1479 | \\t\\t\\t}\\n1480 | \\t\\t\\tbreak\\n1481 | \\t\\t}\\n1482 | \\t\\tcase \\\"filterMarketplaceItems\\\": {\\n1483 | \\t\\t\\t// Check if marketplace is enabled before making API calls\\n1484 | \\t\\t\\tconst { experiments } = await provider.getState()\\n1485 | \\t\\t\\tif (!experiments.marketplace) {\\n1486 | \\t\\t\\t\\tconsole.log(\\\"Marketplace: Feature disabled, skipping API call\\\")\\n1487 | \\t\\t\\t\\tbreak\\n1488 | \\t\\t\\t}\\n1489 | \\n1490 | \\t\\t\\tif (marketplaceManager && message.filters) {\\n1491 | \\t\\t\\t\\ttry {\\n1492 | \\t\\t\\t\\t\\tawait marketplaceManager.updateWithFilteredItems({\\n1493 | \\t\\t\\t\\t\\t\\ttype: message.filters.type as MarketplaceItemType | undefined,\\n1494 | \\t\\t\\t\\t\\t\\tsearch: message.filters.search,\\n1495 | \\t\\t\\t\\t\\t\\ttags: message.filters.tags,\\n1496 | \\t\\t\\t\\t\\t})\\n1497 | \\t\\t\\t\\t\\tawait provider.postStateToWebview()\\n1498 | \\t\\t\\t\\t} catch (error) {\\n1499 | \\t\\t\\t\\t\\tconsole.error(\\\"Marketplace: Error filtering items:\\\", error)\\n1500 | \\t\\t\\t\\t\\tvscode.window.showErrorMessage(\\\"Failed to filter marketplace items\\\")\\n1501 | \\t\\t\\t\\t}\\n1502 | \\t\\t\\t}\\n1503 | \\t\\t\\tbreak\\n1504 | \\t\\t}\\n1505 | \\n1506 | \\t\\tcase \\\"installMarketplaceItem\\\": {\\n1507 | \\t\\t\\t// Check if marketplace is enabled before installing\\n1508 | \\t\\t\\tconst { experiments } = await provider.getState()\\n1509 | \\t\\t\\tif (!experiments.marketplace) {\\n1510 | \\t\\t\\t\\tconsole.log(\\\"Marketplace: Feature disabled, skipping installation\\\")\\n1511 | \\t\\t\\t\\tbreak\\n1512 | \\t\\t\\t}\\n1513 | \\n1514 | \\t\\t\\tif (marketplaceManager && message.mpItem && message.mpInstallOptions) {\\n1515 | \\t\\t\\t\\ttry {\\n1516 | \\t\\t\\t\\t\\tconst configFilePath = await marketplaceManager.installMarketplaceItem(\\n1517 | \\t\\t\\t\\t\\t\\tmessage.mpItem,\\n1518 | \\t\\t\\t\\t\\t\\tmessage.mpInstallOptions,\\n1519 | \\t\\t\\t\\t\\t)\\n1520 | \\t\\t\\t\\t\\tawait provider.postStateToWebview()\\n1521 | \\t\\t\\t\\t\\tconsole.log(`Marketplace item installed and config file opened: ${configFilePath}`)\\n1522 | \\t\\t\\t\\t\\t// Send success message to webview\\n1523 | \\t\\t\\t\\t\\tprovider.postMessageToWebview({\\n1524 | \\t\\t\\t\\t\\t\\ttype: \\\"marketplaceInstallResult\\\",\\n1525 | \\t\\t\\t\\t\\t\\tsuccess: true,\\n1526 | \\t\\t\\t\\t\\t\\tslug: message.mpItem.id,\\n1527 | \\t\\t\\t\\t\\t})\\n1528 | \\t\\t\\t\\t} catch (error) {\\n1529 | \\t\\t\\t\\t\\tconsole.error(`Error installing marketplace item: ${error}`)\\n1530 | \\t\\t\\t\\t\\t// Send error message to webview\\n1531 | \\t\\t\\t\\t\\tprovider.postMessageToWebview({\\n1532 | \\t\\t\\t\\t\\t\\ttype: \\\"marketplaceInstallResult\\\",\\n1533 | \\t\\t\\t\\t\\t\\tsuccess: false,\\n1534 | \\t\\t\\t\\t\\t\\terror: error instanceof Error ? error.message : String(error),\\n1535 | \\t\\t\\t\\t\\t\\tslug: message.mpItem.id,\\n1536 | \\t\\t\\t\\t\\t})\\n1537 | \\t\\t\\t\\t}\\n1538 | \\t\\t\\t}\\n1539 | \\t\\t\\tbreak\\n1540 | \\t\\t}\\n1541 | \\n1542 | \\t\\tcase \\\"removeInstalledMarketplaceItem\\\": {\\n1543 | \\t\\t\\t// Check if marketplace is enabled before removing\\n1544 | \\t\\t\\tconst { experiments } = await provider.getState()\\n1545 | \\t\\t\\tif (!experiments.marketplace) {\\n1546 | \\t\\t\\t\\tconsole.log(\\\"Marketplace: Feature disabled, skipping removal\\\")\\n1547 | \\t\\t\\t\\tbreak\\n1548 | \\t\\t\\t}\\n1549 | \\n1550 | \\t\\t\\tif (marketplaceManager && message.mpItem && message.mpInstallOptions) {\\n1551 | \\t\\t\\t\\ttry {\\n1552 | \\t\\t\\t\\t\\tawait marketplaceManager.removeInstalledMarketplaceItem(message.mpItem, message.mpInstallOptions)\\n1553 | \\t\\t\\t\\t\\tawait provider.postStateToWebview()\\n1554 | \\t\\t\\t\\t} catch (error) {\\n1555 | \\t\\t\\t\\t\\tconsole.error(`Error removing marketplace item: ${error}`)\\n1556 | \\t\\t\\t\\t}\\n1557 | \\t\\t\\t}\\n1558 | \\t\\t\\tbreak\\n1559 | \\t\\t}\\n1560 | \\n1561 | \\t\\tcase \\\"installMarketplaceItemWithParameters\\\": {\\n1562 | \\t\\t\\t// Check if marketplace is enabled before installing with parameters\\n1563 | \\t\\t\\tconst { experiments } = await provider.getState()\\n1564 | \\t\\t\\tif (!experiments.marketplace) {\\n1565 | \\t\\t\\t\\tconsole.log(\\\"Marketplace: Feature disabled, skipping installation with parameters\\\")\\n1566 | \\t\\t\\t\\tbreak\\n1567 | \\t\\t\\t}\\n1568 | \\n1569 | \\t\\t\\tif (marketplaceManager && message.payload && \\\"item\\\" in message.payload && \\\"parameters\\\" in message.payload) {\\n1570 | \\t\\t\\t\\ttry {\\n1571 | \\t\\t\\t\\t\\tconst configFilePath = await marketplaceManager.installMarketplaceItem(message.payload.item, {\\n1572 | \\t\\t\\t\\t\\t\\tparameters: message.payload.parameters,\\n1573 | \\t\\t\\t\\t\\t})\\n1574 | \\t\\t\\t\\t\\tawait provider.postStateToWebview()\\n1575 | \\t\\t\\t\\t\\tconsole.log(`Marketplace item with parameters installed and config file opened: ${configFilePath}`)\\n1576 | \\t\\t\\t\\t} catch (error) {\\n1577 | \\t\\t\\t\\t\\tconsole.error(`Error installing marketplace item with parameters: ${error}`)\\n1578 | \\t\\t\\t\\t\\tvscode.window.showErrorMessage(\\n1579 | \\t\\t\\t\\t\\t\\t`Failed to install marketplace item: ${error instanceof Error ? error.message : String(error)}`,\\n1580 | \\t\\t\\t\\t\\t)\\n1581 | \\t\\t\\t\\t}\\n1582 | \\t\\t\\t}\\n1583 | \\t\\t\\tbreak\\n1584 | \\t\\t}\\n1585 | \\n1586 | \\t\\tcase \\\"switchTab\\\": {\\n1587 | \\t\\t\\tif (message.tab) {\\n1588 | \\t\\t\\t\\t// Send a message to the webview to switch to the specified tab\\n1589 | \\t\\t\\t\\tawait provider.postMessageToWebview({ type: \\\"action\\\", action: \\\"switchTab\\\", tab: message.tab })\\n1590 | \\t\\t\\t}\\n1591 | \\t\\t\\tbreak\\n1592 | \\t\\t}\\n1593 | \\t}\\n1594 | }\\n</content>\\n</file>\\n</files>\\n\\nLoading...\"}"
}
[2025-06-13T05:00:01.323Z] {
  "ts": 1749790800308,
  "type": "say",
  "say": "text",
  "text": "Now I can see the authentication implementation! I found the Clerk authentication callback handler and the CloudService integration. Let me search for more authentication-related files to understand the token refresh mechanism.",
  "partial": false
}
[2025-06-13T05:00:01.942Z] {
  "ts": 1749790801323,
  "type": "ask",
  "ask": "tool",
  "text": "{\"tool\":\"searchFiles\",\"path\":\"Roo-Code-Cloud-Agents\",\"regex\":\"CloudService|refresh|token|auth\",\"filePattern\":\"*.ts\",\"content\":\"Showing first 300 of 300+ results. Use a more specific search if necessary.\\n\\n# apps/web-evals/src/lib/formatters.ts\\n 30 | \\n 31 | export const formatTokens = (tokens: number) => {\\n 32 | \\tif (tokens < 1000) {\\n 33 | \\t\\treturn tokens.toString()\\n 34 | \\t}\\n 35 | \\n 36 | \\tif (tokens < 1000000) {\\n 37 | \\t\\treturn `${(tokens / 1000).toFixed(1)}k`\\n 38 | \\t}\\n 39 | \\n 40 | \\tif (tokens < 1000000000) {\\n 41 | \\t\\treturn `${(tokens / 1000000).toFixed(1)}M`\\n 42 | \\t}\\n 43 | \\n 44 | \\treturn `${(tokens / 1000000000).toFixed(1)}B`\\n 45 | }\\n----\\n\\n# src/services/ripgrep/index.ts\\n101 | \\t\\tconst rgProcess = childProcess.spawn(bin, args)\\n102 | \\t\\t// cross-platform alternative to head, which is ripgrep author's recommendation for limiting output.\\n103 | \\t\\tconst rl = readline.createInterface({\\n----\\n\\n# apps/web-roo-code/src/lib/index.ts\\n  3 | export * from \\\"./format-score\\\"\\n  4 | export * from \\\"./format-tokens\\\"\\n----\\n\\n# apps/web-roo-code/src/lib/format-tokens.ts\\n  1 | export const formatTokens = (tokens: number, decimals = 0) => {\\n  2 | \\tif (tokens < 1000) {\\n  3 | \\t\\treturn tokens.toString()\\n  4 | \\t}\\n  5 | \\n  6 | \\tif (tokens < 1000000) {\\n  7 | \\t\\treturn `${(tokens / 1000).toFixed(decimals)}K`\\n  8 | \\t}\\n  9 | \\n 10 | \\tif (tokens < 1000000000) {\\n 11 | \\t\\treturn `${(tokens / 1000000).toFixed(decimals)}M`\\n 12 | \\t}\\n 13 | \\n 14 | \\treturn `${(tokens / 1000000000).toFixed(decimals)}B`\\n 15 | }\\n----\\n\\n# apps/web-roo-code/src/lib/hooks/use-open-router-models.ts\\n 19 | \\t\\t.object({\\n 20 | \\t\\t\\tmax_completion_tokens: z.number().nullish(),\\n 21 | \\t\\t})\\n----\\n 53 | \\t\\t\\tmodelInfo: {\\n 54 | \\t\\t\\t\\tmaxTokens: rawModel.top_provider?.max_completion_tokens ?? undefined,\\n 55 | \\t\\t\\t\\tcontextWindow: rawModel.context_length,\\n----\\n\\n# src/services/mcp/__tests__/McpHub.test.ts\\n 50 | \\t\\t\\tscheme: \\\"file\\\",\\n 51 | \\t\\t\\tauthority: \\\"\\\",\\n 52 | \\t\\t\\tpath: \\\"/test/path\\\",\\n----\\n\\n# src/services/mcp/McpHub.ts\\n1092 | \\n1093 | \\tpublic async refreshAllConnections(): Promise<void> {\\n1094 | \\t\\tif (this.isConnecting) {\\n1095 | \\t\\t\\tvscode.window.showInformationMessage(t(\\\"mcp:info.already_refreshing\\\"))\\n1096 | \\t\\t\\treturn\\n----\\n1099 | \\t\\tthis.isConnecting = true\\n1100 | \\t\\tvscode.window.showInformationMessage(t(\\\"mcp:info.refreshing_all\\\"))\\n1101 | \\n----\\n1151 | \\n1152 | \\t\\t\\tvscode.window.showInformationMessage(t(\\\"mcp:info.all_refreshed\\\"))\\n1153 | \\t\\t} catch (error) {\\n1154 | \\t\\t\\tthis.showErrorMessage(\\\"Failed to refresh MCP servers\\\", error)\\n1155 | \\t\\t} finally {\\n----\\n1244 | \\n1245 | \\t\\t\\t\\t\\t// Only refresh capabilities if connected\\n1246 | \\t\\t\\t\\t\\tif (connection.server.status === \\\"connected\\\") {\\n----\\n1254 | \\t\\t\\t\\t} catch (error) {\\n1255 | \\t\\t\\t\\t\\tconsole.error(`Failed to refresh capabilities for ${serverName}:`, error)\\n1256 | \\t\\t\\t\\t}\\n----\\n\\n# src/services/code-index/embedders/openai.ts\\n 52 | \\t\\t\\t\\t\\tconsole.warn(\\n 53 | \\t\\t\\t\\t\\t\\t`Text at index ${i} exceeds maximum token limit (${itemTokens} > ${MAX_ITEM_TOKENS}). Skipping.`,\\n 54 | \\t\\t\\t\\t\\t)\\n----\\n108 | \\t\\t\\t\\t\\tusage: {\\n109 | \\t\\t\\t\\t\\t\\tpromptTokens: response.usage?.prompt_tokens || 0,\\n110 | \\t\\t\\t\\t\\t\\ttotalTokens: response.usage?.total_tokens || 0,\\n111 | \\t\\t\\t\\t\\t},\\n----\\n\\n# src/services/code-index/embedders/openai-compatible.ts\\n 18 | \\tusage?: {\\n 19 | \\t\\tprompt_tokens?: number\\n 20 | \\t\\ttotal_tokens?: number\\n 21 | \\t}\\n----\\n 34 | \\t * @param baseUrl The base URL for the OpenAI-compatible API endpoint\\n 35 | \\t * @param apiKey The API key for authentication\\n 36 | \\t * @param modelId Optional model identifier (defaults to \\\"text-embedding-3-small\\\")\\n----\\n 75 | \\t\\t\\t\\t\\tconsole.warn(\\n 76 | \\t\\t\\t\\t\\t\\t`Text at index ${i} exceeds maximum token limit (${itemTokens} > ${MAX_ITEM_TOKENS}). Skipping.`,\\n 77 | \\t\\t\\t\\t\\t)\\n----\\n156 | \\t\\t\\t\\t\\tusage: {\\n157 | \\t\\t\\t\\t\\t\\tpromptTokens: response.usage?.prompt_tokens || 0,\\n158 | \\t\\t\\t\\t\\t\\ttotalTokens: response.usage?.total_tokens || 0,\\n159 | \\t\\t\\t\\t\\t},\\n----\\n\\n# src/services/code-index/embedders/__tests__/openai-compatible.spec.ts\\n103 | \\t\\t\\t\\tdata: [{ embedding: [0.1, 0.2, 0.3] }],\\n104 | \\t\\t\\t\\tusage: { prompt_tokens: 10, total_tokens: 15 },\\n105 | \\t\\t\\t}\\n----\\n124 | \\t\\t\\t\\tdata: [{ embedding: [0.1, 0.2, 0.3] }, { embedding: [0.4, 0.5, 0.6] }],\\n125 | \\t\\t\\t\\tusage: { prompt_tokens: 20, total_tokens: 30 },\\n126 | \\t\\t\\t}\\n----\\n149 | \\t\\t\\t\\tdata: [{ embedding: [0.1, 0.2, 0.3] }],\\n150 | \\t\\t\\t\\tusage: { prompt_tokens: 10, total_tokens: 15 },\\n151 | \\t\\t\\t}\\n----\\n194 | \\t\\t\\t\\t\\tdata: [{ embedding: base64String }], // Base64 string instead of array\\n195 | \\t\\t\\t\\t\\tusage: { prompt_tokens: 10, total_tokens: 15 },\\n196 | \\t\\t\\t\\t}\\n----\\n226 | \\t\\t\\t\\t\\tdata: [{ embedding: base64String1 }, { embedding: base64String2 }],\\n227 | \\t\\t\\t\\t\\tusage: { prompt_tokens: 20, total_tokens: 30 },\\n228 | \\t\\t\\t\\t}\\n----\\n253 | \\t\\t\\t\\t\\t],\\n254 | \\t\\t\\t\\t\\tusage: { prompt_tokens: 20, total_tokens: 30 },\\n255 | \\t\\t\\t\\t}\\n----\\n270 | \\t\\t/**\\n271 | \\t\\t * Test batching logic when texts exceed token limits\\n272 | \\t\\t */\\n----\\n283 | \\t\\t\\t\\t\\t],\\n284 | \\t\\t\\t\\t\\tusage: { prompt_tokens: 10, total_tokens: 15 },\\n285 | \\t\\t\\t\\t})\\n----\\n299 | \\t\\t\\t\\t\\tdata: [{ embedding: [0.1, 0.2, 0.3] }, { embedding: [0.4, 0.5, 0.6] }],\\n300 | \\t\\t\\t\\t\\tusage: { prompt_tokens: 10, total_tokens: 15 },\\n301 | \\t\\t\\t\\t}\\n----\\n306 | \\t\\t\\t\\t// Should warn about oversized text\\n307 | \\t\\t\\t\\texpect(console.warn).toHaveBeenCalledWith(expect.stringContaining(\\\"exceeds maximum token limit\\\"))\\n308 | \\n----\\n317 | \\t\\t\\t\\t\\tdata: [{ embedding: [0.1, 0.2, 0.3] }, { embedding: [0.4, 0.5, 0.6] }],\\n318 | \\t\\t\\t\\t\\tusage: { prompt_tokens: 10, total_tokens: 15 },\\n319 | \\t\\t\\t\\t})\\n----\\n354 | \\t\\t\\t\\t\\t\\tdata: [{ embedding: base64String }],\\n355 | \\t\\t\\t\\t\\t\\tusage: { prompt_tokens: 10, total_tokens: 15 },\\n356 | \\t\\t\\t\\t\\t})\\n----\\n375 | \\t\\t\\t\\tconst testTexts = [\\\"Hello world\\\"]\\n376 | \\t\\t\\t\\tconst authError = new Error(\\\"Unauthorized\\\")\\n377 | \\t\\t\\t\\t;(authError as any).status = 401\\n378 | \\n379 | \\t\\t\\t\\tmockEmbeddingsCreate.mockRejectedValue(authError)\\n380 | \\n----\\n452 | \\t\\t\\t\\t\\tdata: null,\\n453 | \\t\\t\\t\\t\\tusage: { prompt_tokens: 10, total_tokens: 15 },\\n454 | \\t\\t\\t\\t}\\n----\\n494 | \\t\\t\\t\\t\\tusage: {\\n495 | \\t\\t\\t\\t\\t\\tprompt_tokens: 2,\\n496 | \\t\\t\\t\\t\\t\\ttotal_tokens: 2,\\n497 | \\t\\t\\t\\t\\t},\\n----\\n\\n# apps/web-evals/src/hooks/use-run-status.ts\\n 16 | \\ttasks: (Task & { taskMetrics: TaskMetrics | null })[] | undefined\\n 17 | \\ttokenUsage: Map<number, TokenUsage & { duration?: number }>\\n 18 | \\tusageUpdatedAt: number | undefined\\n----\\n 24 | \\n 25 | \\tconst tokenUsage = useRef<Map<number, TokenUsage & { duration?: number }>>(new Map())\\n 26 | \\tconst startTimes = useRef<Map<number, number>>(new Map())\\n----\\n 79 | \\t\\t\\t\\tconst duration = startTime ? Date.now() - startTime : undefined\\n 80 | \\t\\t\\t\\ttokenUsage.current.set(taskId, { ...payload[1], duration })\\n 81 | \\t\\t\\t\\tsetUsageUpdatedAt(Date.now())\\n----\\n 97 | \\t\\ttasks,\\n 98 | \\t\\ttokenUsage: tokenUsage.current,\\n 99 | \\t\\tusageUpdatedAt,\\n----\\n\\n# src/activate/handleUri.ts\\n  2 | \\n  3 | import { CloudService } from \\\"@roo-code/cloud\\\"\\n  4 | \\n----\\n 37 | \\t\\t}\\n 38 | \\t\\tcase \\\"/auth/clerk/callback\\\": {\\n 39 | \\t\\t\\tconst code = query.get(\\\"code\\\")\\n 40 | \\t\\t\\tconst state = query.get(\\\"state\\\")\\n 41 | \\t\\t\\tawait CloudService.instance.handleAuthCallback(code, state)\\n 42 | \\t\\t\\tbreak\\n----\\n\\n# src/services/marketplace/types.ts\\n 20 | \\tdescription: string\\n 21 | \\tauthor?: string\\n 22 | \\tauthorUrl?: string\\n 23 | \\ttags?: string[]\\n----\\n 45 | \\tdescription: string\\n 46 | \\tauthor?: string\\n 47 | \\tauthorUrl?: string\\n 48 | \\turl: string // Required url field\\n----\\n 62 | \\ttype: MarketplaceItemType\\n 63 | \\tauthor?: string\\n 64 | \\tauthorUrl?: string\\n 65 | \\turl?: string // Optional - only MCPs have url\\n----\\n\\n# src/services/tree-sitter/__tests__/fixtures/sample-embedded_template.ts\\n 35 | <div class=\\\"container\\\">\\n 36 |   <% if user.authenticated? %>\\n 37 |     <h1>Welcome, <%= user.name %></h1>\\n----\\n\\n# src/services/tree-sitter/__tests__/parseSourceCodeDefinitions.embedded_template.test.ts\\n 43 | \\tit(\\\"should detect control structures\\\", () => {\\n 44 | \\t\\texpect(parseResult).toMatch(/\\\\d+--\\\\d+ \\\\|\\\\s+<% if user\\\\.authenticated\\\\? %>/)\\n 45 | \\t\\texpect(parseResult).toMatch(/\\\\d+--\\\\d+ \\\\|\\\\s+<% user\\\\.posts\\\\.each do \\\\|post\\\\| %>/)\\n----\\n\\n# src/services/marketplace/schemas.ts\\n 35 | \\ttype: marketplaceItemTypeSchema,\\n 36 | \\tauthor: z.string().optional(),\\n 37 | \\tauthorUrl: z.string().url(\\\"Author URL must be a valid URL\\\").optional(),\\n 38 | \\ttags: z.array(z.string()).optional(),\\n----\\n 61 | \\tdescription: z.string(),\\n 62 | \\tauthor: z.string().optional(),\\n 63 | \\tauthorUrl: z.string().url().optional(),\\n 64 | \\ttags: z.array(z.string()).optional(),\\n----\\n 72 | \\tdescription: z.string(),\\n 73 | \\tauthor: z.string().optional(),\\n 74 | \\tauthorUrl: z.string().url().optional(),\\n 75 | \\turl: z.string().url(), // Required url field\\n----\\n\\n# src/extension.ts\\n 14 | \\n 15 | import { CloudService } from \\\"@roo-code/cloud\\\"\\n 16 | import { TelemetryService, PostHogTelemetryClient } from \\\"@roo-code/telemetry\\\"\\n----\\n 75 | \\t// Initialize Roo Code Cloud service.\\n 76 | \\tawait CloudService.createInstance(context, {\\n 77 | \\t\\tstateChanged: () => ClineProvider.getVisibleInstance()?.postStateToWebview(),\\n----\\n\\n# src/workers/countTokens.ts\\n  4 | \\n  5 | import { tiktoken } from \\\"../utils/tiktoken\\\"\\n  6 | \\n----\\n 10 | \\ttry {\\n 11 | \\t\\tconst count = await tiktoken(content)\\n 12 | \\t\\treturn { success: true, count }\\n----\\n\\n# src/api/index.ts\\n 50 | \\t/**\\n 51 | \\t * Counts tokens for content blocks\\n 52 | \\t * All providers extend BaseProvider which provides a default tiktoken implementation,\\n 53 | \\t * but they can override this to use their native token counting endpoints\\n 54 | \\t *\\n 55 | \\t * @param content The content to count tokens for\\n 56 | \\t * @returns A promise resolving to the token count\\n 57 | \\t */\\n----\\n\\n# src/extension/api.ts\\n219 | \\n220 | \\t\\t\\tcline.on(\\\"taskCompleted\\\", async (_, tokenUsage, toolUsage) => {\\n221 | \\t\\t\\t\\tthis.emit(RooCodeEventName.TaskCompleted, cline.taskId, tokenUsage, toolUsage)\\n222 | \\t\\t\\t\\tthis.taskMap.delete(cline.taskId)\\n----\\n224 | \\t\\t\\t\\tawait this.fileLog(\\n225 | \\t\\t\\t\\t\\t`[${new Date().toISOString()}] taskCompleted -> ${cline.taskId} | ${JSON.stringify(tokenUsage, null, 2)} | ${JSON.stringify(toolUsage, null, 2)}\\\\n`,\\n226 | \\t\\t\\t\\t)\\n----\\n\\n# src/api/providers/ollama.ts\\n 79 | \\t\\t\\t\\ttype: \\\"usage\\\",\\n 80 | \\t\\t\\t\\tinputTokens: lastUsage?.prompt_tokens || 0,\\n 81 | \\t\\t\\t\\toutputTokens: lastUsage?.completion_tokens || 0,\\n 82 | \\t\\t\\t}\\n----\\n\\n# src/core/checkpoints/index.ts\\n215 | \\t\\t\\t\\tJSON.stringify({\\n216 | \\t\\t\\t\\t\\ttokensIn: totalTokensIn,\\n217 | \\t\\t\\t\\t\\ttokensOut: totalTokensOut,\\n218 | \\t\\t\\t\\t\\tcacheWrites: totalCacheWrites,\\n----\\n\\n# src/utils/git.ts\\n 11 | \\tsubject: string\\n 12 | \\tauthor: string\\n 13 | \\tdate: string\\n----\\n 57 | \\t\\t\\tconst { stdout: hashStdout } = await execAsync(\\n 58 | \\t\\t\\t\\t`git log -n 10 --format=\\\"%H%n%h%n%s%n%an%n%ad\\\" --date=short ` + `--author-date-order ${query}`,\\n 59 | \\t\\t\\t\\t{ cwd },\\n----\\n 79 | \\t\\t\\t\\tsubject: lines[i + 2],\\n 80 | \\t\\t\\t\\tauthor: lines[i + 3],\\n 81 | \\t\\t\\t\\tdate: lines[i + 4],\\n----\\n107 | \\t\\t})\\n108 | \\t\\tconst [fullHash, shortHash, subject, author, date, body] = info.trim().split(\\\"\\\\n\\\")\\n109 | \\n----\\n115 | \\t\\t\\t`Commit: ${shortHash} (${fullHash})`,\\n116 | \\t\\t\\t`Author: ${author}`,\\n117 | \\t\\t\\t`Date: ${date}`,\\n----\\n\\n# src/api/providers/__tests__/bedrock-invokedModelId.spec.ts\\n176 | \\n177 | \\t\\t// The last usage event should have the token counts from the metadata\\n178 | \\t\\tconst lastUsageEvent = usageEvents[usageEvents.length - 1]\\n179 | \\t\\t// Expect the usage event to include all token information\\n180 | \\t\\texpect(lastUsageEvent).toMatchObject({\\n----\\n183 | \\t\\t\\toutputTokens: 200,\\n184 | \\t\\t\\t// Cache tokens may be present with default values\\n185 | \\t\\t\\tcacheReadTokens: expect.any(Number),\\n----\\n\\n# src/api/providers/__tests__/groq.spec.ts\\n101 | \\t\\t\\t\\t\\t\\t\\tdone: false,\\n102 | \\t\\t\\t\\t\\t\\t\\tvalue: { choices: [{ delta: {} }], usage: { prompt_tokens: 10, completion_tokens: 20 } },\\n103 | \\t\\t\\t\\t\\t\\t})\\n----\\n139 | \\t\\t\\t\\tmodel: modelId,\\n140 | \\t\\t\\t\\tmax_tokens: modelInfo.maxTokens,\\n141 | \\t\\t\\t\\ttemperature: 0.5,\\n----\\n\\n# src/api/providers/__tests__/gemini.spec.ts\\n167 | \\tdescribe(\\\"calculateCost\\\", () => {\\n168 | \\t\\t// Mock ModelInfo based on gemini-1.5-flash-latest pricing (per 1M tokens)\\n169 | \\t\\t// Removed 'id' and 'name' as they are not part of ModelInfo type directly\\n170 | \\t\\tconst mockInfo: ModelInfo = {\\n171 | \\t\\t\\tinputPrice: 0.125, // $/1M tokens\\n172 | \\t\\t\\toutputPrice: 0.375, // $/1M tokens\\n173 | \\t\\t\\tcacheWritesPrice: 0.125, // Assume same as input for test\\n----\\n179 | \\n180 | \\t\\tit(\\\"should calculate cost correctly based on input and output tokens\\\", () => {\\n181 | \\t\\t\\tconst inputTokens = 10000 // Use larger numbers for per-million pricing\\n----\\n190 | \\n191 | \\t\\tit(\\\"should return 0 if token counts are zero\\\", () => {\\n192 | \\t\\t\\t// Note: The method expects numbers, not undefined. Passing undefined would be a type error.\\n193 | \\t\\t\\t// The calculateCost method itself returns undefined if prices are missing, but 0 if tokens are 0 and prices exist.\\n194 | \\t\\t\\texpect(handler.calculateCost({ info: mockInfo, inputTokens: 0, outputTokens: 0 })).toBe(0)\\n----\\n196 | \\n197 | \\t\\tit(\\\"should handle only input tokens\\\", () => {\\n198 | \\t\\t\\tconst inputTokens = 5000\\n----\\n203 | \\n204 | \\t\\tit(\\\"should handle only output tokens\\\", () => {\\n205 | \\t\\t\\tconst outputTokens = 15000\\n----\\n210 | \\n211 | \\t\\tit(\\\"should calculate cost with cache write tokens\\\", () => {\\n212 | \\t\\t\\tconst inputTokens = 10000\\n----\\n227 | \\n228 | \\t\\tit(\\\"should calculate cost with cache read tokens\\\", () => {\\n229 | \\t\\t\\tconst inputTokens = 10000 // Total logical input\\n----\\n\\n# src/core/condense/index.ts\\n 55 | \\tcost: number // The cost of the summarization operation\\n 56 | \\tnewContextTokens?: number // The number of tokens in the context for the next API request\\n 57 | \\terror?: string // Populated iff the operation fails: error message shown to the user on failure (see Task.ts)\\n----\\n 63 |  * @param {ApiMessage[]} messages - The conversation messages\\n 64 |  * @param {ApiHandler} apiHandler - The API handler to use for token counting.\\n 65 |  * @param {string} systemPrompt - The system prompt for API requests, which should be considered in the context token count\\n 66 |  * @param {string} taskId - The task ID for the conversation, used for telemetry\\n----\\n 73 |  * @param {ApiMessage[]} messages - The conversation messages\\n 74 |  * @param {ApiHandler} apiHandler - The API handler to use for token counting (fallback if condensingApiHandler not provided)\\n 75 |  * @param {string} systemPrompt - The system prompt for API requests (fallback if customCondensingPrompt not provided)\\n 76 |  * @param {string} taskId - The task ID for the conversation, used for telemetry\\n 77 |  * @param {number} prevContextTokens - The number of tokens currently in the context, used to ensure we don't grow the context\\n 78 |  * @param {boolean} isAutomaticTrigger - Whether the summarization is triggered automatically\\n----\\n186 | \\n187 | \\t// Count the tokens in the context for the next API request\\n188 | \\t// We only estimate the tokens in summaryMesage if outputTokens is 0, otherwise we use outputTokens\\n189 | \\tconst systemPromptMessage: ApiMessage = { role: \\\"user\\\", content: systemPrompt }\\n----\\n\\n# src/api/providers/__tests__/vertex.spec.ts\\n 49 | \\t\\t\\t// The test expects 4 chunks:\\n 50 | \\t\\t\\t// 1. Usage chunk with input tokens\\n 51 | \\t\\t\\t// 2. Text chunk with \\\"Gemini response part 1\\\"\\n 52 | \\t\\t\\t// 3. Text chunk with \\\" part 2\\\"\\n 53 | \\t\\t\\t// 4. Usage chunk with output tokens\\n 54 | \\n----\\n\\n# src/api/providers/__tests__/ollama.spec.ts\\n 28 | \\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 29 | \\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 30 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 31 | \\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 32 | \\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 54 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 55 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 56 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 57 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 58 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n\\n# src/core/environment/getEnvironmentDetails.ts\\n192 | \\n193 | \\t// Add context tokens information.\\n194 | \\tconst { contextTokens, totalCost } = getApiMetrics(cline.clineMessages)\\n----\\n\\n# src/api/providers/__tests__/openai.spec.ts\\n 29 | \\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 30 | \\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 31 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 32 | \\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 33 | \\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 55 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 56 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 57 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 58 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 59 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n\\n# src/api/providers/__tests__/anthropic.spec.ts\\n 19 | \\t\\t\\t\\t\\t\\tusage: {\\n 20 | \\t\\t\\t\\t\\t\\t\\tinput_tokens: 10,\\n 21 | \\t\\t\\t\\t\\t\\t\\toutput_tokens: 5,\\n 22 | \\t\\t\\t\\t\\t\\t},\\n----\\n 30 | \\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 31 | \\t\\t\\t\\t\\t\\t\\t\\t\\tinput_tokens: 100,\\n 32 | \\t\\t\\t\\t\\t\\t\\t\\t\\toutput_tokens: 50,\\n 33 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcache_creation_input_tokens: 20,\\n 34 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcache_read_input_tokens: 10,\\n 35 | \\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n105 | \\n106 | \\t\\tit(\\\"use apiKey for passing token if anthropicUseAuthToken is not set\\\", () => {\\n107 | \\t\\t\\tconst handlerWithCustomUrl = new AnthropicHandler({\\n----\\n112 | \\t\\t\\texpect(mockAnthropicConstructor.mock.calls[0]![0]!.apiKey).toEqual(\\\"test-api-key\\\")\\n113 | \\t\\t\\texpect(mockAnthropicConstructor.mock.calls[0]![0]!.authToken).toBeUndefined()\\n114 | \\t\\t})\\n115 | \\n116 | \\t\\tit(\\\"use apiKey for passing token if anthropicUseAuthToken is set but custom base URL is not given\\\", () => {\\n117 | \\t\\t\\tconst handlerWithCustomUrl = new AnthropicHandler({\\n----\\n123 | \\t\\t\\texpect(mockAnthropicConstructor.mock.calls[0]![0]!.apiKey).toEqual(\\\"test-api-key\\\")\\n124 | \\t\\t\\texpect(mockAnthropicConstructor.mock.calls[0]![0]!.authToken).toBeUndefined()\\n125 | \\t\\t})\\n126 | \\n127 | \\t\\tit(\\\"use authToken for passing token if both of anthropicBaseUrl and anthropicUseAuthToken are set\\\", () => {\\n128 | \\t\\t\\tconst customBaseUrl = \\\"https://custom.anthropic.com\\\"\\n----\\n135 | \\t\\t\\texpect(mockAnthropicConstructor).toHaveBeenCalledTimes(1)\\n136 | \\t\\t\\texpect(mockAnthropicConstructor.mock.calls[0]![0]!.authToken).toEqual(\\\"test-api-key\\\")\\n137 | \\t\\t\\texpect(mockAnthropicConstructor.mock.calls[0]![0]!.apiKey).toBeUndefined()\\n----\\n190 | \\t\\t\\t\\tmessages: [{ role: \\\"user\\\", content: \\\"Test prompt\\\" }],\\n191 | \\t\\t\\t\\tmax_tokens: 8192,\\n192 | \\t\\t\\t\\ttemperature: 0,\\n----\\n\\n# src/api/providers/__tests__/xai.test.ts\\n228 | \\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n229 | \\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n230 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 20,\\n231 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcache_read_input_tokens: 5,\\n232 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcache_creation_input_tokens: 15,\\n233 | \\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n284 | \\t\\t\\t\\tmodel: modelId,\\n285 | \\t\\t\\t\\tmax_tokens: modelInfo.maxTokens,\\n286 | \\t\\t\\t\\ttemperature: 0,\\n----\\n\\n# src/api/providers/__tests__/openrouter.spec.ts\\n132 | \\t\\t\\t\\t\\t\\tchoices: [{ delta: {} }],\\n133 | \\t\\t\\t\\t\\t\\tusage: { prompt_tokens: 10, completion_tokens: 20, cost: 0.001 },\\n134 | \\t\\t\\t\\t\\t}\\n----\\n162 | \\t\\t\\t\\texpect.objectContaining({\\n163 | \\t\\t\\t\\t\\tmax_tokens: 8192,\\n164 | \\t\\t\\t\\t\\tmessages: [\\n----\\n285 | \\t\\t\\t\\tmodel: mockOptions.openRouterModelId,\\n286 | \\t\\t\\t\\tmax_tokens: 8192,\\n287 | \\t\\t\\t\\tthinking: undefined,\\n----\\n\\n# src/api/providers/unbound.ts\\n 22 | interface UnboundUsage extends OpenAI.CompletionUsage {\\n 23 | \\tcache_creation_input_tokens?: number\\n 24 | \\tcache_read_input_tokens?: number\\n 25 | }\\n----\\n 73 | \\n 74 | \\t\\t// Required by Anthropic; other providers default to max tokens allowed.\\n 75 | \\t\\tlet maxTokens: number | undefined\\n----\\n 82 | \\t\\t\\tmodel: modelId.split(\\\"/\\\")[1],\\n 83 | \\t\\t\\tmax_tokens: maxTokens,\\n 84 | \\t\\t\\tmessages: openAiMessages,\\n----\\n111 | \\t\\t\\t\\t\\ttype: \\\"usage\\\",\\n112 | \\t\\t\\t\\t\\tinputTokens: usage.prompt_tokens || 0,\\n113 | \\t\\t\\t\\t\\toutputTokens: usage.completion_tokens || 0,\\n114 | \\t\\t\\t\\t}\\n115 | \\n116 | \\t\\t\\t\\t// Only add cache tokens if they exist.\\n117 | \\t\\t\\t\\tif (usage.cache_creation_input_tokens) {\\n118 | \\t\\t\\t\\t\\tusageData.cacheWriteTokens = usage.cache_creation_input_tokens\\n119 | \\t\\t\\t\\t}\\n120 | \\n121 | \\t\\t\\t\\tif (usage.cache_read_input_tokens) {\\n122 | \\t\\t\\t\\t\\tusageData.cacheReadTokens = usage.cache_read_input_tokens\\n123 | \\t\\t\\t\\t}\\n----\\n146 | \\t\\t\\tif (modelId.startsWith(\\\"anthropic/\\\")) {\\n147 | \\t\\t\\t\\trequestOptions.max_tokens = info.maxTokens\\n148 | \\t\\t\\t}\\n----\\n\\n# src/api/providers/vscode-lm.ts\\n128 | \\t\\t\\t\\tmaxInputTokens: 8192,\\n129 | \\t\\t\\t\\tsendRequest: async (_messages, _options, _token) => {\\n130 | \\t\\t\\t\\t\\t// Provide a minimal implementation\\n----\\n179 | \\t * Implements the ApiHandler countTokens interface method\\n180 | \\t * Provides token counting for Anthropic content blocks\\n181 | \\t *\\n182 | \\t * @param content The content blocks to count tokens for\\n183 | \\t * @returns A promise resolving to the token count\\n184 | \\t */\\n185 | \\toverride async countTokens(content: Array<Anthropic.Messages.ContentBlockParam>): Promise<number> {\\n186 | \\t\\t// Convert Anthropic content blocks to a string for VSCode LM token counting\\n187 | \\t\\tlet textContent = \\\"\\\"\\n----\\n201 | \\t/**\\n202 | \\t * Private implementation of token counting used internally by VsCodeLmHandler\\n203 | \\t */\\n----\\n206 | \\t\\tif (!this.client) {\\n207 | \\t\\t\\tconsole.warn(\\\"Roo Code <Language Model API>: No client available for token counting\\\")\\n208 | \\t\\t\\treturn 0\\n----\\n211 | \\t\\tif (!this.currentRequestCancellation) {\\n212 | \\t\\t\\tconsole.warn(\\\"Roo Code <Language Model API>: No cancellation token available for token counting\\\")\\n213 | \\t\\t\\treturn 0\\n----\\n217 | \\t\\tif (!text) {\\n218 | \\t\\t\\tconsole.debug(\\\"Roo Code <Language Model API>: Empty text provided for token counting\\\")\\n219 | \\t\\t\\treturn 0\\n----\\n223 | \\t\\t\\t// Handle different input types\\n224 | \\t\\t\\tlet tokenCount: number\\n225 | \\n226 | \\t\\t\\tif (typeof text === \\\"string\\\") {\\n227 | \\t\\t\\t\\ttokenCount = await this.client.countTokens(text, this.currentRequestCancellation.token)\\n228 | \\t\\t\\t} else if (text instanceof vscode.LanguageModelChatMessage) {\\n----\\n233 | \\t\\t\\t\\t}\\n234 | \\t\\t\\t\\ttokenCount = await this.client.countTokens(text, this.currentRequestCancellation.token)\\n235 | \\t\\t\\t} else {\\n236 | \\t\\t\\t\\tconsole.warn(\\\"Roo Code <Language Model API>: Invalid input type for token counting\\\")\\n237 | \\t\\t\\t\\treturn 0\\n----\\n240 | \\t\\t\\t// Validate the result\\n241 | \\t\\t\\tif (typeof tokenCount !== \\\"number\\\") {\\n242 | \\t\\t\\t\\tconsole.warn(\\\"Roo Code <Language Model API>: Non-numeric token count received:\\\", tokenCount)\\n243 | \\t\\t\\t\\treturn 0\\n----\\n245 | \\n246 | \\t\\t\\tif (tokenCount < 0) {\\n247 | \\t\\t\\t\\tconsole.warn(\\\"Roo Code <Language Model API>: Negative token count received:\\\", tokenCount)\\n248 | \\t\\t\\t\\treturn 0\\n----\\n250 | \\n251 | \\t\\t\\treturn tokenCount\\n252 | \\t\\t} catch (error) {\\n----\\n278 | \\n279 | \\t\\treturn systemTokens + messageTokens.reduce((sum: number, tokens: number): number => sum + tokens, 0)\\n280 | \\t}\\n----\\n357 | \\n358 | \\t\\t// Initialize cancellation token for the request\\n359 | \\t\\tthis.currentRequestCancellation = new vscode.CancellationTokenSource()\\n360 | \\n361 | \\t\\t// Calculate input tokens before starting the stream\\n362 | \\t\\tconst totalInputTokens: number = await this.calculateTotalInputTokens(systemPrompt, vsCodeLmMessages)\\n363 | \\n364 | \\t\\t// Accumulate the text and count at the end of the stream to reduce token counting overhead.\\n365 | \\t\\tlet accumulatedText: string = \\\"\\\"\\n----\\n378 | \\t\\t\\t\\trequestOptions,\\n379 | \\t\\t\\t\\tthis.currentRequestCancellation.token,\\n380 | \\t\\t\\t)\\n----\\n446 | \\n447 | \\t\\t\\t// Count tokens in the accumulated text after stream completion\\n448 | \\t\\t\\tconst totalOutputTokens: number = await this.internalCountTokens(accumulatedText)\\n----\\n511 | \\t\\t\\tconst modelInfo: ModelInfo = {\\n512 | \\t\\t\\t\\tmaxTokens: -1, // Unlimited tokens by default\\n513 | \\t\\t\\t\\tcontextWindow:\\n----\\n548 | \\t\\t\\t\\t{},\\n549 | \\t\\t\\t\\tnew vscode.CancellationTokenSource().token,\\n550 | \\t\\t\\t)\\n----\\n\\n# src/core/condense/__tests__/index.test.ts\\n203 | \\n204 | \\t\\t// Check the cost and token counts\\n205 | \\t\\texpect(result.cost).toBe(0.05)\\n206 | \\t\\texpect(result.summary).toBe(\\\"This is a summary\\\")\\n207 | \\t\\texpect(result.newContextTokens).toBe(250) // 150 output tokens + 100 from countTokens\\n208 | \\t\\texpect(result.error).toBeUndefined()\\n----\\n317 | \\t\\t// Check the newContextTokens calculation includes system prompt\\n318 | \\t\\texpect(result.newContextTokens).toBe(300) // 200 output tokens + 100 from countTokens\\n319 | \\t\\texpect(result.cost).toBe(0.06)\\n----\\n323 | \\n324 | \\tit(\\\"should return error when new context tokens >= previous context tokens\\\", async () => {\\n325 | \\t\\tconst messages: ApiMessage[] = [\\n----\\n336 | \\t\\tconst streamWithLargeTokens = (async function* () {\\n337 | \\t\\t\\tyield { type: \\\"text\\\" as const, text: \\\"This is a very long summary that uses many tokens\\\" }\\n338 | \\t\\t\\tyield { type: \\\"usage\\\" as const, totalCost: 0.08, outputTokens: 500 }\\n----\\n364 | \\n365 | \\tit(\\\"should successfully summarize when new context tokens < previous context tokens\\\", async () => {\\n366 | \\t\\tconst messages: ApiMessage[] = [\\n----\\n375 | \\n376 | \\t\\t// Create a stream that produces a summary with reasonable token count\\n377 | \\t\\tconst streamWithSmallTokens = (async function* () {\\n----\\n401 | \\t\\texpect(result.error).toBeUndefined()\\n402 | \\t\\texpect(result.newContextTokens).toBe(80) // 50 output tokens + 30 from countTokens\\n403 | \\t\\texpect(result.newContextTokens).toBeLessThan(prevContextTokens)\\n----\\n\\n# src/api/providers/__tests__/lmstudio.test.ts\\n 25 | \\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 26 | \\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 27 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 28 | \\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 29 | \\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 51 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 52 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 53 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 54 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 55 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n\\n# src/api/providers/__tests__/bedrock-reasoning.test.ts\\n 94 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n 95 | \\t\\t\\t\\tbudget_tokens: 4096, // Uses the full modelMaxThinkingTokens value\\n 96 | \\t\\t\\t})\\n----\\n143 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n144 | \\t\\t\\t\\tbudget_tokens: 8192,\\n145 | \\t\\t\\t})\\n----\\n266 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n267 | \\t\\t\\t\\tbudget_tokens: 4096,\\n268 | \\t\\t\\t})\\n----\\n\\n# src/api/providers/lm-studio.ts\\n 40 | \\t\\t// -------------------------\\n 41 | \\t\\t// Track token usage\\n 42 | \\t\\t// -------------------------\\n----\\n 68 | \\t\\t} catch (err) {\\n 69 | \\t\\t\\tconsole.error(\\\"[LmStudio] Failed to count input tokens:\\\", err)\\n 70 | \\t\\t\\tinputTokens = 0\\n----\\n116 | \\t\\t\\t} catch (err) {\\n117 | \\t\\t\\t\\tconsole.error(\\\"[LmStudio] Failed to count output tokens:\\\", err)\\n118 | \\t\\t\\t\\toutputTokens = 0\\n----\\n\\n# src/utils/countTokens.ts\\n  4 | import { countTokensResultSchema } from \\\"../workers/types\\\"\\n  5 | import { tiktoken } from \\\"./tiktoken\\\"\\n  6 | \\n----\\n 27 | \\tif (!useWorker || !pool) {\\n 28 | \\t\\treturn tiktoken(content)\\n 29 | \\t}\\n----\\n 42 | \\t\\tconsole.error(error)\\n 43 | \\t\\treturn tiktoken(content)\\n 44 | \\t}\\n----\\n\\n# src/api/providers/__tests__/requesty.spec.ts\\n124 | \\t\\t\\t\\t\\t\\tusage: {\\n125 | \\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n126 | \\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 20,\\n127 | \\t\\t\\t\\t\\t\\t\\tprompt_tokens_details: {\\n128 | \\t\\t\\t\\t\\t\\t\\t\\tcaching_tokens: 5,\\n129 | \\t\\t\\t\\t\\t\\t\\t\\tcached_tokens: 2,\\n130 | \\t\\t\\t\\t\\t\\t\\t},\\n----\\n162 | \\t\\t\\t\\texpect.objectContaining({\\n163 | \\t\\t\\t\\t\\tmax_tokens: 8192,\\n164 | \\t\\t\\t\\t\\tmessages: [\\n----\\n204 | \\t\\t\\t\\tmodel: mockOptions.requestyModelId,\\n205 | \\t\\t\\t\\tmax_tokens: 8192,\\n206 | \\t\\t\\t\\tmessages: [{ role: \\\"system\\\", content: \\\"test prompt\\\" }],\\n----\\n\\n# src/api/providers/__tests__/vscode-lm.test.ts\\n 28 | \\t\\tCancellationTokenSource: jest.fn(() => ({\\n 29 | \\t\\t\\ttoken: {\\n 30 | \\t\\t\\t\\tisCancellationRequested: false,\\n----\\n\\n# src/api/providers/__tests__/chutes.spec.ts\\n 49 | \\t\\t\\t\\t\\tusage: {\\n 50 | \\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 51 | \\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 52 | \\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 53 | \\t\\t\\t\\t\\t},\\n----\\n103 | \\t\\t\\t\\t\\t],\\n104 | \\t\\t\\t\\t\\tusage: { prompt_tokens: 10, completion_tokens: 5 },\\n105 | \\t\\t\\t\\t}\\n----\\n211 | \\t\\t\\t\\t\\t\\t\\tdone: false,\\n212 | \\t\\t\\t\\t\\t\\t\\tvalue: { choices: [{ delta: {} }], usage: { prompt_tokens: 10, completion_tokens: 20 } },\\n213 | \\t\\t\\t\\t\\t\\t})\\n----\\n284 | \\t\\t\\t\\tmodel: modelId,\\n285 | \\t\\t\\t\\tmax_tokens: modelInfo.maxTokens,\\n286 | \\t\\t\\t\\ttemperature: 0.5,\\n----\\n\\n# src/api/providers/__tests__/openai-native.spec.ts\\n 29 | \\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 30 | \\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 31 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 32 | \\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 33 | \\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 55 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 56 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 57 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 58 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 59 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n145 | \\t\\t\\t\\t\\t\\tusage: {\\n146 | \\t\\t\\t\\t\\t\\t\\tprompt_tokens: 0,\\n147 | \\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 0,\\n148 | \\t\\t\\t\\t\\t\\t\\ttotal_tokens: 0,\\n149 | \\t\\t\\t\\t\\t\\t},\\n----\\n215 | \\t\\t\\t\\t{ choices: [{ delta: { content: \\\" there\\\" } }], usage: null },\\n216 | \\t\\t\\t\\t{ choices: [{ delta: { content: \\\"!\\\" } }], usage: { prompt_tokens: 10, completion_tokens: 5 } },\\n217 | \\t\\t\\t]\\n----\\n261 | \\t\\t\\t\\t{ choices: [{ delta: { content: null } }], usage: null },\\n262 | \\t\\t\\t\\t{ choices: [{ delta: { content: \\\"Hello\\\" } }], usage: { prompt_tokens: 10, completion_tokens: 5 } },\\n263 | \\t\\t\\t]\\n----\\n\\n# src/api/providers/__tests__/deepseek.test.ts\\n 28 | \\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 29 | \\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 30 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 31 | \\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 32 | \\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens_details: {\\n 33 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcache_miss_tokens: 8,\\n 34 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcached_tokens: 2,\\n 35 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 59 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 60 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 61 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 62 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 63 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens_details: {\\n 64 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcache_miss_tokens: 8,\\n 65 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcached_tokens: 2,\\n 66 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n261 | \\t\\t\\tconst usage = {\\n262 | \\t\\t\\t\\tprompt_tokens: 100,\\n263 | \\t\\t\\t\\tcompletion_tokens: 50,\\n264 | \\t\\t\\t\\ttotal_tokens: 150,\\n265 | \\t\\t\\t\\tprompt_tokens_details: {\\n266 | \\t\\t\\t\\t\\tcache_miss_tokens: 80,\\n267 | \\t\\t\\t\\t\\tcached_tokens: 20,\\n268 | \\t\\t\\t\\t},\\n----\\n289 | \\t\\t\\tconst usage = {\\n290 | \\t\\t\\t\\tprompt_tokens: 100,\\n291 | \\t\\t\\t\\tcompletion_tokens: 50,\\n292 | \\t\\t\\t\\ttotal_tokens: 150,\\n293 | \\t\\t\\t\\t// No prompt_tokens_details\\n294 | \\t\\t\\t}\\n----\\n\\n# src/api/transform/cache-strategy/multi-point-strategy.ts\\n  7 |  * Strategy for handling multiple cache points.\\n  8 |  * Creates cache points after messages as soon as uncached tokens exceed minimumTokenCount.\\n  9 |  */\\n----\\n 58 | \\t *\\n 59 | \\t * @param minTokensPerPoint Minimum tokens required per cache point\\n 60 | \\t * @param remainingCachePoints Number of cache points available\\n----\\n 97 | \\n 98 | \\t\\t// Calculate tokens in new messages (added since last cache point placement)\\n 99 | \\t\\tconst lastPreviousIndex = previousPlacements[previousPlacements.length - 1].index\\n----\\n103 | \\n104 | \\t\\t// If new messages have enough tokens for a cache point, we need to decide\\n105 | \\t\\t// whether to keep all previous cache points or combine some\\n----\\n127 | \\t\\t\\t\\t// We need to decide which previous cache points to keep and which to combine\\n128 | \\t\\t\\t\\t// Strategy: Compare the token count of new messages with the smallest combined token gap\\n129 | \\n130 | \\t\\t\\t\\t// First, analyze the token distribution between previous cache points\\n131 | \\t\\t\\t\\tconst tokensBetweenPlacements: number[] = []\\n132 | \\t\\t\\t\\tlet startIdx = 0\\n----\\n134 | \\t\\t\\t\\tfor (const placement of previousPlacements) {\\n135 | \\t\\t\\t\\t\\tconst tokens = this.config.messages\\n136 | \\t\\t\\t\\t\\t\\t.slice(startIdx, placement.index + 1)\\n----\\n138 | \\n139 | \\t\\t\\t\\t\\ttokensBetweenPlacements.push(tokens)\\n140 | \\t\\t\\t\\t\\tstartIdx = placement.index + 1\\n----\\n142 | \\n143 | \\t\\t\\t\\t// Find the two consecutive placements with the smallest token gap\\n144 | \\t\\t\\t\\tlet smallestGapIndex = 0\\n----\\n146 | \\n147 | \\t\\t\\t\\tfor (let i = 0; i < tokensBetweenPlacements.length - 1; i++) {\\n148 | \\t\\t\\t\\t\\tconst gap = tokensBetweenPlacements[i] + tokensBetweenPlacements[i + 1]\\n149 | \\t\\t\\t\\t\\tif (gap < smallestGap) {\\n----\\n155 | \\t\\t\\t\\t// Only combine cache points if it's beneficial\\n156 | \\t\\t\\t\\t// Compare the token count of new messages with the smallest combined token gap\\n157 | \\t\\t\\t\\t// Apply a required percentage increase to ensure reallocation is worth it\\n----\\n161 | \\t\\t\\t\\tif (newMessagesTokens >= requiredTokenThreshold) {\\n162 | \\t\\t\\t\\t\\t// It's beneficial to combine cache points since new messages have significantly more tokens\\n163 | \\t\\t\\t\\t\\tlogger.info(\\\"Combining cache points is beneficial\\\", {\\n----\\n231 | \\t\\t} else {\\n232 | \\t\\t\\t// New messages don't have enough tokens for a cache point\\n233 | \\t\\t\\t// Keep all previous placements that are still valid\\n----\\n266 | \\t\\tif (lastUserMessageIndex >= 0) {\\n267 | \\t\\t\\t// Calculate the total tokens covered from the previous cache point (or start of conversation)\\n268 | \\t\\t\\t// to this cache point. This ensures tokensCovered represents the full span of tokens\\n269 | \\t\\t\\t// that will be cached by this cache point.\\n----\\n281 | \\n282 | \\t\\t\\t// Calculate tokens from previous cache point (or start) to this cache point\\n283 | \\t\\t\\tconst tokenStartIndex = previousCachePointIndex + 1\\n284 | \\t\\t\\ttotalTokensCovered = this.config.messages\\n285 | \\t\\t\\t\\t.slice(tokenStartIndex, lastUserMessageIndex + 1)\\n286 | \\t\\t\\t\\t.reduce((acc, curr) => acc + this.estimateTokenCount(curr), 0)\\n287 | \\n288 | \\t\\t\\t// Guard clause: ensure we have enough tokens to justify a cache point\\n289 | \\t\\t\\tif (totalTokensCovered < minTokensPerPoint) {\\n----\\n294 | \\t\\t\\t\\ttype: \\\"message\\\",\\n295 | \\t\\t\\t\\ttokensCovered: totalTokensCovered,\\n296 | \\t\\t\\t}\\n----\\n\\n# src/api/transform/model-params.ts\\n 95 | \\n 96 | \\t\\t// Reasoning cannot be less than 1024 tokens.\\n 97 | \\t\\tif (reasoningBudget < 1024) {\\n----\\n\\n# src/api/transform/cache-strategy/types.ts\\n  7 | export interface ModelInfo {\\n  8 | \\t/** Maximum number of tokens the model can generate */\\n  9 | \\tmaxTokens: number\\n 10 | \\t/** Maximum context window size in tokens */\\n 11 | \\tcontextWindow: number\\n----\\n 15 | \\tmaxCachePoints: number\\n 16 | \\t/** Minimum number of tokens required for a cache point */\\n 17 | \\tminTokensPerCachePoint: number\\n----\\n 49 | \\ttype: \\\"system\\\" | \\\"message\\\"\\n 50 | \\t/** Number of tokens this cache point covers */\\n 51 | \\ttokensCovered: number\\n 52 | }\\n----\\n\\n# src/api/transform/__tests__/reasoning.spec.ts\\n 40 | \\n 41 | \\t\\t\\texpect(result).toEqual({ max_tokens: 1000 })\\n 42 | \\t\\t})\\n----\\n 61 | \\n 62 | \\t\\t\\texpect(result).toEqual({ max_tokens: 1000 })\\n 63 | \\t\\t})\\n----\\n123 | \\n124 | \\t\\t\\texpect(result).toEqual({ max_tokens: 1000 })\\n125 | \\t\\t})\\n----\\n140 | \\n141 | \\t\\t\\texpect(result).toEqual({ max_tokens: undefined })\\n142 | \\t\\t})\\n----\\n189 | \\n190 | \\t\\t\\texpect(result).toEqual({ max_tokens: 0 })\\n191 | \\t\\t})\\n----\\n244 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n245 | \\t\\t\\t\\tbudget_tokens: 1000,\\n246 | \\t\\t\\t})\\n----\\n268 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n269 | \\t\\t\\t\\tbudget_tokens: 1000,\\n270 | \\t\\t\\t})\\n----\\n314 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n315 | \\t\\t\\t\\tbudget_tokens: undefined,\\n316 | \\t\\t\\t})\\n----\\n334 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n335 | \\t\\t\\t\\tbudget_tokens: 0,\\n336 | \\t\\t\\t})\\n----\\n354 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n355 | \\t\\t\\t\\tbudget_tokens: 100000,\\n356 | \\t\\t\\t})\\n----\\n374 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n375 | \\t\\t\\t\\tbudget_tokens: 1000,\\n376 | \\t\\t\\t})\\n----\\n547 | \\n548 | \\t\\t\\texpect(openRouterResult).toEqual({ max_tokens: 1000 })\\n549 | \\t\\t\\texpect(anthropicResult).toEqual({ type: \\\"enabled\\\", budget_tokens: 1000 })\\n550 | \\t\\t\\texpect(openAiResult).toBeUndefined()\\n----\\n601 | \\t\\t\\t// Budget should take precedence for OpenRouter and Anthropic\\n602 | \\t\\t\\texpect(openRouterResult).toEqual({ max_tokens: 1000 })\\n603 | \\t\\t\\texpect(anthropicResult).toEqual({ type: \\\"enabled\\\", budget_tokens: 1000 })\\n604 | \\t\\t\\t// OpenAI should still use effort since it doesn't support budget\\n----\\n672 | \\t\\t\\t\\texpect(typeof result).toBe(\\\"object\\\")\\n673 | \\t\\t\\t\\texpect(\\\"max_tokens\\\" in result || \\\"effort\\\" in result || \\\"exclude\\\" in result).toBe(true)\\n674 | \\t\\t\\t}\\n----\\n688 | \\t\\t\\t\\texpect(result).toHaveProperty(\\\"type\\\", \\\"enabled\\\")\\n689 | \\t\\t\\t\\texpect(result).toHaveProperty(\\\"budget_tokens\\\")\\n690 | \\t\\t\\t}\\n----\\n\\n# src/api/transform/__tests__/model-params.spec.ts\\n205 | \\t\\t\\t\\t\\ttype: \\\"enabled\\\",\\n206 | \\t\\t\\t\\t\\tbudget_tokens: 1600,\\n207 | \\t\\t\\t\\t},\\n----\\n226 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n227 | \\t\\t\\t\\tbudget_tokens: 1600,\\n228 | \\t\\t\\t})\\n----\\n262 | \\t\\t\\t\\t\\ttype: \\\"enabled\\\",\\n263 | \\t\\t\\t\\t\\tbudget_tokens: 2400,\\n264 | \\t\\t\\t\\t},\\n----\\n287 | \\t\\t\\t\\t\\ttype: \\\"enabled\\\",\\n288 | \\t\\t\\t\\t\\tbudget_tokens: 1500,\\n289 | \\t\\t\\t\\t},\\n----\\n308 | \\n309 | \\t\\tit(\\\"should clamp thinking budget to at least 1024 tokens\\\", () => {\\n310 | \\t\\t\\tconst model: ModelInfo = {\\n----\\n328 | \\t\\t\\t\\t\\ttype: \\\"enabled\\\",\\n329 | \\t\\t\\t\\t\\tbudget_tokens: 1024,\\n330 | \\t\\t\\t\\t},\\n----\\n333 | \\n334 | \\t\\tit(\\\"should clamp thinking budget to at most 80% of max tokens\\\", () => {\\n335 | \\t\\t\\tconst model: ModelInfo = {\\n----\\n353 | \\t\\t\\t\\t\\ttype: \\\"enabled\\\",\\n354 | \\t\\t\\t\\t\\tbudget_tokens: 3200,\\n355 | \\t\\t\\t\\t},\\n----\\n372 | \\t\\t\\t\\t\\ttype: \\\"enabled\\\",\\n373 | \\t\\t\\t\\t\\tbudget_tokens: DEFAULT_HYBRID_REASONING_MODEL_THINKING_TOKENS,\\n374 | \\t\\t\\t\\t},\\n----\\n390 | \\t\\t\\texpect(result.maxTokens).toBe(5000)\\n391 | \\t\\t\\texpect(result.reasoningBudget).toBe(2000) // Custom thinking tokens takes precedence\\n392 | \\t\\t})\\n393 | \\n394 | \\t\\tit(\\\"should clamp custom thinking tokens even when custom max tokens is provided\\\", () => {\\n395 | \\t\\t\\tconst model: ModelInfo = {\\n----\\n719 | \\n720 | \\t\\t\\texpect(result.reasoning).toEqual({ max_tokens: 3200 })\\n721 | \\t\\t})\\n----\\n\\n# src/api/transform/cache-strategy/base-strategy.ts\\n 27 | \\t/**\\n 28 | \\t * Calculate token count for system prompt using a more accurate approach\\n 29 | \\t */\\n----\\n 33 | \\n 34 | \\t\\t\\t// Use a more accurate token estimation than simple character count\\n 35 | \\t\\t\\t// Count words and add overhead for punctuation and special tokens\\n 36 | \\t\\t\\tconst words = text.split(/\\\\s+/).filter((word) => word.length > 0)\\n 37 | \\t\\t\\t// Average English word is ~1.3 tokens\\n 38 | \\t\\t\\tlet tokenCount = words.length * 1.3\\n 39 | \\t\\t\\t// Add overhead for punctuation and special characters\\n 40 | \\t\\t\\ttokenCount += (text.match(/[.,!?;:()[\\\\]{}\\\"\\\"''`]/g) || []).length * 0.3\\n 41 | \\t\\t\\t// Add overhead for newlines\\n 42 | \\t\\t\\ttokenCount += (text.match(/\\\\n/g) || []).length * 0.5\\n 43 | \\t\\t\\t// Add a small overhead for system prompt structure\\n 44 | \\t\\t\\ttokenCount += 5\\n 45 | \\n 46 | \\t\\t\\tthis.systemTokenCount = Math.ceil(tokenCount)\\n 47 | \\t\\t}\\n----\\n 84 | \\t/**\\n 85 | \\t * Check if a token count meets the minimum threshold for caching\\n 86 | \\t */\\n 87 | \\tprotected meetsMinTokenThreshold(tokenCount: number): boolean {\\n 88 | \\t\\tconst minTokens = this.config.modelInfo.minTokensPerCachePoint\\n----\\n 91 | \\t\\t}\\n 92 | \\t\\treturn tokenCount >= minTokens\\n 93 | \\t}\\n----\\n 95 | \\t/**\\n 96 | \\t * Estimate token count for a message using a more accurate approach\\n 97 | \\t * This implementation is based on the BaseProvider's countTokens method\\n----\\n100 | \\tprotected estimateTokenCount(message: Anthropic.Messages.MessageParam): number {\\n101 | \\t\\t// Use a more sophisticated token counting approach\\n102 | \\t\\tif (!message.content) return 0\\n----\\n108 | \\t\\t\\t\\tif (block.type === \\\"text\\\") {\\n109 | \\t\\t\\t\\t\\t// Use a more accurate token estimation than simple character count\\n110 | \\t\\t\\t\\t\\t// This is still an approximation but better than character/4\\n----\\n112 | \\t\\t\\t\\t\\tif (text.length > 0) {\\n113 | \\t\\t\\t\\t\\t\\t// Count words and add overhead for punctuation and special tokens\\n114 | \\t\\t\\t\\t\\t\\tconst words = text.split(/\\\\s+/).filter((word) => word.length > 0)\\n115 | \\t\\t\\t\\t\\t\\t// Average English word is ~1.3 tokens\\n116 | \\t\\t\\t\\t\\t\\ttotalTokens += words.length * 1.3\\n----\\n128 | \\t\\t\\tconst text = message.content\\n129 | \\t\\t\\t// Count words and add overhead for punctuation and special tokens\\n130 | \\t\\t\\tconst words = text.split(/\\\\s+/).filter((word) => word.length > 0)\\n131 | \\t\\t\\t// Average English word is ~1.3 tokens\\n132 | \\t\\t\\ttotalTokens += words.length * 1.3\\n----\\n\\n# src/api/providers/__tests__/glama.spec.ts\\n 57 | \\t\\t\\t\\t\\t\\t\\t\\t\\tchoices: [{ delta: {}, index: 0 }],\\n 58 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: { prompt_tokens: 10, completion_tokens: 5, total_tokens: 15 },\\n 59 | \\t\\t\\t\\t\\t\\t\\t\\t}\\n----\\n112 | \\t\\t\\tusage: {\\n113 | \\t\\t\\t\\tprompt_tokens: 10,\\n114 | \\t\\t\\t\\tcompletion_tokens: 5,\\n115 | \\t\\t\\t\\ttotal_tokens: 15,\\n116 | \\t\\t\\t},\\n----\\n176 | \\t\\t\\t\\t\\ttemperature: 0,\\n177 | \\t\\t\\t\\t\\tmax_tokens: 8192,\\n178 | \\t\\t\\t\\t}),\\n----\\n194 | \\n195 | \\t\\tit(\\\"should not set max_tokens for non-Anthropic models\\\", async () => {\\n196 | \\t\\t\\t// Reset mock to clear any previous calls\\n----\\n213 | \\t\\t\\t)\\n214 | \\t\\t\\texpect(mockCreate.mock.calls[0][0]).not.toHaveProperty(\\\"max_tokens\\\")\\n215 | \\t\\t})\\n----\\n\\n# src/api/transform/__tests__/stream.spec.ts\\n 33 | \\n 34 | \\t\\tit(\\\"should handle usage chunks without cache tokens\\\", () => {\\n 35 | \\t\\t\\tconst usageChunk: ApiStreamChunk = {\\n----\\n 57 | \\n 58 | \\t\\tit(\\\"should handle usage chunks with zero tokens\\\", () => {\\n 59 | \\t\\t\\tconst zeroUsageChunk: ApiStreamChunk = {\\n----\\n 69 | \\n 70 | \\t\\tit(\\\"should handle usage chunks with large token counts\\\", () => {\\n 71 | \\t\\t\\tconst largeUsageChunk: ApiStreamChunk = {\\n----\\n\\n# src/api/transform/cache-strategy/__tests__/cache-strategy.spec.ts\\n 29 | \\n 30 | const createMessageWithTokens = (role: \\\"user\\\" | \\\"assistant\\\", tokenCount: number) => ({\\n 31 | \\trole,\\n 32 | \\tcontent: \\\"x\\\".repeat(tokenCount * 4), // Approximate 4 chars per token\\n 33 | })\\n----\\n130 | \\t\\t\\t\\tit(\\\"adds system cache block when prompt caching is enabled, messages exist, and system prompt is long enough\\\", () => {\\n131 | \\t\\t\\t\\t\\t// Create a system prompt that's at least 50 tokens (200+ characters)\\n132 | \\t\\t\\t\\t\\tconst longSystemPrompt =\\n----\\n236 | \\t\\t\\t\\t\\t\\t\\t\\t(i + 1) +\\n237 | \\t\\t\\t\\t\\t\\t\\t\\t\\\" with some additional text to increase token count. \\\" +\\n238 | \\t\\t\\t\\t\\t\\t\\t\\t\\\"Adding more text to ensure we exceed the token threshold for cache block insertion.\\\",\\n239 | \\t\\t\\t\\t\\t\\t}))\\n----\\n579 | \\n580 | \\t\\tit(\\\"should yield usage results with cache tokens when using MultiPointStrategy\\\", async () => {\\n581 | \\t\\t\\t// Mock the convertToBedrockConverseMessages method to return a result with cache points\\n----\\n589 | \\n590 | \\t\\t\\t// Create a mock stream that includes cache token fields\\n591 | \\t\\t\\tconst mockApiResponse = {\\n----\\n626 | \\n627 | \\t\\t\\t// Verify that usage results with cache tokens are yielded\\n628 | \\t\\t\\texpect(chunks.length).toBeGreaterThan(0)\\n629 | \\t\\t\\t// The test already expects cache tokens, but the implementation might not be including them\\n630 | \\t\\t\\t// Let's make the test more flexible to accept either format\\n----\\n652 | \\n653 | \\t\\t// Helper function to create a message with approximate token count\\n654 | \\t\\tconst createMessage = (role: \\\"user\\\" | \\\"assistant\\\", content: string, tokenCount: number) => {\\n655 | \\t\\t\\t// Pad the content to reach the desired token count (approx 4 chars per token)\\n656 | \\t\\t\\tconst paddingNeeded = Math.max(0, tokenCount * 4 - content.length)\\n657 | \\t\\t\\tconst padding = \\\" \\\".repeat(paddingNeeded)\\n----\\n667 | \\t\\t\\t\\t\\\"Cache point placements:\\\",\\n668 | \\t\\t\\t\\tplacements.map((p) => `index: ${p.index}, tokens: ${p.tokensCovered}`),\\n669 | \\t\\t\\t)\\n----\\n683 | \\t\\t\\t\\t\\tmodelInfo: multiPointModelInfo,\\n684 | \\t\\t\\t\\t\\tsystemPrompt: \\\"You are a helpful assistant.\\\", // ~10 tokens\\n685 | \\t\\t\\t\\t\\tmessages,\\n----\\n730 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n731 | \\t\\t\\t\\t\\t\\ttokensCovered: 300,\\n732 | \\t\\t\\t\\t\\t},\\n----\\n736 | \\t\\t\\t\\t\\tmodelInfo: multiPointModelInfo,\\n737 | \\t\\t\\t\\t\\tsystemPrompt: \\\"You are a helpful assistant.\\\", // ~10 tokens\\n738 | \\t\\t\\t\\t\\tmessages,\\n----\\n759 | \\n760 | \\t\\t\\t\\t// Check if we have a second cache point (may not always be added depending on token distribution)\\n761 | \\t\\t\\t\\tif (result.messageCachePointPlacements && result.messageCachePointPlacements.length > 1) {\\n----\\n789 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n790 | \\t\\t\\t\\t\\t\\ttokensCovered: 300,\\n791 | \\t\\t\\t\\t\\t},\\n----\\n794 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n795 | \\t\\t\\t\\t\\t\\ttokensCovered: 300,\\n796 | \\t\\t\\t\\t\\t},\\n----\\n800 | \\t\\t\\t\\t\\tmodelInfo: multiPointModelInfo,\\n801 | \\t\\t\\t\\t\\tsystemPrompt: \\\"You are a helpful assistant.\\\", // ~10 tokens\\n802 | \\t\\t\\t\\t\\tmessages,\\n----\\n863 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n864 | \\t\\t\\t\\t\\t\\ttokensCovered: 300,\\n865 | \\t\\t\\t\\t\\t},\\n----\\n868 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n869 | \\t\\t\\t\\t\\t\\ttokensCovered: 300,\\n870 | \\t\\t\\t\\t\\t},\\n----\\n873 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n874 | \\t\\t\\t\\t\\t\\ttokensCovered: 300,\\n875 | \\t\\t\\t\\t\\t},\\n----\\n879 | \\t\\t\\t\\t\\tmodelInfo: multiPointModelInfo,\\n880 | \\t\\t\\t\\t\\tsystemPrompt: \\\"You are a helpful assistant.\\\", // ~10 tokens\\n881 | \\t\\t\\t\\t\\tmessages,\\n----\\n929 | \\t\\t\\t\\t// This test verifies that the documentation for Example 5 is correct\\n930 | \\t\\t\\t\\t// In Example 5, the third cache point at index 10 should cover 660 tokens\\n931 | \\t\\t\\t\\t// (260 tokens from messages 7-8 plus 400 tokens from the new messages)\\n932 | \\n----\\n944 | \\t\\t\\t\\t\\tcreateMessage(\\\"assistant\\\", \\\"Deep learning has many applications including...\\\", 160),\\n945 | \\t\\t\\t\\t\\t// New messages with 400 tokens total\\n946 | \\t\\t\\t\\t\\tcreateMessage(\\\"user\\\", \\\"Can you provide a detailed example?\\\", 100),\\n----\\n954 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n955 | \\t\\t\\t\\t\\t\\ttokensCovered: 240,\\n956 | \\t\\t\\t\\t\\t},\\n----\\n959 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n960 | \\t\\t\\t\\t\\t\\ttokensCovered: 440,\\n961 | \\t\\t\\t\\t\\t},\\n----\\n964 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n965 | \\t\\t\\t\\t\\t\\ttokensCovered: 260,\\n966 | \\t\\t\\t\\t\\t},\\n----\\n969 | \\t\\t\\t\\t// In the documentation, the algorithm decides to replace the cache point at index 8\\n970 | \\t\\t\\t\\t// with a new one at index 10, and the tokensCovered value should be 660 tokens\\n971 | \\t\\t\\t\\t// (260 tokens from messages 7-8 plus 400 tokens from the new messages)\\n972 | \\n973 | \\t\\t\\t\\t// However, the actual implementation may behave differently depending on how\\n974 | \\t\\t\\t\\t// it calculates token counts and makes decisions about cache point placement\\n975 | \\n976 | \\t\\t\\t\\t// The important part is that our fix ensures that when a cache point is created,\\n977 | \\t\\t\\t\\t// the tokensCovered value represents all tokens from the previous cache point\\n978 | \\t\\t\\t\\t// to the current cache point, not just the tokens in the new messages\\n979 | \\t\\t\\t})\\n980 | \\n981 | \\t\\t\\tit(\\\"should not combine cache points when new messages have fewer tokens than the smallest combined gap\\\", () => {\\n982 | \\t\\t\\t\\t// This test verifies that when new messages have fewer tokens than the smallest combined gap,\\n983 | \\t\\t\\t\\t// the algorithm keeps all existing cache points and doesn't add a new one\\n----\\n1004 | \\t\\t\\t\\t\\t\\t),\\n1005 | \\t\\t\\t\\t\\t\\t// Small addition (only 50 tokens total)\\n1006 | \\t\\t\\t\\t\\t\\tcreateMessage(\\\"user\\\", \\\"Thanks for the explanation.\\\", 20),\\n----\\n1009 | \\n1010 | \\t\\t\\t\\t\\t// Previous cache point placements with significant token coverage\\n1011 | \\t\\t\\t\\t\\tconst previousCachePointPlacements: CachePointPlacement[] = [\\n----\\n1014 | \\t\\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n1015 | \\t\\t\\t\\t\\t\\t\\ttokensCovered: 400, // Significant token coverage\\n1016 | \\t\\t\\t\\t\\t\\t},\\n----\\n1019 | \\t\\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n1020 | \\t\\t\\t\\t\\t\\t\\ttokensCovered: 300, // Significant token coverage\\n1021 | \\t\\t\\t\\t\\t\\t},\\n----\\n1024 | \\t\\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n1025 | \\t\\t\\t\\t\\t\\t\\ttokensCovered: 300, // Significant token coverage\\n1026 | \\t\\t\\t\\t\\t\\t},\\n----\\n1030 | \\t\\t\\t\\t\\t\\tmodelInfo: multiPointModelInfo,\\n1031 | \\t\\t\\t\\t\\t\\tsystemPrompt: \\\"You are a helpful assistant.\\\", // ~10 tokens\\n1032 | \\t\\t\\t\\t\\t\\tmessages,\\n----\\n1057 | \\n1058 | \\t\\t\\tit(\\\"should make correct decisions based on token counts\\\", () => {\\n1059 | \\t\\t\\t\\t// This test verifies that the algorithm correctly compares token counts\\n1060 | \\t\\t\\t\\t// and makes the right decision about combining cache points\\n1061 | \\n1062 | \\t\\t\\t\\t// Create messages with a variety of token counts\\n1063 | \\t\\t\\t\\tconst messages = [\\n----\\n1081 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n1082 | \\t\\t\\t\\t\\t\\ttokensCovered: 400,\\n1083 | \\t\\t\\t\\t\\t},\\n----\\n1086 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n1087 | \\t\\t\\t\\t\\t\\ttokensCovered: 150,\\n1088 | \\t\\t\\t\\t\\t},\\n----\\n1091 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n1092 | \\t\\t\\t\\t\\t\\ttokensCovered: 150,\\n1093 | \\t\\t\\t\\t\\t},\\n----\\n\\n# src/api/providers/__tests__/openai-usage-tracking.spec.ts\\n 28 | \\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 29 | \\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 30 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 31 | \\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 32 | \\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 47 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 48 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 49 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 2,\\n 50 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 12,\\n 51 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 62 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 63 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 64 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 4,\\n 65 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 14,\\n 66 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 77 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 78 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 79 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 80 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 81 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n155 | \\t\\t\\t\\t\\t\\tchoices: [{ message: { role: \\\"assistant\\\", content: \\\"Test response\\\" } }],\\n156 | \\t\\t\\t\\t\\t\\tusage: { prompt_tokens: 10, completion_tokens: 5, total_tokens: 15 },\\n157 | \\t\\t\\t\\t\\t}\\n----\\n177 | \\t\\t\\t\\t\\t\\t\\tusage: {\\n178 | \\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n179 | \\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n180 | \\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n181 | \\t\\t\\t\\t\\t\\t\\t},\\n----\\n\\n# src/api/providers/__tests__/unbound.spec.ts\\n 82 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 83 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 84 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 85 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 86 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 91 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 92 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 8,\\n 93 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 4,\\n 94 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 12,\\n 95 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcache_creation_input_tokens: 3,\\n 96 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcache_read_input_tokens: 2,\\n 97 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n232 | \\t\\t\\t\\t\\ttemperature: 0,\\n233 | \\t\\t\\t\\t\\tmax_tokens: 8192,\\n234 | \\t\\t\\t\\t}),\\n----\\n253 | \\n254 | \\t\\tit(\\\"should not set max_tokens for non-Anthropic models\\\", async () => {\\n255 | \\t\\t\\tmockCreate.mockClear()\\n----\\n277 | \\n278 | \\t\\t\\texpect(mockCreate.mock.calls[0][0]).not.toHaveProperty(\\\"max_tokens\\\")\\n279 | \\t\\t})\\n----\\n\\n# src/api/providers/__tests__/anthropic-vertex.spec.ts\\n 21 | \\t\\t\\t\\t\\t\\tusage: {\\n 22 | \\t\\t\\t\\t\\t\\t\\tinput_tokens: 10,\\n 23 | \\t\\t\\t\\t\\t\\t\\toutput_tokens: 5,\\n 24 | \\t\\t\\t\\t\\t\\t},\\n----\\n 32 | \\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 33 | \\t\\t\\t\\t\\t\\t\\t\\t\\tinput_tokens: 10,\\n 34 | \\t\\t\\t\\t\\t\\t\\t\\t\\toutput_tokens: 5,\\n 35 | \\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 95 | \\t\\t\\t\\t\\t\\tusage: {\\n 96 | \\t\\t\\t\\t\\t\\t\\tinput_tokens: 10,\\n 97 | \\t\\t\\t\\t\\t\\t\\toutput_tokens: 0,\\n 98 | \\t\\t\\t\\t\\t\\t},\\n----\\n118 | \\t\\t\\t\\t\\tusage: {\\n119 | \\t\\t\\t\\t\\t\\toutput_tokens: 5,\\n120 | \\t\\t\\t\\t\\t},\\n----\\n164 | \\t\\t\\t\\tmodel: \\\"claude-3-5-sonnet-v2@20241022\\\",\\n165 | \\t\\t\\t\\tmax_tokens: 8192,\\n166 | \\t\\t\\t\\ttemperature: 0,\\n----\\n284 | \\t\\t\\t\\t\\t\\tusage: {\\n285 | \\t\\t\\t\\t\\t\\t\\tinput_tokens: 10,\\n286 | \\t\\t\\t\\t\\t\\t\\toutput_tokens: 0,\\n287 | \\t\\t\\t\\t\\t\\t\\tcache_creation_input_tokens: 3,\\n288 | \\t\\t\\t\\t\\t\\t\\tcache_read_input_tokens: 2,\\n289 | \\t\\t\\t\\t\\t\\t},\\n----\\n309 | \\t\\t\\t\\t\\tusage: {\\n310 | \\t\\t\\t\\t\\t\\toutput_tokens: 5,\\n311 | \\t\\t\\t\\t\\t},\\n----\\n419 | \\t\\t\\t\\t\\t\\tusage: {\\n420 | \\t\\t\\t\\t\\t\\t\\tinput_tokens: 10,\\n421 | \\t\\t\\t\\t\\t\\t\\toutput_tokens: 0,\\n422 | \\t\\t\\t\\t\\t\\t\\tcache_creation_input_tokens: 5,\\n423 | \\t\\t\\t\\t\\t\\t\\tcache_read_input_tokens: 3,\\n424 | \\t\\t\\t\\t\\t\\t},\\n----\\n484 | \\t\\t\\t\\t\\t\\tusage: {\\n485 | \\t\\t\\t\\t\\t\\t\\tinput_tokens: 10,\\n486 | \\t\\t\\t\\t\\t\\t\\toutput_tokens: 0,\\n487 | \\t\\t\\t\\t\\t\\t},\\n----\\n618 | \\t\\t\\t\\tmodel: \\\"claude-3-5-sonnet-v2@20241022\\\",\\n619 | \\t\\t\\t\\tmax_tokens: 8192,\\n620 | \\t\\t\\t\\ttemperature: 0,\\n----\\n752 | \\n753 | \\t\\t\\t// Test with default thinking budget (80% of max tokens)\\n754 | \\t\\t\\tconst handlerWithDefaultBudget = new AnthropicVertexHandler({\\n----\\n767 | \\t\\t\\t\\tvertexRegion: \\\"us-central1\\\",\\n768 | \\t\\t\\t\\tmodelMaxTokens: 1000, // This would result in 800 tokens for thinking, but minimum is 1024\\n769 | \\t\\t\\t})\\n----\\n789 | \\t\\t\\t\\t\\t\\tmodel: options.model,\\n790 | \\t\\t\\t\\t\\t\\tusage: { input_tokens: 10, output_tokens: 5 },\\n791 | \\t\\t\\t\\t\\t}\\n----\\n794 | \\t\\t\\t\\t\\tasync *[Symbol.asyncIterator]() {\\n795 | \\t\\t\\t\\t\\t\\tyield { type: \\\"message_start\\\", message: { usage: { input_tokens: 10, output_tokens: 5 } } }\\n796 | \\t\\t\\t\\t\\t},\\n----\\n806 | \\t\\t\\t\\texpect.objectContaining({\\n807 | \\t\\t\\t\\t\\tthinking: { type: \\\"enabled\\\", budget_tokens: 4096 },\\n808 | \\t\\t\\t\\t\\ttemperature: 1.0, // Thinking requires temperature 1.0\\n----\\n\\n# src/core/prompts/tools/codebase-search.ts\\n 12 | \\n 13 | Example: Searching for functions related to user authentication\\n 14 | <codebase_search>\\n----\\n\\n# src/core/task/__tests__/Task.test.ts\\n185 | \\t\\t\\t\\t\\t\\t\\t\\ttask: \\\"historical task\\\",\\n186 | \\t\\t\\t\\t\\t\\t\\t\\ttokensIn: 100,\\n187 | \\t\\t\\t\\t\\t\\t\\t\\ttokensOut: 200,\\n188 | \\t\\t\\t\\t\\t\\t\\t\\tcacheWrites: 0,\\n----\\n253 | \\t\\t\\t\\ttask: \\\"historical task\\\",\\n254 | \\t\\t\\t\\ttokensIn: 100,\\n255 | \\t\\t\\t\\ttokensOut: 200,\\n256 | \\t\\t\\t\\tcacheWrites: 0,\\n----\\n615 | \\t\\t\\t\\t\\t\\ttext: JSON.stringify({\\n616 | \\t\\t\\t\\t\\t\\t\\ttokensIn: 100,\\n617 | \\t\\t\\t\\t\\t\\t\\ttokensOut: 50,\\n618 | \\t\\t\\t\\t\\t\\t\\tcacheWrites: 0,\\n----\\n740 | \\t\\t\\t\\t\\t\\ttext: JSON.stringify({\\n741 | \\t\\t\\t\\t\\t\\t\\ttokensIn: 100,\\n742 | \\t\\t\\t\\t\\t\\t\\ttokensOut: 50,\\n743 | \\t\\t\\t\\t\\t\\t\\tcacheWrites: 0,\\n----\\n\\n# src/core/prompts/instructions/create-mcp-server.ts\\n 11 | \\n 12 | When creating MCP servers, it's important to understand that they operate in a non-interactive environment. The server cannot initiate OAuth flows, open browser windows, or prompt for user input during runtime. All credentials and authentication tokens must be provided upfront through environment variables in the MCP settings configuration. For example, Spotify's API uses OAuth to get a refresh token for the user, but the MCP server cannot initiate this flow. While you can walk the user through  [truncated...]\\n 13 | \\n----\\n 76 | \\t\\t\\t\\t...\\n 77 | \\t\\t\\t\\t\\\"type\\\": \\\"module\\\", // added by default, uses ES module syntax (import/export) rather than CommonJS (require/module.exports) (Important to know if you create additional scripts in this server repository like a get-refresh-token.js script)\\n 78 | \\t\\t\\t\\t\\\"scripts\\\": {\\n----\\n\\n# src/core/task/Task.ts\\n 24 | import { TelemetryService } from \\\"@roo-code/telemetry\\\"\\n 25 | import { CloudService } from \\\"@roo-code/cloud\\\"\\n 26 | \\n----\\n 96 | \\ttaskSpawned: [taskId: string]\\n 97 | \\ttaskCompleted: [taskId: string, tokenUsage: TokenUsage, toolUsage: ToolUsage]\\n 98 | \\ttaskTokenUsageUpdated: [taskId: string, tokenUsage: TokenUsage]\\n 99 | \\ttaskToolFailed: [taskId: string, tool: ToolName, error: string]\\n----\\n346 | \\n347 | \\t\\tconst shouldCaptureMessage = message.partial !== true && CloudService.isEnabled()\\n348 | \\n349 | \\t\\tif (shouldCaptureMessage) {\\n350 | \\t\\t\\tCloudService.instance.captureEvent({\\n351 | \\t\\t\\t\\tevent: TelemetryEventName.TASK_MESSAGE,\\n----\\n366 | \\n367 | \\t\\tconst shouldCaptureMessage = message.partial !== true && CloudService.isEnabled()\\n368 | \\n369 | \\t\\tif (shouldCaptureMessage) {\\n370 | \\t\\t\\tCloudService.instance.captureEvent({\\n371 | \\t\\t\\t\\tevent: TelemetryEventName.TASK_MESSAGE,\\n----\\n384 | \\n385 | \\t\\t\\tconst { historyItem, tokenUsage } = await taskMetadata({\\n386 | \\t\\t\\t\\tmessages: this.clineMessages,\\n----\\n392 | \\n393 | \\t\\t\\tthis.emit(\\\"taskTokenUsageUpdated\\\", this.taskId, tokenUsage)\\n394 | \\n----\\n1246 | \\t\\t\\t\\t\\t...JSON.parse(this.clineMessages[lastApiReqIndex].text || \\\"{}\\\"),\\n1247 | \\t\\t\\t\\t\\ttokensIn: inputTokens,\\n1248 | \\t\\t\\t\\t\\ttokensOut: outputTokens,\\n1249 | \\t\\t\\t\\t\\tcacheWrites: cacheWriteTokens,\\n----\\n1678 | \\t\\tif (contextTokens) {\\n1679 | \\t\\t\\t// Default max tokens value for thinking models when no specific\\n1680 | \\t\\t\\t// value is set.\\n----\\n\\n# src/api/providers/glama.ts\\n 51 | \\n 52 | \\t\\t// Required by Anthropic; other providers default to max tokens allowed.\\n 53 | \\t\\tlet maxTokens: number | undefined\\n----\\n 60 | \\t\\t\\tmodel: modelId,\\n 61 | \\t\\t\\tmax_tokens: maxTokens,\\n 62 | \\t\\t\\tmessages: openAiMessages,\\n----\\n 90 | \\t\\t\\t\\t// In case of an interrupted request, we need to wait for the upstream API to finish processing the request\\n 91 | \\t\\t\\t\\t// before we can fetch information about the token usage and cost.\\n 92 | \\t\\t\\t\\tconst response = await axios.get(\\n----\\n 98 | \\n 99 | \\t\\t\\t\\tif (completionRequest.tokenUsage && completionRequest.totalCostUsd) {\\n100 | \\t\\t\\t\\t\\tyield {\\n101 | \\t\\t\\t\\t\\t\\ttype: \\\"usage\\\",\\n102 | \\t\\t\\t\\t\\t\\tcacheWriteTokens: completionRequest.tokenUsage.cacheCreationInputTokens,\\n103 | \\t\\t\\t\\t\\t\\tcacheReadTokens: completionRequest.tokenUsage.cacheReadInputTokens,\\n104 | \\t\\t\\t\\t\\t\\tinputTokens: completionRequest.tokenUsage.promptTokens,\\n105 | \\t\\t\\t\\t\\t\\toutputTokens: completionRequest.tokenUsage.completionTokens,\\n106 | \\t\\t\\t\\t\\t\\ttotalCost: parseFloat(completionRequest.totalCostUsd),\\n----\\n132 | \\t\\t\\tif (modelId.startsWith(\\\"anthropic/\\\")) {\\n133 | \\t\\t\\t\\trequestOptions.max_tokens = info.maxTokens\\n134 | \\t\\t\\t}\\n----\\n\\n# src/api/transform/reasoning.ts\\n 12 | \\teffort?: ReasoningEffort\\n 13 | \\tmax_tokens?: number\\n 14 | \\texclude?: boolean\\n----\\n 36 | \\tshouldUseReasoningBudget({ model, settings })\\n 37 | \\t\\t? { max_tokens: reasoningBudget }\\n 38 | \\t\\t: shouldUseReasoningEffort({ model, settings })\\n----\\n 46 | }: GetModelReasoningOptions): AnthropicReasoningParams | undefined =>\\n 47 | \\tshouldUseReasoningBudget({ model, settings }) ? { type: \\\"enabled\\\", budget_tokens: reasoningBudget! } : undefined\\n 48 | \\n----\\n\\n# src/api/providers/openrouter.ts\\n 32 | \\tinclude_reasoning?: boolean\\n 33 | \\t// https://openrouter.ai/docs/use-cases/reasoning-tokens\\n 34 | \\treasoning?: OpenRouterReasoningParams\\n----\\n 40 | interface CompletionUsage {\\n 41 | \\tcompletion_tokens?: number\\n 42 | \\tcompletion_tokens_details?: {\\n 43 | \\t\\treasoning_tokens?: number\\n 44 | \\t}\\n 45 | \\tprompt_tokens?: number\\n 46 | \\tprompt_tokens_details?: {\\n 47 | \\t\\tcached_tokens?: number\\n 48 | \\t}\\n 49 | \\ttotal_tokens?: number\\n 50 | \\tcost?: number\\n----\\n 81 | \\n 82 | \\t\\t// OpenRouter sends reasoning tokens by default for Gemini 2.5 Pro\\n 83 | \\t\\t// Preview even if you don't request them. This is not the default for\\n----\\n116 | \\t\\t\\tmodel: modelId,\\n117 | \\t\\t\\t...(maxTokens && maxTokens > 0 && { max_tokens: maxTokens }),\\n118 | \\t\\t\\ttemperature,\\n----\\n165 | \\t\\t\\t\\ttype: \\\"usage\\\",\\n166 | \\t\\t\\t\\tinputTokens: lastUsage.prompt_tokens || 0,\\n167 | \\t\\t\\t\\toutputTokens: lastUsage.completion_tokens || 0,\\n168 | \\t\\t\\t\\t// Waiting on OpenRouter to figure out what this represents in the Gemini case\\n169 | \\t\\t\\t\\t// and how to best support it.\\n170 | \\t\\t\\t\\t// cacheReadTokens: lastUsage.prompt_tokens_details?.cached_tokens,\\n171 | \\t\\t\\t\\treasoningTokens: lastUsage.completion_tokens_details?.reasoning_tokens,\\n172 | \\t\\t\\t\\ttotalCost: (lastUsage.is_byok ? BYOK_COST_MULTIPLIER : 1) * (lastUsage.cost || 0),\\n----\\n219 | \\t\\t\\tmodel: modelId,\\n220 | \\t\\t\\tmax_tokens: maxTokens,\\n221 | \\t\\t\\ttemperature,\\n----\\n\\n# src/utils/__tests__/cost.spec.ts\\n 13 | \\t\\t\\tsupportsPromptCache: true,\\n 14 | \\t\\t\\tinputPrice: 3.0, // $3 per million tokens\\n 15 | \\t\\t\\toutputPrice: 15.0, // $15 per million tokens\\n 16 | \\t\\t\\tcacheWritesPrice: 3.75, // $3.75 per million tokens\\n 17 | \\t\\t\\tcacheReadsPrice: 0.3, // $0.30 per million tokens\\n 18 | \\t\\t}\\n----\\n 70 | \\n 71 | \\t\\tit(\\\"should handle zero tokens\\\", () => {\\n 72 | \\t\\t\\tconst cost = calculateApiCostAnthropic(mockModelInfo, 0, 0, 0, 0)\\n----\\n106 | \\t\\t\\tsupportsPromptCache: true,\\n107 | \\t\\t\\tinputPrice: 3.0, // $3 per million tokens\\n108 | \\t\\t\\toutputPrice: 15.0, // $15 per million tokens\\n109 | \\t\\t\\tcacheWritesPrice: 3.75, // $3.75 per million tokens\\n110 | \\t\\t\\tcacheReadsPrice: 0.3, // $0.30 per million tokens\\n111 | \\t\\t}\\n----\\n163 | \\n164 | \\t\\tit(\\\"should handle zero tokens\\\", () => {\\n165 | \\t\\t\\tconst cost = calculateApiCostOpenAI(mockModelInfo, 0, 0, 0, 0)\\n----\\n\\n# src/utils/__tests__/git.spec.ts\\n 98 | \\t\\t\\t\\tsubject: \\\"fix: test commit\\\",\\n 99 | \\t\\t\\t\\tauthor: \\\"John Doe\\\",\\n100 | \\t\\t\\t\\tdate: \\\"2024-01-06\\\",\\n----\\n162 | \\t\\t\\t\\t[\\n163 | \\t\\t\\t\\t\\t'git log -n 10 --format=\\\"%H%n%h%n%s%n%an%n%ad\\\" --date=short --author-date-order abc123',\\n164 | \\t\\t\\t\\t\\t{ stdout: mockCommitData, stderr: \\\"\\\" },\\n----\\n185 | \\t\\t\\t\\tsubject: \\\"fix: test commit\\\",\\n186 | \\t\\t\\t\\tauthor: \\\"John Doe\\\",\\n187 | \\t\\t\\t\\tdate: \\\"2024-01-06\\\",\\n----\\n\\n# src/api/providers/gemini.ts\\n  7 | } from \\\"@google/genai\\\"\\n  8 | import type { JWTInput } from \\\"google-auth-library\\\"\\n  9 | \\n----\\n179 | \\t\\t\\tif (response.totalTokens === undefined) {\\n180 | \\t\\t\\t\\tconsole.warn(\\\"Gemini token counting returned undefined, using fallback\\\")\\n181 | \\t\\t\\t\\treturn super.countTokens(content)\\n----\\n185 | \\t\\t} catch (error) {\\n186 | \\t\\t\\tconsole.warn(\\\"Gemini token counting failed, using fallback\\\", error)\\n187 | \\t\\t\\treturn super.countTokens(content)\\n----\\n209 | \\n210 | \\t\\t// If there's tiered pricing then adjust the input and output token prices\\n211 | \\t\\t// based on the input tokens used.\\n212 | \\t\\tif (info.tiers) {\\n----\\n221 | \\n222 | \\t\\t// Subtract the cached input tokens from the total input tokens.\\n223 | \\t\\tconst uncachedInputTokens = inputTokens - cacheReadTokens\\n----\\n230 | \\n231 | \\t\\tconst trace: Record<string, { price: number; tokens: number; cost: number }> = {\\n232 | \\t\\t\\tinput: { price: inputPrice, tokens: uncachedInputTokens, cost: inputTokensCost },\\n233 | \\t\\t\\toutput: { price: outputPrice, tokens: outputTokens, cost: outputTokensCost },\\n234 | \\t\\t}\\n----\\n236 | \\t\\tif (cacheReadTokens > 0) {\\n237 | \\t\\t\\ttrace.cacheRead = { price: cacheReadsPrice, tokens: cacheReadTokens, cost: cacheReadCost }\\n238 | \\t\\t}\\n----\\n\\n# src/api/providers/lite-llm.ts\\n 46 | \\n 47 | \\t\\t// Required by some providers; others default to max tokens allowed\\n 48 | \\t\\tlet maxTokens: number | undefined = info.maxTokens ?? undefined\\n----\\n 51 | \\t\\t\\tmodel: modelId,\\n 52 | \\t\\t\\tmax_tokens: maxTokens,\\n 53 | \\t\\t\\tmessages: openAiMessages,\\n----\\n 84 | \\t\\t\\t\\t\\ttype: \\\"usage\\\",\\n 85 | \\t\\t\\t\\t\\tinputTokens: lastUsage.prompt_tokens || 0,\\n 86 | \\t\\t\\t\\t\\toutputTokens: lastUsage.completion_tokens || 0,\\n 87 | \\t\\t\\t\\t\\tcacheWriteTokens: lastUsage.cache_creation_input_tokens || 0,\\n 88 | \\t\\t\\t\\t\\tcacheReadTokens: lastUsage.prompt_tokens_details?.cached_tokens || 0,\\n 89 | \\t\\t\\t\\t}\\n----\\n121 | \\n122 | \\t\\t\\trequestOptions.max_tokens = info.maxTokens\\n123 | \\n----\\n136 | interface LiteLLMUsage extends OpenAI.CompletionUsage {\\n137 | \\tcache_creation_input_tokens?: number\\n138 | }\\n----\\n\\n# src/api/providers/requesty.ts\\n 19 | // Requesty usage includes an extra field for Anthropic use cases.\\n 20 | // Safely cast the prompt token details section to the appropriate structure.\\n 21 | interface RequestyUsage extends OpenAI.CompletionUsage {\\n 22 | \\tprompt_tokens_details?: {\\n 23 | \\t\\tcaching_tokens?: number\\n 24 | \\t\\tcached_tokens?: number\\n 25 | \\t}\\n----\\n 76 | \\t\\tconst requestyUsage = usage as RequestyUsage\\n 77 | \\t\\tconst inputTokens = requestyUsage?.prompt_tokens || 0\\n 78 | \\t\\tconst outputTokens = requestyUsage?.completion_tokens || 0\\n 79 | \\t\\tconst cacheWriteTokens = requestyUsage?.prompt_tokens_details?.caching_tokens || 0\\n 80 | \\t\\tconst cacheReadTokens = requestyUsage?.prompt_tokens_details?.cached_tokens || 0\\n 81 | \\t\\tconst totalCost = modelInfo\\n----\\n102 | \\t\\t\\tinfo,\\n103 | \\t\\t\\tmaxTokens: max_tokens,\\n104 | \\t\\t\\ttemperature,\\n----\\n116 | \\t\\t\\tmodel,\\n117 | \\t\\t\\tmax_tokens,\\n118 | \\t\\t\\ttemperature,\\n----\\n150 | \\tasync completePrompt(prompt: string): Promise<string> {\\n151 | \\t\\tconst { id: model, maxTokens: max_tokens, temperature } = await this.fetchModel()\\n152 | \\n----\\n156 | \\t\\t\\tmodel,\\n157 | \\t\\t\\tmax_tokens,\\n158 | \\t\\t\\tmessages: openAiMessages,\\n----\\n\\n# src/api/providers/anthropic.ts\\n 29 | \\t\\tconst apiKeyFieldName =\\n 30 | \\t\\t\\tthis.options.anthropicBaseUrl && this.options.anthropicUseAuthToken ? \\\"authToken\\\" : \\\"apiKey\\\"\\n 31 | \\n----\\n 75 | \\t\\t\\t\\t\\t\\tmodel: modelId,\\n 76 | \\t\\t\\t\\t\\t\\tmax_tokens: maxTokens ?? ANTHROPIC_DEFAULT_MAX_TOKENS,\\n 77 | \\t\\t\\t\\t\\t\\ttemperature,\\n----\\n124 | \\t\\t\\t\\t\\tmodel: modelId,\\n125 | \\t\\t\\t\\t\\tmax_tokens: maxTokens ?? ANTHROPIC_DEFAULT_MAX_TOKENS,\\n126 | \\t\\t\\t\\t\\ttemperature,\\n----\\n142 | \\t\\t\\t\\t\\t\\ttype: \\\"usage\\\",\\n143 | \\t\\t\\t\\t\\t\\tinputTokens: usage.input_tokens || 0,\\n144 | \\t\\t\\t\\t\\t\\toutputTokens: usage.output_tokens || 0,\\n145 | \\t\\t\\t\\t\\t\\tcacheWriteTokens: usage.cache_creation_input_tokens || undefined,\\n146 | \\t\\t\\t\\t\\t\\tcacheReadTokens: usage.cache_read_input_tokens || undefined,\\n147 | \\t\\t\\t\\t\\t}\\n----\\n151 | \\t\\t\\t\\tcase \\\"message_delta\\\":\\n152 | \\t\\t\\t\\t\\t// Tells us stop_reason, stop_sequence, and output tokens\\n153 | \\t\\t\\t\\t\\t// along the way and at the end of the message.\\n----\\n156 | \\t\\t\\t\\t\\t\\tinputTokens: 0,\\n157 | \\t\\t\\t\\t\\t\\toutputTokens: chunk.usage.output_tokens || 0,\\n158 | \\t\\t\\t\\t\\t}\\n----\\n231 | \\t\\t\\tmodel,\\n232 | \\t\\t\\tmax_tokens: ANTHROPIC_DEFAULT_MAX_TOKENS,\\n233 | \\t\\t\\tthinking: undefined,\\n----\\n243 | \\t/**\\n244 | \\t * Counts tokens for the given content using Anthropic's API\\n245 | \\t *\\n246 | \\t * @param content The content blocks to count tokens for\\n247 | \\t * @returns A promise resolving to the token count\\n248 | \\t */\\n----\\n258 | \\n259 | \\t\\t\\treturn response.input_tokens\\n260 | \\t\\t} catch (error) {\\n261 | \\t\\t\\t// Log error but fallback to tiktoken estimation\\n262 | \\t\\t\\tconsole.warn(\\\"Anthropic token counting failed, using fallback\\\", error)\\n263 | \\n----\\n\\n# src/api/providers/deepseek.ts\\n 32 | \\t\\t\\ttype: \\\"usage\\\",\\n 33 | \\t\\t\\tinputTokens: usage?.prompt_tokens || 0,\\n 34 | \\t\\t\\toutputTokens: usage?.completion_tokens || 0,\\n 35 | \\t\\t\\tcacheWriteTokens: usage?.prompt_tokens_details?.cache_miss_tokens,\\n 36 | \\t\\t\\tcacheReadTokens: usage?.prompt_tokens_details?.cached_tokens,\\n 37 | \\t\\t}\\n----\\n\\n# src/api/providers/openai.ts\\n162 | \\t\\t\\tif (this.options.includeMaxTokens) {\\n163 | \\t\\t\\t\\trequestOptions.max_tokens = modelInfo.maxTokens\\n164 | \\t\\t\\t}\\n----\\n241 | \\t\\t\\ttype: \\\"usage\\\",\\n242 | \\t\\t\\tinputTokens: usage?.prompt_tokens || 0,\\n243 | \\t\\t\\toutputTokens: usage?.completion_tokens || 0,\\n244 | \\t\\t\\tcacheWriteTokens: usage?.cache_creation_input_tokens || undefined,\\n245 | \\t\\t\\tcacheReadTokens: usage?.cache_read_input_tokens || undefined,\\n246 | \\t\\t}\\n----\\n347 | \\t\\t\\t\\t\\ttype: \\\"usage\\\",\\n348 | \\t\\t\\t\\t\\tinputTokens: chunk.usage.prompt_tokens || 0,\\n349 | \\t\\t\\t\\t\\toutputTokens: chunk.usage.completion_tokens || 0,\\n350 | \\t\\t\\t\\t}\\n----\\n\\n# src/api/providers/xai.ts\\n 52 | \\t\\t\\tmodel: modelId,\\n 53 | \\t\\t\\tmax_tokens: modelInfo.maxTokens,\\n 54 | \\t\\t\\ttemperature: this.options.modelTemperature ?? XAI_DEFAULT_TEMPERATURE,\\n----\\n 78 | \\t\\t\\tif (chunk.usage) {\\n 79 | \\t\\t\\t\\t// Extract detailed token information if available\\n 80 | \\t\\t\\t\\t// First check for prompt_tokens_details structure (real API response)\\n 81 | \\t\\t\\t\\tconst promptDetails = \\\"prompt_tokens_details\\\" in chunk.usage ? chunk.usage.prompt_tokens_details : null;\\n 82 | \\t\\t\\t\\tconst cachedTokens = promptDetails && \\\"cached_tokens\\\" in promptDetails ? promptDetails.cached_tokens : 0;\\n 83 | \\n 84 | \\t\\t\\t\\t// Fall back to direct fields in usage (used in test mocks)\\n 85 | \\t\\t\\t\\tconst readTokens = cachedTokens || (\\\"cache_read_input_tokens\\\" in chunk.usage ? (chunk.usage as any).cache_read_input_tokens : 0);\\n 86 | \\t\\t\\t\\tconst writeTokens = \\\"cache_creation_input_tokens\\\" in chunk.usage ? (chunk.usage as any).cache_creation_input_tokens : 0;\\n 87 | \\n----\\n 89 | \\t\\t\\t\\t\\ttype: \\\"usage\\\",\\n 90 | \\t\\t\\t\\t\\tinputTokens: chunk.usage.prompt_tokens || 0,\\n 91 | \\t\\t\\t\\t\\toutputTokens: chunk.usage.completion_tokens || 0,\\n 92 | \\t\\t\\t\\t\\tcacheReadTokens: readTokens,\\n----\"}",
  "partial": false
}
[2025-06-13T05:00:02.337Z] {
  "ts": 1749790802303,
  "type": "say",
  "say": "api_req_started",
  "text": "{\"request\":\"[search_files for 'CloudService|refresh|token|auth' in '*.ts'] Result:\\n\\nShowing first 300 of 300+ results. Use a more specific search if necessary.\\n\\n# apps/web-evals/src/lib/formatters.ts\\n 30 | \\n 31 | export const formatTokens = (tokens: number) => {\\n 32 | \\tif (tokens < 1000) {\\n 33 | \\t\\treturn tokens.toString()\\n 34 | \\t}\\n 35 | \\n 36 | \\tif (tokens < 1000000) {\\n 37 | \\t\\treturn `${(tokens / 1000).toFixed(1)}k`\\n 38 | \\t}\\n 39 | \\n 40 | \\tif (tokens < 1000000000) {\\n 41 | \\t\\treturn `${(tokens / 1000000).toFixed(1)}M`\\n 42 | \\t}\\n 43 | \\n 44 | \\treturn `${(tokens / 1000000000).toFixed(1)}B`\\n 45 | }\\n----\\n\\n# src/services/ripgrep/index.ts\\n101 | \\t\\tconst rgProcess = childProcess.spawn(bin, args)\\n102 | \\t\\t// cross-platform alternative to head, which is ripgrep author's recommendation for limiting output.\\n103 | \\t\\tconst rl = readline.createInterface({\\n----\\n\\n# apps/web-roo-code/src/lib/index.ts\\n  3 | export * from \\\"./format-score\\\"\\n  4 | export * from \\\"./format-tokens\\\"\\n----\\n\\n# apps/web-roo-code/src/lib/format-tokens.ts\\n  1 | export const formatTokens = (tokens: number, decimals = 0) => {\\n  2 | \\tif (tokens < 1000) {\\n  3 | \\t\\treturn tokens.toString()\\n  4 | \\t}\\n  5 | \\n  6 | \\tif (tokens < 1000000) {\\n  7 | \\t\\treturn `${(tokens / 1000).toFixed(decimals)}K`\\n  8 | \\t}\\n  9 | \\n 10 | \\tif (tokens < 1000000000) {\\n 11 | \\t\\treturn `${(tokens / 1000000).toFixed(decimals)}M`\\n 12 | \\t}\\n 13 | \\n 14 | \\treturn `${(tokens / 1000000000).toFixed(decimals)}B`\\n 15 | }\\n----\\n\\n# apps/web-roo-code/src/lib/hooks/use-open-router-models.ts\\n 19 | \\t\\t.object({\\n 20 | \\t\\t\\tmax_completion_tokens: z.number().nullish(),\\n 21 | \\t\\t})\\n----\\n 53 | \\t\\t\\tmodelInfo: {\\n 54 | \\t\\t\\t\\tmaxTokens: rawModel.top_provider?.max_completion_tokens ?? undefined,\\n 55 | \\t\\t\\t\\tcontextWindow: rawModel.context_length,\\n----\\n\\n# src/services/mcp/__tests__/McpHub.test.ts\\n 50 | \\t\\t\\tscheme: \\\"file\\\",\\n 51 | \\t\\t\\tauthority: \\\"\\\",\\n 52 | \\t\\t\\tpath: \\\"/test/path\\\",\\n----\\n\\n# src/services/mcp/McpHub.ts\\n1092 | \\n1093 | \\tpublic async refreshAllConnections(): Promise<void> {\\n1094 | \\t\\tif (this.isConnecting) {\\n1095 | \\t\\t\\tvscode.window.showInformationMessage(t(\\\"mcp:info.already_refreshing\\\"))\\n1096 | \\t\\t\\treturn\\n----\\n1099 | \\t\\tthis.isConnecting = true\\n1100 | \\t\\tvscode.window.showInformationMessage(t(\\\"mcp:info.refreshing_all\\\"))\\n1101 | \\n----\\n1151 | \\n1152 | \\t\\t\\tvscode.window.showInformationMessage(t(\\\"mcp:info.all_refreshed\\\"))\\n1153 | \\t\\t} catch (error) {\\n1154 | \\t\\t\\tthis.showErrorMessage(\\\"Failed to refresh MCP servers\\\", error)\\n1155 | \\t\\t} finally {\\n----\\n1244 | \\n1245 | \\t\\t\\t\\t\\t// Only refresh capabilities if connected\\n1246 | \\t\\t\\t\\t\\tif (connection.server.status === \\\"connected\\\") {\\n----\\n1254 | \\t\\t\\t\\t} catch (error) {\\n1255 | \\t\\t\\t\\t\\tconsole.error(`Failed to refresh capabilities for ${serverName}:`, error)\\n1256 | \\t\\t\\t\\t}\\n----\\n\\n# src/services/code-index/embedders/openai.ts\\n 52 | \\t\\t\\t\\t\\tconsole.warn(\\n 53 | \\t\\t\\t\\t\\t\\t`Text at index ${i} exceeds maximum token limit (${itemTokens} > ${MAX_ITEM_TOKENS}). Skipping.`,\\n 54 | \\t\\t\\t\\t\\t)\\n----\\n108 | \\t\\t\\t\\t\\tusage: {\\n109 | \\t\\t\\t\\t\\t\\tpromptTokens: response.usage?.prompt_tokens || 0,\\n110 | \\t\\t\\t\\t\\t\\ttotalTokens: response.usage?.total_tokens || 0,\\n111 | \\t\\t\\t\\t\\t},\\n----\\n\\n# src/services/code-index/embedders/openai-compatible.ts\\n 18 | \\tusage?: {\\n 19 | \\t\\tprompt_tokens?: number\\n 20 | \\t\\ttotal_tokens?: number\\n 21 | \\t}\\n----\\n 34 | \\t * @param baseUrl The base URL for the OpenAI-compatible API endpoint\\n 35 | \\t * @param apiKey The API key for authentication\\n 36 | \\t * @param modelId Optional model identifier (defaults to \\\"text-embedding-3-small\\\")\\n----\\n 75 | \\t\\t\\t\\t\\tconsole.warn(\\n 76 | \\t\\t\\t\\t\\t\\t`Text at index ${i} exceeds maximum token limit (${itemTokens} > ${MAX_ITEM_TOKENS}). Skipping.`,\\n 77 | \\t\\t\\t\\t\\t)\\n----\\n156 | \\t\\t\\t\\t\\tusage: {\\n157 | \\t\\t\\t\\t\\t\\tpromptTokens: response.usage?.prompt_tokens || 0,\\n158 | \\t\\t\\t\\t\\t\\ttotalTokens: response.usage?.total_tokens || 0,\\n159 | \\t\\t\\t\\t\\t},\\n----\\n\\n# src/services/code-index/embedders/__tests__/openai-compatible.spec.ts\\n103 | \\t\\t\\t\\tdata: [{ embedding: [0.1, 0.2, 0.3] }],\\n104 | \\t\\t\\t\\tusage: { prompt_tokens: 10, total_tokens: 15 },\\n105 | \\t\\t\\t}\\n----\\n124 | \\t\\t\\t\\tdata: [{ embedding: [0.1, 0.2, 0.3] }, { embedding: [0.4, 0.5, 0.6] }],\\n125 | \\t\\t\\t\\tusage: { prompt_tokens: 20, total_tokens: 30 },\\n126 | \\t\\t\\t}\\n----\\n149 | \\t\\t\\t\\tdata: [{ embedding: [0.1, 0.2, 0.3] }],\\n150 | \\t\\t\\t\\tusage: { prompt_tokens: 10, total_tokens: 15 },\\n151 | \\t\\t\\t}\\n----\\n194 | \\t\\t\\t\\t\\tdata: [{ embedding: base64String }], // Base64 string instead of array\\n195 | \\t\\t\\t\\t\\tusage: { prompt_tokens: 10, total_tokens: 15 },\\n196 | \\t\\t\\t\\t}\\n----\\n226 | \\t\\t\\t\\t\\tdata: [{ embedding: base64String1 }, { embedding: base64String2 }],\\n227 | \\t\\t\\t\\t\\tusage: { prompt_tokens: 20, total_tokens: 30 },\\n228 | \\t\\t\\t\\t}\\n----\\n253 | \\t\\t\\t\\t\\t],\\n254 | \\t\\t\\t\\t\\tusage: { prompt_tokens: 20, total_tokens: 30 },\\n255 | \\t\\t\\t\\t}\\n----\\n270 | \\t\\t/**\\n271 | \\t\\t * Test batching logic when texts exceed token limits\\n272 | \\t\\t */\\n----\\n283 | \\t\\t\\t\\t\\t],\\n284 | \\t\\t\\t\\t\\tusage: { prompt_tokens: 10, total_tokens: 15 },\\n285 | \\t\\t\\t\\t})\\n----\\n299 | \\t\\t\\t\\t\\tdata: [{ embedding: [0.1, 0.2, 0.3] }, { embedding: [0.4, 0.5, 0.6] }],\\n300 | \\t\\t\\t\\t\\tusage: { prompt_tokens: 10, total_tokens: 15 },\\n301 | \\t\\t\\t\\t}\\n----\\n306 | \\t\\t\\t\\t// Should warn about oversized text\\n307 | \\t\\t\\t\\texpect(console.warn).toHaveBeenCalledWith(expect.stringContaining(\\\"exceeds maximum token limit\\\"))\\n308 | \\n----\\n317 | \\t\\t\\t\\t\\tdata: [{ embedding: [0.1, 0.2, 0.3] }, { embedding: [0.4, 0.5, 0.6] }],\\n318 | \\t\\t\\t\\t\\tusage: { prompt_tokens: 10, total_tokens: 15 },\\n319 | \\t\\t\\t\\t})\\n----\\n354 | \\t\\t\\t\\t\\t\\tdata: [{ embedding: base64String }],\\n355 | \\t\\t\\t\\t\\t\\tusage: { prompt_tokens: 10, total_tokens: 15 },\\n356 | \\t\\t\\t\\t\\t})\\n----\\n375 | \\t\\t\\t\\tconst testTexts = [\\\"Hello world\\\"]\\n376 | \\t\\t\\t\\tconst authError = new Error(\\\"Unauthorized\\\")\\n377 | \\t\\t\\t\\t;(authError as any).status = 401\\n378 | \\n379 | \\t\\t\\t\\tmockEmbeddingsCreate.mockRejectedValue(authError)\\n380 | \\n----\\n452 | \\t\\t\\t\\t\\tdata: null,\\n453 | \\t\\t\\t\\t\\tusage: { prompt_tokens: 10, total_tokens: 15 },\\n454 | \\t\\t\\t\\t}\\n----\\n494 | \\t\\t\\t\\t\\tusage: {\\n495 | \\t\\t\\t\\t\\t\\tprompt_tokens: 2,\\n496 | \\t\\t\\t\\t\\t\\ttotal_tokens: 2,\\n497 | \\t\\t\\t\\t\\t},\\n----\\n\\n# apps/web-evals/src/hooks/use-run-status.ts\\n 16 | \\ttasks: (Task & { taskMetrics: TaskMetrics | null })[] | undefined\\n 17 | \\ttokenUsage: Map<number, TokenUsage & { duration?: number }>\\n 18 | \\tusageUpdatedAt: number | undefined\\n----\\n 24 | \\n 25 | \\tconst tokenUsage = useRef<Map<number, TokenUsage & { duration?: number }>>(new Map())\\n 26 | \\tconst startTimes = useRef<Map<number, number>>(new Map())\\n----\\n 79 | \\t\\t\\t\\tconst duration = startTime ? Date.now() - startTime : undefined\\n 80 | \\t\\t\\t\\ttokenUsage.current.set(taskId, { ...payload[1], duration })\\n 81 | \\t\\t\\t\\tsetUsageUpdatedAt(Date.now())\\n----\\n 97 | \\t\\ttasks,\\n 98 | \\t\\ttokenUsage: tokenUsage.current,\\n 99 | \\t\\tusageUpdatedAt,\\n----\\n\\n# src/activate/handleUri.ts\\n  2 | \\n  3 | import { CloudService } from \\\"@roo-code/cloud\\\"\\n  4 | \\n----\\n 37 | \\t\\t}\\n 38 | \\t\\tcase \\\"/auth/clerk/callback\\\": {\\n 39 | \\t\\t\\tconst code = query.get(\\\"code\\\")\\n 40 | \\t\\t\\tconst state = query.get(\\\"state\\\")\\n 41 | \\t\\t\\tawait CloudService.instance.handleAuthCallback(code, state)\\n 42 | \\t\\t\\tbreak\\n----\\n\\n# src/services/marketplace/types.ts\\n 20 | \\tdescription: string\\n 21 | \\tauthor?: string\\n 22 | \\tauthorUrl?: string\\n 23 | \\ttags?: string[]\\n----\\n 45 | \\tdescription: string\\n 46 | \\tauthor?: string\\n 47 | \\tauthorUrl?: string\\n 48 | \\turl: string // Required url field\\n----\\n 62 | \\ttype: MarketplaceItemType\\n 63 | \\tauthor?: string\\n 64 | \\tauthorUrl?: string\\n 65 | \\turl?: string // Optional - only MCPs have url\\n----\\n\\n# src/services/tree-sitter/__tests__/fixtures/sample-embedded_template.ts\\n 35 | <div class=\\\"container\\\">\\n 36 |   <% if user.authenticated? %>\\n 37 |     <h1>Welcome, <%= user.name %></h1>\\n----\\n\\n# src/services/tree-sitter/__tests__/parseSourceCodeDefinitions.embedded_template.test.ts\\n 43 | \\tit(\\\"should detect control structures\\\", () => {\\n 44 | \\t\\texpect(parseResult).toMatch(/\\\\d+--\\\\d+ \\\\|\\\\s+<% if user\\\\.authenticated\\\\? %>/)\\n 45 | \\t\\texpect(parseResult).toMatch(/\\\\d+--\\\\d+ \\\\|\\\\s+<% user\\\\.posts\\\\.each do \\\\|post\\\\| %>/)\\n----\\n\\n# src/services/marketplace/schemas.ts\\n 35 | \\ttype: marketplaceItemTypeSchema,\\n 36 | \\tauthor: z.string().optional(),\\n 37 | \\tauthorUrl: z.string().url(\\\"Author URL must be a valid URL\\\").optional(),\\n 38 | \\ttags: z.array(z.string()).optional(),\\n----\\n 61 | \\tdescription: z.string(),\\n 62 | \\tauthor: z.string().optional(),\\n 63 | \\tauthorUrl: z.string().url().optional(),\\n 64 | \\ttags: z.array(z.string()).optional(),\\n----\\n 72 | \\tdescription: z.string(),\\n 73 | \\tauthor: z.string().optional(),\\n 74 | \\tauthorUrl: z.string().url().optional(),\\n 75 | \\turl: z.string().url(), // Required url field\\n----\\n\\n# src/extension.ts\\n 14 | \\n 15 | import { CloudService } from \\\"@roo-code/cloud\\\"\\n 16 | import { TelemetryService, PostHogTelemetryClient } from \\\"@roo-code/telemetry\\\"\\n----\\n 75 | \\t// Initialize Roo Code Cloud service.\\n 76 | \\tawait CloudService.createInstance(context, {\\n 77 | \\t\\tstateChanged: () => ClineProvider.getVisibleInstance()?.postStateToWebview(),\\n----\\n\\n# src/workers/countTokens.ts\\n  4 | \\n  5 | import { tiktoken } from \\\"../utils/tiktoken\\\"\\n  6 | \\n----\\n 10 | \\ttry {\\n 11 | \\t\\tconst count = await tiktoken(content)\\n 12 | \\t\\treturn { success: true, count }\\n----\\n\\n# src/api/index.ts\\n 50 | \\t/**\\n 51 | \\t * Counts tokens for content blocks\\n 52 | \\t * All providers extend BaseProvider which provides a default tiktoken implementation,\\n 53 | \\t * but they can override this to use their native token counting endpoints\\n 54 | \\t *\\n 55 | \\t * @param content The content to count tokens for\\n 56 | \\t * @returns A promise resolving to the token count\\n 57 | \\t */\\n----\\n\\n# src/extension/api.ts\\n219 | \\n220 | \\t\\t\\tcline.on(\\\"taskCompleted\\\", async (_, tokenUsage, toolUsage) => {\\n221 | \\t\\t\\t\\tthis.emit(RooCodeEventName.TaskCompleted, cline.taskId, tokenUsage, toolUsage)\\n222 | \\t\\t\\t\\tthis.taskMap.delete(cline.taskId)\\n----\\n224 | \\t\\t\\t\\tawait this.fileLog(\\n225 | \\t\\t\\t\\t\\t`[${new Date().toISOString()}] taskCompleted -> ${cline.taskId} | ${JSON.stringify(tokenUsage, null, 2)} | ${JSON.stringify(toolUsage, null, 2)}\\\\n`,\\n226 | \\t\\t\\t\\t)\\n----\\n\\n# src/api/providers/ollama.ts\\n 79 | \\t\\t\\t\\ttype: \\\"usage\\\",\\n 80 | \\t\\t\\t\\tinputTokens: lastUsage?.prompt_tokens || 0,\\n 81 | \\t\\t\\t\\toutputTokens: lastUsage?.completion_tokens || 0,\\n 82 | \\t\\t\\t}\\n----\\n\\n# src/core/checkpoints/index.ts\\n215 | \\t\\t\\t\\tJSON.stringify({\\n216 | \\t\\t\\t\\t\\ttokensIn: totalTokensIn,\\n217 | \\t\\t\\t\\t\\ttokensOut: totalTokensOut,\\n218 | \\t\\t\\t\\t\\tcacheWrites: totalCacheWrites,\\n----\\n\\n# src/utils/git.ts\\n 11 | \\tsubject: string\\n 12 | \\tauthor: string\\n 13 | \\tdate: string\\n----\\n 57 | \\t\\t\\tconst { stdout: hashStdout } = await execAsync(\\n 58 | \\t\\t\\t\\t`git log -n 10 --format=\\\"%H%n%h%n%s%n%an%n%ad\\\" --date=short ` + `--author-date-order ${query}`,\\n 59 | \\t\\t\\t\\t{ cwd },\\n----\\n 79 | \\t\\t\\t\\tsubject: lines[i + 2],\\n 80 | \\t\\t\\t\\tauthor: lines[i + 3],\\n 81 | \\t\\t\\t\\tdate: lines[i + 4],\\n----\\n107 | \\t\\t})\\n108 | \\t\\tconst [fullHash, shortHash, subject, author, date, body] = info.trim().split(\\\"\\\\n\\\")\\n109 | \\n----\\n115 | \\t\\t\\t`Commit: ${shortHash} (${fullHash})`,\\n116 | \\t\\t\\t`Author: ${author}`,\\n117 | \\t\\t\\t`Date: ${date}`,\\n----\\n\\n# src/api/providers/__tests__/bedrock-invokedModelId.spec.ts\\n176 | \\n177 | \\t\\t// The last usage event should have the token counts from the metadata\\n178 | \\t\\tconst lastUsageEvent = usageEvents[usageEvents.length - 1]\\n179 | \\t\\t// Expect the usage event to include all token information\\n180 | \\t\\texpect(lastUsageEvent).toMatchObject({\\n----\\n183 | \\t\\t\\toutputTokens: 200,\\n184 | \\t\\t\\t// Cache tokens may be present with default values\\n185 | \\t\\t\\tcacheReadTokens: expect.any(Number),\\n----\\n\\n# src/api/providers/__tests__/groq.spec.ts\\n101 | \\t\\t\\t\\t\\t\\t\\tdone: false,\\n102 | \\t\\t\\t\\t\\t\\t\\tvalue: { choices: [{ delta: {} }], usage: { prompt_tokens: 10, completion_tokens: 20 } },\\n103 | \\t\\t\\t\\t\\t\\t})\\n----\\n139 | \\t\\t\\t\\tmodel: modelId,\\n140 | \\t\\t\\t\\tmax_tokens: modelInfo.maxTokens,\\n141 | \\t\\t\\t\\ttemperature: 0.5,\\n----\\n\\n# src/api/providers/__tests__/gemini.spec.ts\\n167 | \\tdescribe(\\\"calculateCost\\\", () => {\\n168 | \\t\\t// Mock ModelInfo based on gemini-1.5-flash-latest pricing (per 1M tokens)\\n169 | \\t\\t// Removed 'id' and 'name' as they are not part of ModelInfo type directly\\n170 | \\t\\tconst mockInfo: ModelInfo = {\\n171 | \\t\\t\\tinputPrice: 0.125, // $/1M tokens\\n172 | \\t\\t\\toutputPrice: 0.375, // $/1M tokens\\n173 | \\t\\t\\tcacheWritesPrice: 0.125, // Assume same as input for test\\n----\\n179 | \\n180 | \\t\\tit(\\\"should calculate cost correctly based on input and output tokens\\\", () => {\\n181 | \\t\\t\\tconst inputTokens = 10000 // Use larger numbers for per-million pricing\\n----\\n190 | \\n191 | \\t\\tit(\\\"should return 0 if token counts are zero\\\", () => {\\n192 | \\t\\t\\t// Note: The method expects numbers, not undefined. Passing undefined would be a type error.\\n193 | \\t\\t\\t// The calculateCost method itself returns undefined if prices are missing, but 0 if tokens are 0 and prices exist.\\n194 | \\t\\t\\texpect(handler.calculateCost({ info: mockInfo, inputTokens: 0, outputTokens: 0 })).toBe(0)\\n----\\n196 | \\n197 | \\t\\tit(\\\"should handle only input tokens\\\", () => {\\n198 | \\t\\t\\tconst inputTokens = 5000\\n----\\n203 | \\n204 | \\t\\tit(\\\"should handle only output tokens\\\", () => {\\n205 | \\t\\t\\tconst outputTokens = 15000\\n----\\n210 | \\n211 | \\t\\tit(\\\"should calculate cost with cache write tokens\\\", () => {\\n212 | \\t\\t\\tconst inputTokens = 10000\\n----\\n227 | \\n228 | \\t\\tit(\\\"should calculate cost with cache read tokens\\\", () => {\\n229 | \\t\\t\\tconst inputTokens = 10000 // Total logical input\\n----\\n\\n# src/core/condense/index.ts\\n 55 | \\tcost: number // The cost of the summarization operation\\n 56 | \\tnewContextTokens?: number // The number of tokens in the context for the next API request\\n 57 | \\terror?: string // Populated iff the operation fails: error message shown to the user on failure (see Task.ts)\\n----\\n 63 |  * @param {ApiMessage[]} messages - The conversation messages\\n 64 |  * @param {ApiHandler} apiHandler - The API handler to use for token counting.\\n 65 |  * @param {string} systemPrompt - The system prompt for API requests, which should be considered in the context token count\\n 66 |  * @param {string} taskId - The task ID for the conversation, used for telemetry\\n----\\n 73 |  * @param {ApiMessage[]} messages - The conversation messages\\n 74 |  * @param {ApiHandler} apiHandler - The API handler to use for token counting (fallback if condensingApiHandler not provided)\\n 75 |  * @param {string} systemPrompt - The system prompt for API requests (fallback if customCondensingPrompt not provided)\\n 76 |  * @param {string} taskId - The task ID for the conversation, used for telemetry\\n 77 |  * @param {number} prevContextTokens - The number of tokens currently in the context, used to ensure we don't grow the context\\n 78 |  * @param {boolean} isAutomaticTrigger - Whether the summarization is triggered automatically\\n----\\n186 | \\n187 | \\t// Count the tokens in the context for the next API request\\n188 | \\t// We only estimate the tokens in summaryMesage if outputTokens is 0, otherwise we use outputTokens\\n189 | \\tconst systemPromptMessage: ApiMessage = { role: \\\"user\\\", content: systemPrompt }\\n----\\n\\n# src/api/providers/__tests__/vertex.spec.ts\\n 49 | \\t\\t\\t// The test expects 4 chunks:\\n 50 | \\t\\t\\t// 1. Usage chunk with input tokens\\n 51 | \\t\\t\\t// 2. Text chunk with \\\"Gemini response part 1\\\"\\n 52 | \\t\\t\\t// 3. Text chunk with \\\" part 2\\\"\\n 53 | \\t\\t\\t// 4. Usage chunk with output tokens\\n 54 | \\n----\\n\\n# src/api/providers/__tests__/ollama.spec.ts\\n 28 | \\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 29 | \\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 30 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 31 | \\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 32 | \\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 54 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 55 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 56 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 57 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 58 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n\\n# src/core/environment/getEnvironmentDetails.ts\\n192 | \\n193 | \\t// Add context tokens information.\\n194 | \\tconst { contextTokens, totalCost } = getApiMetrics(cline.clineMessages)\\n----\\n\\n# src/api/providers/__tests__/openai.spec.ts\\n 29 | \\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 30 | \\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 31 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 32 | \\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 33 | \\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 55 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 56 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 57 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 58 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 59 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n\\n# src/api/providers/__tests__/anthropic.spec.ts\\n 19 | \\t\\t\\t\\t\\t\\tusage: {\\n 20 | \\t\\t\\t\\t\\t\\t\\tinput_tokens: 10,\\n 21 | \\t\\t\\t\\t\\t\\t\\toutput_tokens: 5,\\n 22 | \\t\\t\\t\\t\\t\\t},\\n----\\n 30 | \\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 31 | \\t\\t\\t\\t\\t\\t\\t\\t\\tinput_tokens: 100,\\n 32 | \\t\\t\\t\\t\\t\\t\\t\\t\\toutput_tokens: 50,\\n 33 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcache_creation_input_tokens: 20,\\n 34 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcache_read_input_tokens: 10,\\n 35 | \\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n105 | \\n106 | \\t\\tit(\\\"use apiKey for passing token if anthropicUseAuthToken is not set\\\", () => {\\n107 | \\t\\t\\tconst handlerWithCustomUrl = new AnthropicHandler({\\n----\\n112 | \\t\\t\\texpect(mockAnthropicConstructor.mock.calls[0]![0]!.apiKey).toEqual(\\\"test-api-key\\\")\\n113 | \\t\\t\\texpect(mockAnthropicConstructor.mock.calls[0]![0]!.authToken).toBeUndefined()\\n114 | \\t\\t})\\n115 | \\n116 | \\t\\tit(\\\"use apiKey for passing token if anthropicUseAuthToken is set but custom base URL is not given\\\", () => {\\n117 | \\t\\t\\tconst handlerWithCustomUrl = new AnthropicHandler({\\n----\\n123 | \\t\\t\\texpect(mockAnthropicConstructor.mock.calls[0]![0]!.apiKey).toEqual(\\\"test-api-key\\\")\\n124 | \\t\\t\\texpect(mockAnthropicConstructor.mock.calls[0]![0]!.authToken).toBeUndefined()\\n125 | \\t\\t})\\n126 | \\n127 | \\t\\tit(\\\"use authToken for passing token if both of anthropicBaseUrl and anthropicUseAuthToken are set\\\", () => {\\n128 | \\t\\t\\tconst customBaseUrl = \\\"https://custom.anthropic.com\\\"\\n----\\n135 | \\t\\t\\texpect(mockAnthropicConstructor).toHaveBeenCalledTimes(1)\\n136 | \\t\\t\\texpect(mockAnthropicConstructor.mock.calls[0]![0]!.authToken).toEqual(\\\"test-api-key\\\")\\n137 | \\t\\t\\texpect(mockAnthropicConstructor.mock.calls[0]![0]!.apiKey).toBeUndefined()\\n----\\n190 | \\t\\t\\t\\tmessages: [{ role: \\\"user\\\", content: \\\"Test prompt\\\" }],\\n191 | \\t\\t\\t\\tmax_tokens: 8192,\\n192 | \\t\\t\\t\\ttemperature: 0,\\n----\\n\\n# src/api/providers/__tests__/xai.test.ts\\n228 | \\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n229 | \\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n230 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 20,\\n231 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcache_read_input_tokens: 5,\\n232 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcache_creation_input_tokens: 15,\\n233 | \\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n284 | \\t\\t\\t\\tmodel: modelId,\\n285 | \\t\\t\\t\\tmax_tokens: modelInfo.maxTokens,\\n286 | \\t\\t\\t\\ttemperature: 0,\\n----\\n\\n# src/api/providers/__tests__/openrouter.spec.ts\\n132 | \\t\\t\\t\\t\\t\\tchoices: [{ delta: {} }],\\n133 | \\t\\t\\t\\t\\t\\tusage: { prompt_tokens: 10, completion_tokens: 20, cost: 0.001 },\\n134 | \\t\\t\\t\\t\\t}\\n----\\n162 | \\t\\t\\t\\texpect.objectContaining({\\n163 | \\t\\t\\t\\t\\tmax_tokens: 8192,\\n164 | \\t\\t\\t\\t\\tmessages: [\\n----\\n285 | \\t\\t\\t\\tmodel: mockOptions.openRouterModelId,\\n286 | \\t\\t\\t\\tmax_tokens: 8192,\\n287 | \\t\\t\\t\\tthinking: undefined,\\n----\\n\\n# src/api/providers/unbound.ts\\n 22 | interface UnboundUsage extends OpenAI.CompletionUsage {\\n 23 | \\tcache_creation_input_tokens?: number\\n 24 | \\tcache_read_input_tokens?: number\\n 25 | }\\n----\\n 73 | \\n 74 | \\t\\t// Required by Anthropic; other providers default to max tokens allowed.\\n 75 | \\t\\tlet maxTokens: number | undefined\\n----\\n 82 | \\t\\t\\tmodel: modelId.split(\\\"/\\\")[1],\\n 83 | \\t\\t\\tmax_tokens: maxTokens,\\n 84 | \\t\\t\\tmessages: openAiMessages,\\n----\\n111 | \\t\\t\\t\\t\\ttype: \\\"usage\\\",\\n112 | \\t\\t\\t\\t\\tinputTokens: usage.prompt_tokens || 0,\\n113 | \\t\\t\\t\\t\\toutputTokens: usage.completion_tokens || 0,\\n114 | \\t\\t\\t\\t}\\n115 | \\n116 | \\t\\t\\t\\t// Only add cache tokens if they exist.\\n117 | \\t\\t\\t\\tif (usage.cache_creation_input_tokens) {\\n118 | \\t\\t\\t\\t\\tusageData.cacheWriteTokens = usage.cache_creation_input_tokens\\n119 | \\t\\t\\t\\t}\\n120 | \\n121 | \\t\\t\\t\\tif (usage.cache_read_input_tokens) {\\n122 | \\t\\t\\t\\t\\tusageData.cacheReadTokens = usage.cache_read_input_tokens\\n123 | \\t\\t\\t\\t}\\n----\\n146 | \\t\\t\\tif (modelId.startsWith(\\\"anthropic/\\\")) {\\n147 | \\t\\t\\t\\trequestOptions.max_tokens = info.maxTokens\\n148 | \\t\\t\\t}\\n----\\n\\n# src/api/providers/vscode-lm.ts\\n128 | \\t\\t\\t\\tmaxInputTokens: 8192,\\n129 | \\t\\t\\t\\tsendRequest: async (_messages, _options, _token) => {\\n130 | \\t\\t\\t\\t\\t// Provide a minimal implementation\\n----\\n179 | \\t * Implements the ApiHandler countTokens interface method\\n180 | \\t * Provides token counting for Anthropic content blocks\\n181 | \\t *\\n182 | \\t * @param content The content blocks to count tokens for\\n183 | \\t * @returns A promise resolving to the token count\\n184 | \\t */\\n185 | \\toverride async countTokens(content: Array<Anthropic.Messages.ContentBlockParam>): Promise<number> {\\n186 | \\t\\t// Convert Anthropic content blocks to a string for VSCode LM token counting\\n187 | \\t\\tlet textContent = \\\"\\\"\\n----\\n201 | \\t/**\\n202 | \\t * Private implementation of token counting used internally by VsCodeLmHandler\\n203 | \\t */\\n----\\n206 | \\t\\tif (!this.client) {\\n207 | \\t\\t\\tconsole.warn(\\\"Roo Code <Language Model API>: No client available for token counting\\\")\\n208 | \\t\\t\\treturn 0\\n----\\n211 | \\t\\tif (!this.currentRequestCancellation) {\\n212 | \\t\\t\\tconsole.warn(\\\"Roo Code <Language Model API>: No cancellation token available for token counting\\\")\\n213 | \\t\\t\\treturn 0\\n----\\n217 | \\t\\tif (!text) {\\n218 | \\t\\t\\tconsole.debug(\\\"Roo Code <Language Model API>: Empty text provided for token counting\\\")\\n219 | \\t\\t\\treturn 0\\n----\\n223 | \\t\\t\\t// Handle different input types\\n224 | \\t\\t\\tlet tokenCount: number\\n225 | \\n226 | \\t\\t\\tif (typeof text === \\\"string\\\") {\\n227 | \\t\\t\\t\\ttokenCount = await this.client.countTokens(text, this.currentRequestCancellation.token)\\n228 | \\t\\t\\t} else if (text instanceof vscode.LanguageModelChatMessage) {\\n----\\n233 | \\t\\t\\t\\t}\\n234 | \\t\\t\\t\\ttokenCount = await this.client.countTokens(text, this.currentRequestCancellation.token)\\n235 | \\t\\t\\t} else {\\n236 | \\t\\t\\t\\tconsole.warn(\\\"Roo Code <Language Model API>: Invalid input type for token counting\\\")\\n237 | \\t\\t\\t\\treturn 0\\n----\\n240 | \\t\\t\\t// Validate the result\\n241 | \\t\\t\\tif (typeof tokenCount !== \\\"number\\\") {\\n242 | \\t\\t\\t\\tconsole.warn(\\\"Roo Code <Language Model API>: Non-numeric token count received:\\\", tokenCount)\\n243 | \\t\\t\\t\\treturn 0\\n----\\n245 | \\n246 | \\t\\t\\tif (tokenCount < 0) {\\n247 | \\t\\t\\t\\tconsole.warn(\\\"Roo Code <Language Model API>: Negative token count received:\\\", tokenCount)\\n248 | \\t\\t\\t\\treturn 0\\n----\\n250 | \\n251 | \\t\\t\\treturn tokenCount\\n252 | \\t\\t} catch (error) {\\n----\\n278 | \\n279 | \\t\\treturn systemTokens + messageTokens.reduce((sum: number, tokens: number): number => sum + tokens, 0)\\n280 | \\t}\\n----\\n357 | \\n358 | \\t\\t// Initialize cancellation token for the request\\n359 | \\t\\tthis.currentRequestCancellation = new vscode.CancellationTokenSource()\\n360 | \\n361 | \\t\\t// Calculate input tokens before starting the stream\\n362 | \\t\\tconst totalInputTokens: number = await this.calculateTotalInputTokens(systemPrompt, vsCodeLmMessages)\\n363 | \\n364 | \\t\\t// Accumulate the text and count at the end of the stream to reduce token counting overhead.\\n365 | \\t\\tlet accumulatedText: string = \\\"\\\"\\n----\\n378 | \\t\\t\\t\\trequestOptions,\\n379 | \\t\\t\\t\\tthis.currentRequestCancellation.token,\\n380 | \\t\\t\\t)\\n----\\n446 | \\n447 | \\t\\t\\t// Count tokens in the accumulated text after stream completion\\n448 | \\t\\t\\tconst totalOutputTokens: number = await this.internalCountTokens(accumulatedText)\\n----\\n511 | \\t\\t\\tconst modelInfo: ModelInfo = {\\n512 | \\t\\t\\t\\tmaxTokens: -1, // Unlimited tokens by default\\n513 | \\t\\t\\t\\tcontextWindow:\\n----\\n548 | \\t\\t\\t\\t{},\\n549 | \\t\\t\\t\\tnew vscode.CancellationTokenSource().token,\\n550 | \\t\\t\\t)\\n----\\n\\n# src/core/condense/__tests__/index.test.ts\\n203 | \\n204 | \\t\\t// Check the cost and token counts\\n205 | \\t\\texpect(result.cost).toBe(0.05)\\n206 | \\t\\texpect(result.summary).toBe(\\\"This is a summary\\\")\\n207 | \\t\\texpect(result.newContextTokens).toBe(250) // 150 output tokens + 100 from countTokens\\n208 | \\t\\texpect(result.error).toBeUndefined()\\n----\\n317 | \\t\\t// Check the newContextTokens calculation includes system prompt\\n318 | \\t\\texpect(result.newContextTokens).toBe(300) // 200 output tokens + 100 from countTokens\\n319 | \\t\\texpect(result.cost).toBe(0.06)\\n----\\n323 | \\n324 | \\tit(\\\"should return error when new context tokens >= previous context tokens\\\", async () => {\\n325 | \\t\\tconst messages: ApiMessage[] = [\\n----\\n336 | \\t\\tconst streamWithLargeTokens = (async function* () {\\n337 | \\t\\t\\tyield { type: \\\"text\\\" as const, text: \\\"This is a very long summary that uses many tokens\\\" }\\n338 | \\t\\t\\tyield { type: \\\"usage\\\" as const, totalCost: 0.08, outputTokens: 500 }\\n----\\n364 | \\n365 | \\tit(\\\"should successfully summarize when new context tokens < previous context tokens\\\", async () => {\\n366 | \\t\\tconst messages: ApiMessage[] = [\\n----\\n375 | \\n376 | \\t\\t// Create a stream that produces a summary with reasonable token count\\n377 | \\t\\tconst streamWithSmallTokens = (async function* () {\\n----\\n401 | \\t\\texpect(result.error).toBeUndefined()\\n402 | \\t\\texpect(result.newContextTokens).toBe(80) // 50 output tokens + 30 from countTokens\\n403 | \\t\\texpect(result.newContextTokens).toBeLessThan(prevContextTokens)\\n----\\n\\n# src/api/providers/__tests__/lmstudio.test.ts\\n 25 | \\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 26 | \\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 27 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 28 | \\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 29 | \\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 51 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 52 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 53 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 54 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 55 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n\\n# src/api/providers/__tests__/bedrock-reasoning.test.ts\\n 94 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n 95 | \\t\\t\\t\\tbudget_tokens: 4096, // Uses the full modelMaxThinkingTokens value\\n 96 | \\t\\t\\t})\\n----\\n143 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n144 | \\t\\t\\t\\tbudget_tokens: 8192,\\n145 | \\t\\t\\t})\\n----\\n266 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n267 | \\t\\t\\t\\tbudget_tokens: 4096,\\n268 | \\t\\t\\t})\\n----\\n\\n# src/api/providers/lm-studio.ts\\n 40 | \\t\\t// -------------------------\\n 41 | \\t\\t// Track token usage\\n 42 | \\t\\t// -------------------------\\n----\\n 68 | \\t\\t} catch (err) {\\n 69 | \\t\\t\\tconsole.error(\\\"[LmStudio] Failed to count input tokens:\\\", err)\\n 70 | \\t\\t\\tinputTokens = 0\\n----\\n116 | \\t\\t\\t} catch (err) {\\n117 | \\t\\t\\t\\tconsole.error(\\\"[LmStudio] Failed to count output tokens:\\\", err)\\n118 | \\t\\t\\t\\toutputTokens = 0\\n----\\n\\n# src/utils/countTokens.ts\\n  4 | import { countTokensResultSchema } from \\\"../workers/types\\\"\\n  5 | import { tiktoken } from \\\"./tiktoken\\\"\\n  6 | \\n----\\n 27 | \\tif (!useWorker || !pool) {\\n 28 | \\t\\treturn tiktoken(content)\\n 29 | \\t}\\n----\\n 42 | \\t\\tconsole.error(error)\\n 43 | \\t\\treturn tiktoken(content)\\n 44 | \\t}\\n----\\n\\n# src/api/providers/__tests__/requesty.spec.ts\\n124 | \\t\\t\\t\\t\\t\\tusage: {\\n125 | \\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n126 | \\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 20,\\n127 | \\t\\t\\t\\t\\t\\t\\tprompt_tokens_details: {\\n128 | \\t\\t\\t\\t\\t\\t\\t\\tcaching_tokens: 5,\\n129 | \\t\\t\\t\\t\\t\\t\\t\\tcached_tokens: 2,\\n130 | \\t\\t\\t\\t\\t\\t\\t},\\n----\\n162 | \\t\\t\\t\\texpect.objectContaining({\\n163 | \\t\\t\\t\\t\\tmax_tokens: 8192,\\n164 | \\t\\t\\t\\t\\tmessages: [\\n----\\n204 | \\t\\t\\t\\tmodel: mockOptions.requestyModelId,\\n205 | \\t\\t\\t\\tmax_tokens: 8192,\\n206 | \\t\\t\\t\\tmessages: [{ role: \\\"system\\\", content: \\\"test prompt\\\" }],\\n----\\n\\n# src/api/providers/__tests__/vscode-lm.test.ts\\n 28 | \\t\\tCancellationTokenSource: jest.fn(() => ({\\n 29 | \\t\\t\\ttoken: {\\n 30 | \\t\\t\\t\\tisCancellationRequested: false,\\n----\\n\\n# src/api/providers/__tests__/chutes.spec.ts\\n 49 | \\t\\t\\t\\t\\tusage: {\\n 50 | \\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 51 | \\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 52 | \\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 53 | \\t\\t\\t\\t\\t},\\n----\\n103 | \\t\\t\\t\\t\\t],\\n104 | \\t\\t\\t\\t\\tusage: { prompt_tokens: 10, completion_tokens: 5 },\\n105 | \\t\\t\\t\\t}\\n----\\n211 | \\t\\t\\t\\t\\t\\t\\tdone: false,\\n212 | \\t\\t\\t\\t\\t\\t\\tvalue: { choices: [{ delta: {} }], usage: { prompt_tokens: 10, completion_tokens: 20 } },\\n213 | \\t\\t\\t\\t\\t\\t})\\n----\\n284 | \\t\\t\\t\\tmodel: modelId,\\n285 | \\t\\t\\t\\tmax_tokens: modelInfo.maxTokens,\\n286 | \\t\\t\\t\\ttemperature: 0.5,\\n----\\n\\n# src/api/providers/__tests__/openai-native.spec.ts\\n 29 | \\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 30 | \\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 31 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 32 | \\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 33 | \\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 55 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 56 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 57 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 58 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 59 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n145 | \\t\\t\\t\\t\\t\\tusage: {\\n146 | \\t\\t\\t\\t\\t\\t\\tprompt_tokens: 0,\\n147 | \\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 0,\\n148 | \\t\\t\\t\\t\\t\\t\\ttotal_tokens: 0,\\n149 | \\t\\t\\t\\t\\t\\t},\\n----\\n215 | \\t\\t\\t\\t{ choices: [{ delta: { content: \\\" there\\\" } }], usage: null },\\n216 | \\t\\t\\t\\t{ choices: [{ delta: { content: \\\"!\\\" } }], usage: { prompt_tokens: 10, completion_tokens: 5 } },\\n217 | \\t\\t\\t]\\n----\\n261 | \\t\\t\\t\\t{ choices: [{ delta: { content: null } }], usage: null },\\n262 | \\t\\t\\t\\t{ choices: [{ delta: { content: \\\"Hello\\\" } }], usage: { prompt_tokens: 10, completion_tokens: 5 } },\\n263 | \\t\\t\\t]\\n----\\n\\n# src/api/providers/__tests__/deepseek.test.ts\\n 28 | \\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 29 | \\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 30 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 31 | \\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 32 | \\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens_details: {\\n 33 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcache_miss_tokens: 8,\\n 34 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcached_tokens: 2,\\n 35 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 59 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 60 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 61 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 62 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 63 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens_details: {\\n 64 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcache_miss_tokens: 8,\\n 65 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcached_tokens: 2,\\n 66 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n261 | \\t\\t\\tconst usage = {\\n262 | \\t\\t\\t\\tprompt_tokens: 100,\\n263 | \\t\\t\\t\\tcompletion_tokens: 50,\\n264 | \\t\\t\\t\\ttotal_tokens: 150,\\n265 | \\t\\t\\t\\tprompt_tokens_details: {\\n266 | \\t\\t\\t\\t\\tcache_miss_tokens: 80,\\n267 | \\t\\t\\t\\t\\tcached_tokens: 20,\\n268 | \\t\\t\\t\\t},\\n----\\n289 | \\t\\t\\tconst usage = {\\n290 | \\t\\t\\t\\tprompt_tokens: 100,\\n291 | \\t\\t\\t\\tcompletion_tokens: 50,\\n292 | \\t\\t\\t\\ttotal_tokens: 150,\\n293 | \\t\\t\\t\\t// No prompt_tokens_details\\n294 | \\t\\t\\t}\\n----\\n\\n# src/api/transform/cache-strategy/multi-point-strategy.ts\\n  7 |  * Strategy for handling multiple cache points.\\n  8 |  * Creates cache points after messages as soon as uncached tokens exceed minimumTokenCount.\\n  9 |  */\\n----\\n 58 | \\t *\\n 59 | \\t * @param minTokensPerPoint Minimum tokens required per cache point\\n 60 | \\t * @param remainingCachePoints Number of cache points available\\n----\\n 97 | \\n 98 | \\t\\t// Calculate tokens in new messages (added since last cache point placement)\\n 99 | \\t\\tconst lastPreviousIndex = previousPlacements[previousPlacements.length - 1].index\\n----\\n103 | \\n104 | \\t\\t// If new messages have enough tokens for a cache point, we need to decide\\n105 | \\t\\t// whether to keep all previous cache points or combine some\\n----\\n127 | \\t\\t\\t\\t// We need to decide which previous cache points to keep and which to combine\\n128 | \\t\\t\\t\\t// Strategy: Compare the token count of new messages with the smallest combined token gap\\n129 | \\n130 | \\t\\t\\t\\t// First, analyze the token distribution between previous cache points\\n131 | \\t\\t\\t\\tconst tokensBetweenPlacements: number[] = []\\n132 | \\t\\t\\t\\tlet startIdx = 0\\n----\\n134 | \\t\\t\\t\\tfor (const placement of previousPlacements) {\\n135 | \\t\\t\\t\\t\\tconst tokens = this.config.messages\\n136 | \\t\\t\\t\\t\\t\\t.slice(startIdx, placement.index + 1)\\n----\\n138 | \\n139 | \\t\\t\\t\\t\\ttokensBetweenPlacements.push(tokens)\\n140 | \\t\\t\\t\\t\\tstartIdx = placement.index + 1\\n----\\n142 | \\n143 | \\t\\t\\t\\t// Find the two consecutive placements with the smallest token gap\\n144 | \\t\\t\\t\\tlet smallestGapIndex = 0\\n----\\n146 | \\n147 | \\t\\t\\t\\tfor (let i = 0; i < tokensBetweenPlacements.length - 1; i++) {\\n148 | \\t\\t\\t\\t\\tconst gap = tokensBetweenPlacements[i] + tokensBetweenPlacements[i + 1]\\n149 | \\t\\t\\t\\t\\tif (gap < smallestGap) {\\n----\\n155 | \\t\\t\\t\\t// Only combine cache points if it's beneficial\\n156 | \\t\\t\\t\\t// Compare the token count of new messages with the smallest combined token gap\\n157 | \\t\\t\\t\\t// Apply a required percentage increase to ensure reallocation is worth it\\n----\\n161 | \\t\\t\\t\\tif (newMessagesTokens >= requiredTokenThreshold) {\\n162 | \\t\\t\\t\\t\\t// It's beneficial to combine cache points since new messages have significantly more tokens\\n163 | \\t\\t\\t\\t\\tlogger.info(\\\"Combining cache points is beneficial\\\", {\\n----\\n231 | \\t\\t} else {\\n232 | \\t\\t\\t// New messages don't have enough tokens for a cache point\\n233 | \\t\\t\\t// Keep all previous placements that are still valid\\n----\\n266 | \\t\\tif (lastUserMessageIndex >= 0) {\\n267 | \\t\\t\\t// Calculate the total tokens covered from the previous cache point (or start of conversation)\\n268 | \\t\\t\\t// to this cache point. This ensures tokensCovered represents the full span of tokens\\n269 | \\t\\t\\t// that will be cached by this cache point.\\n----\\n281 | \\n282 | \\t\\t\\t// Calculate tokens from previous cache point (or start) to this cache point\\n283 | \\t\\t\\tconst tokenStartIndex = previousCachePointIndex + 1\\n284 | \\t\\t\\ttotalTokensCovered = this.config.messages\\n285 | \\t\\t\\t\\t.slice(tokenStartIndex, lastUserMessageIndex + 1)\\n286 | \\t\\t\\t\\t.reduce((acc, curr) => acc + this.estimateTokenCount(curr), 0)\\n287 | \\n288 | \\t\\t\\t// Guard clause: ensure we have enough tokens to justify a cache point\\n289 | \\t\\t\\tif (totalTokensCovered < minTokensPerPoint) {\\n----\\n294 | \\t\\t\\t\\ttype: \\\"message\\\",\\n295 | \\t\\t\\t\\ttokensCovered: totalTokensCovered,\\n296 | \\t\\t\\t}\\n----\\n\\n# src/api/transform/model-params.ts\\n 95 | \\n 96 | \\t\\t// Reasoning cannot be less than 1024 tokens.\\n 97 | \\t\\tif (reasoningBudget < 1024) {\\n----\\n\\n# src/api/transform/cache-strategy/types.ts\\n  7 | export interface ModelInfo {\\n  8 | \\t/** Maximum number of tokens the model can generate */\\n  9 | \\tmaxTokens: number\\n 10 | \\t/** Maximum context window size in tokens */\\n 11 | \\tcontextWindow: number\\n----\\n 15 | \\tmaxCachePoints: number\\n 16 | \\t/** Minimum number of tokens required for a cache point */\\n 17 | \\tminTokensPerCachePoint: number\\n----\\n 49 | \\ttype: \\\"system\\\" | \\\"message\\\"\\n 50 | \\t/** Number of tokens this cache point covers */\\n 51 | \\ttokensCovered: number\\n 52 | }\\n----\\n\\n# src/api/transform/__tests__/reasoning.spec.ts\\n 40 | \\n 41 | \\t\\t\\texpect(result).toEqual({ max_tokens: 1000 })\\n 42 | \\t\\t})\\n----\\n 61 | \\n 62 | \\t\\t\\texpect(result).toEqual({ max_tokens: 1000 })\\n 63 | \\t\\t})\\n----\\n123 | \\n124 | \\t\\t\\texpect(result).toEqual({ max_tokens: 1000 })\\n125 | \\t\\t})\\n----\\n140 | \\n141 | \\t\\t\\texpect(result).toEqual({ max_tokens: undefined })\\n142 | \\t\\t})\\n----\\n189 | \\n190 | \\t\\t\\texpect(result).toEqual({ max_tokens: 0 })\\n191 | \\t\\t})\\n----\\n244 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n245 | \\t\\t\\t\\tbudget_tokens: 1000,\\n246 | \\t\\t\\t})\\n----\\n268 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n269 | \\t\\t\\t\\tbudget_tokens: 1000,\\n270 | \\t\\t\\t})\\n----\\n314 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n315 | \\t\\t\\t\\tbudget_tokens: undefined,\\n316 | \\t\\t\\t})\\n----\\n334 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n335 | \\t\\t\\t\\tbudget_tokens: 0,\\n336 | \\t\\t\\t})\\n----\\n354 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n355 | \\t\\t\\t\\tbudget_tokens: 100000,\\n356 | \\t\\t\\t})\\n----\\n374 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n375 | \\t\\t\\t\\tbudget_tokens: 1000,\\n376 | \\t\\t\\t})\\n----\\n547 | \\n548 | \\t\\t\\texpect(openRouterResult).toEqual({ max_tokens: 1000 })\\n549 | \\t\\t\\texpect(anthropicResult).toEqual({ type: \\\"enabled\\\", budget_tokens: 1000 })\\n550 | \\t\\t\\texpect(openAiResult).toBeUndefined()\\n----\\n601 | \\t\\t\\t// Budget should take precedence for OpenRouter and Anthropic\\n602 | \\t\\t\\texpect(openRouterResult).toEqual({ max_tokens: 1000 })\\n603 | \\t\\t\\texpect(anthropicResult).toEqual({ type: \\\"enabled\\\", budget_tokens: 1000 })\\n604 | \\t\\t\\t// OpenAI should still use effort since it doesn't support budget\\n----\\n672 | \\t\\t\\t\\texpect(typeof result).toBe(\\\"object\\\")\\n673 | \\t\\t\\t\\texpect(\\\"max_tokens\\\" in result || \\\"effort\\\" in result || \\\"exclude\\\" in result).toBe(true)\\n674 | \\t\\t\\t}\\n----\\n688 | \\t\\t\\t\\texpect(result).toHaveProperty(\\\"type\\\", \\\"enabled\\\")\\n689 | \\t\\t\\t\\texpect(result).toHaveProperty(\\\"budget_tokens\\\")\\n690 | \\t\\t\\t}\\n----\\n\\n# src/api/transform/__tests__/model-params.spec.ts\\n205 | \\t\\t\\t\\t\\ttype: \\\"enabled\\\",\\n206 | \\t\\t\\t\\t\\tbudget_tokens: 1600,\\n207 | \\t\\t\\t\\t},\\n----\\n226 | \\t\\t\\t\\ttype: \\\"enabled\\\",\\n227 | \\t\\t\\t\\tbudget_tokens: 1600,\\n228 | \\t\\t\\t})\\n----\\n262 | \\t\\t\\t\\t\\ttype: \\\"enabled\\\",\\n263 | \\t\\t\\t\\t\\tbudget_tokens: 2400,\\n264 | \\t\\t\\t\\t},\\n----\\n287 | \\t\\t\\t\\t\\ttype: \\\"enabled\\\",\\n288 | \\t\\t\\t\\t\\tbudget_tokens: 1500,\\n289 | \\t\\t\\t\\t},\\n----\\n308 | \\n309 | \\t\\tit(\\\"should clamp thinking budget to at least 1024 tokens\\\", () => {\\n310 | \\t\\t\\tconst model: ModelInfo = {\\n----\\n328 | \\t\\t\\t\\t\\ttype: \\\"enabled\\\",\\n329 | \\t\\t\\t\\t\\tbudget_tokens: 1024,\\n330 | \\t\\t\\t\\t},\\n----\\n333 | \\n334 | \\t\\tit(\\\"should clamp thinking budget to at most 80% of max tokens\\\", () => {\\n335 | \\t\\t\\tconst model: ModelInfo = {\\n----\\n353 | \\t\\t\\t\\t\\ttype: \\\"enabled\\\",\\n354 | \\t\\t\\t\\t\\tbudget_tokens: 3200,\\n355 | \\t\\t\\t\\t},\\n----\\n372 | \\t\\t\\t\\t\\ttype: \\\"enabled\\\",\\n373 | \\t\\t\\t\\t\\tbudget_tokens: DEFAULT_HYBRID_REASONING_MODEL_THINKING_TOKENS,\\n374 | \\t\\t\\t\\t},\\n----\\n390 | \\t\\t\\texpect(result.maxTokens).toBe(5000)\\n391 | \\t\\t\\texpect(result.reasoningBudget).toBe(2000) // Custom thinking tokens takes precedence\\n392 | \\t\\t})\\n393 | \\n394 | \\t\\tit(\\\"should clamp custom thinking tokens even when custom max tokens is provided\\\", () => {\\n395 | \\t\\t\\tconst model: ModelInfo = {\\n----\\n719 | \\n720 | \\t\\t\\texpect(result.reasoning).toEqual({ max_tokens: 3200 })\\n721 | \\t\\t})\\n----\\n\\n# src/api/transform/cache-strategy/base-strategy.ts\\n 27 | \\t/**\\n 28 | \\t * Calculate token count for system prompt using a more accurate approach\\n 29 | \\t */\\n----\\n 33 | \\n 34 | \\t\\t\\t// Use a more accurate token estimation than simple character count\\n 35 | \\t\\t\\t// Count words and add overhead for punctuation and special tokens\\n 36 | \\t\\t\\tconst words = text.split(/\\\\s+/).filter((word) => word.length > 0)\\n 37 | \\t\\t\\t// Average English word is ~1.3 tokens\\n 38 | \\t\\t\\tlet tokenCount = words.length * 1.3\\n 39 | \\t\\t\\t// Add overhead for punctuation and special characters\\n 40 | \\t\\t\\ttokenCount += (text.match(/[.,!?;:()[\\\\]{}\\\"\\\"''`]/g) || []).length * 0.3\\n 41 | \\t\\t\\t// Add overhead for newlines\\n 42 | \\t\\t\\ttokenCount += (text.match(/\\\\n/g) || []).length * 0.5\\n 43 | \\t\\t\\t// Add a small overhead for system prompt structure\\n 44 | \\t\\t\\ttokenCount += 5\\n 45 | \\n 46 | \\t\\t\\tthis.systemTokenCount = Math.ceil(tokenCount)\\n 47 | \\t\\t}\\n----\\n 84 | \\t/**\\n 85 | \\t * Check if a token count meets the minimum threshold for caching\\n 86 | \\t */\\n 87 | \\tprotected meetsMinTokenThreshold(tokenCount: number): boolean {\\n 88 | \\t\\tconst minTokens = this.config.modelInfo.minTokensPerCachePoint\\n----\\n 91 | \\t\\t}\\n 92 | \\t\\treturn tokenCount >= minTokens\\n 93 | \\t}\\n----\\n 95 | \\t/**\\n 96 | \\t * Estimate token count for a message using a more accurate approach\\n 97 | \\t * This implementation is based on the BaseProvider's countTokens method\\n----\\n100 | \\tprotected estimateTokenCount(message: Anthropic.Messages.MessageParam): number {\\n101 | \\t\\t// Use a more sophisticated token counting approach\\n102 | \\t\\tif (!message.content) return 0\\n----\\n108 | \\t\\t\\t\\tif (block.type === \\\"text\\\") {\\n109 | \\t\\t\\t\\t\\t// Use a more accurate token estimation than simple character count\\n110 | \\t\\t\\t\\t\\t// This is still an approximation but better than character/4\\n----\\n112 | \\t\\t\\t\\t\\tif (text.length > 0) {\\n113 | \\t\\t\\t\\t\\t\\t// Count words and add overhead for punctuation and special tokens\\n114 | \\t\\t\\t\\t\\t\\tconst words = text.split(/\\\\s+/).filter((word) => word.length > 0)\\n115 | \\t\\t\\t\\t\\t\\t// Average English word is ~1.3 tokens\\n116 | \\t\\t\\t\\t\\t\\ttotalTokens += words.length * 1.3\\n----\\n128 | \\t\\t\\tconst text = message.content\\n129 | \\t\\t\\t// Count words and add overhead for punctuation and special tokens\\n130 | \\t\\t\\tconst words = text.split(/\\\\s+/).filter((word) => word.length > 0)\\n131 | \\t\\t\\t// Average English word is ~1.3 tokens\\n132 | \\t\\t\\ttotalTokens += words.length * 1.3\\n----\\n\\n# src/api/providers/__tests__/glama.spec.ts\\n 57 | \\t\\t\\t\\t\\t\\t\\t\\t\\tchoices: [{ delta: {}, index: 0 }],\\n 58 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: { prompt_tokens: 10, completion_tokens: 5, total_tokens: 15 },\\n 59 | \\t\\t\\t\\t\\t\\t\\t\\t}\\n----\\n112 | \\t\\t\\tusage: {\\n113 | \\t\\t\\t\\tprompt_tokens: 10,\\n114 | \\t\\t\\t\\tcompletion_tokens: 5,\\n115 | \\t\\t\\t\\ttotal_tokens: 15,\\n116 | \\t\\t\\t},\\n----\\n176 | \\t\\t\\t\\t\\ttemperature: 0,\\n177 | \\t\\t\\t\\t\\tmax_tokens: 8192,\\n178 | \\t\\t\\t\\t}),\\n----\\n194 | \\n195 | \\t\\tit(\\\"should not set max_tokens for non-Anthropic models\\\", async () => {\\n196 | \\t\\t\\t// Reset mock to clear any previous calls\\n----\\n213 | \\t\\t\\t)\\n214 | \\t\\t\\texpect(mockCreate.mock.calls[0][0]).not.toHaveProperty(\\\"max_tokens\\\")\\n215 | \\t\\t})\\n----\\n\\n# src/api/transform/__tests__/stream.spec.ts\\n 33 | \\n 34 | \\t\\tit(\\\"should handle usage chunks without cache tokens\\\", () => {\\n 35 | \\t\\t\\tconst usageChunk: ApiStreamChunk = {\\n----\\n 57 | \\n 58 | \\t\\tit(\\\"should handle usage chunks with zero tokens\\\", () => {\\n 59 | \\t\\t\\tconst zeroUsageChunk: ApiStreamChunk = {\\n----\\n 69 | \\n 70 | \\t\\tit(\\\"should handle usage chunks with large token counts\\\", () => {\\n 71 | \\t\\t\\tconst largeUsageChunk: ApiStreamChunk = {\\n----\\n\\n# src/api/transform/cache-strategy/__tests__/cache-strategy.spec.ts\\n 29 | \\n 30 | const createMessageWithTokens = (role: \\\"user\\\" | \\\"assistant\\\", tokenCount: number) => ({\\n 31 | \\trole,\\n 32 | \\tcontent: \\\"x\\\".repeat(tokenCount * 4), // Approximate 4 chars per token\\n 33 | })\\n----\\n130 | \\t\\t\\t\\tit(\\\"adds system cache block when prompt caching is enabled, messages exist, and system prompt is long enough\\\", () => {\\n131 | \\t\\t\\t\\t\\t// Create a system prompt that's at least 50 tokens (200+ characters)\\n132 | \\t\\t\\t\\t\\tconst longSystemPrompt =\\n----\\n236 | \\t\\t\\t\\t\\t\\t\\t\\t(i + 1) +\\n237 | \\t\\t\\t\\t\\t\\t\\t\\t\\\" with some additional text to increase token count. \\\" +\\n238 | \\t\\t\\t\\t\\t\\t\\t\\t\\\"Adding more text to ensure we exceed the token threshold for cache block insertion.\\\",\\n239 | \\t\\t\\t\\t\\t\\t}))\\n----\\n579 | \\n580 | \\t\\tit(\\\"should yield usage results with cache tokens when using MultiPointStrategy\\\", async () => {\\n581 | \\t\\t\\t// Mock the convertToBedrockConverseMessages method to return a result with cache points\\n----\\n589 | \\n590 | \\t\\t\\t// Create a mock stream that includes cache token fields\\n591 | \\t\\t\\tconst mockApiResponse = {\\n----\\n626 | \\n627 | \\t\\t\\t// Verify that usage results with cache tokens are yielded\\n628 | \\t\\t\\texpect(chunks.length).toBeGreaterThan(0)\\n629 | \\t\\t\\t// The test already expects cache tokens, but the implementation might not be including them\\n630 | \\t\\t\\t// Let's make the test more flexible to accept either format\\n----\\n652 | \\n653 | \\t\\t// Helper function to create a message with approximate token count\\n654 | \\t\\tconst createMessage = (role: \\\"user\\\" | \\\"assistant\\\", content: string, tokenCount: number) => {\\n655 | \\t\\t\\t// Pad the content to reach the desired token count (approx 4 chars per token)\\n656 | \\t\\t\\tconst paddingNeeded = Math.max(0, tokenCount * 4 - content.length)\\n657 | \\t\\t\\tconst padding = \\\" \\\".repeat(paddingNeeded)\\n----\\n667 | \\t\\t\\t\\t\\\"Cache point placements:\\\",\\n668 | \\t\\t\\t\\tplacements.map((p) => `index: ${p.index}, tokens: ${p.tokensCovered}`),\\n669 | \\t\\t\\t)\\n----\\n683 | \\t\\t\\t\\t\\tmodelInfo: multiPointModelInfo,\\n684 | \\t\\t\\t\\t\\tsystemPrompt: \\\"You are a helpful assistant.\\\", // ~10 tokens\\n685 | \\t\\t\\t\\t\\tmessages,\\n----\\n730 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n731 | \\t\\t\\t\\t\\t\\ttokensCovered: 300,\\n732 | \\t\\t\\t\\t\\t},\\n----\\n736 | \\t\\t\\t\\t\\tmodelInfo: multiPointModelInfo,\\n737 | \\t\\t\\t\\t\\tsystemPrompt: \\\"You are a helpful assistant.\\\", // ~10 tokens\\n738 | \\t\\t\\t\\t\\tmessages,\\n----\\n759 | \\n760 | \\t\\t\\t\\t// Check if we have a second cache point (may not always be added depending on token distribution)\\n761 | \\t\\t\\t\\tif (result.messageCachePointPlacements && result.messageCachePointPlacements.length > 1) {\\n----\\n789 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n790 | \\t\\t\\t\\t\\t\\ttokensCovered: 300,\\n791 | \\t\\t\\t\\t\\t},\\n----\\n794 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n795 | \\t\\t\\t\\t\\t\\ttokensCovered: 300,\\n796 | \\t\\t\\t\\t\\t},\\n----\\n800 | \\t\\t\\t\\t\\tmodelInfo: multiPointModelInfo,\\n801 | \\t\\t\\t\\t\\tsystemPrompt: \\\"You are a helpful assistant.\\\", // ~10 tokens\\n802 | \\t\\t\\t\\t\\tmessages,\\n----\\n863 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n864 | \\t\\t\\t\\t\\t\\ttokensCovered: 300,\\n865 | \\t\\t\\t\\t\\t},\\n----\\n868 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n869 | \\t\\t\\t\\t\\t\\ttokensCovered: 300,\\n870 | \\t\\t\\t\\t\\t},\\n----\\n873 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n874 | \\t\\t\\t\\t\\t\\ttokensCovered: 300,\\n875 | \\t\\t\\t\\t\\t},\\n----\\n879 | \\t\\t\\t\\t\\tmodelInfo: multiPointModelInfo,\\n880 | \\t\\t\\t\\t\\tsystemPrompt: \\\"You are a helpful assistant.\\\", // ~10 tokens\\n881 | \\t\\t\\t\\t\\tmessages,\\n----\\n929 | \\t\\t\\t\\t// This test verifies that the documentation for Example 5 is correct\\n930 | \\t\\t\\t\\t// In Example 5, the third cache point at index 10 should cover 660 tokens\\n931 | \\t\\t\\t\\t// (260 tokens from messages 7-8 plus 400 tokens from the new messages)\\n932 | \\n----\\n944 | \\t\\t\\t\\t\\tcreateMessage(\\\"assistant\\\", \\\"Deep learning has many applications including...\\\", 160),\\n945 | \\t\\t\\t\\t\\t// New messages with 400 tokens total\\n946 | \\t\\t\\t\\t\\tcreateMessage(\\\"user\\\", \\\"Can you provide a detailed example?\\\", 100),\\n----\\n954 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n955 | \\t\\t\\t\\t\\t\\ttokensCovered: 240,\\n956 | \\t\\t\\t\\t\\t},\\n----\\n959 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n960 | \\t\\t\\t\\t\\t\\ttokensCovered: 440,\\n961 | \\t\\t\\t\\t\\t},\\n----\\n964 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n965 | \\t\\t\\t\\t\\t\\ttokensCovered: 260,\\n966 | \\t\\t\\t\\t\\t},\\n----\\n969 | \\t\\t\\t\\t// In the documentation, the algorithm decides to replace the cache point at index 8\\n970 | \\t\\t\\t\\t// with a new one at index 10, and the tokensCovered value should be 660 tokens\\n971 | \\t\\t\\t\\t// (260 tokens from messages 7-8 plus 400 tokens from the new messages)\\n972 | \\n973 | \\t\\t\\t\\t// However, the actual implementation may behave differently depending on how\\n974 | \\t\\t\\t\\t// it calculates token counts and makes decisions about cache point placement\\n975 | \\n976 | \\t\\t\\t\\t// The important part is that our fix ensures that when a cache point is created,\\n977 | \\t\\t\\t\\t// the tokensCovered value represents all tokens from the previous cache point\\n978 | \\t\\t\\t\\t// to the current cache point, not just the tokens in the new messages\\n979 | \\t\\t\\t})\\n980 | \\n981 | \\t\\t\\tit(\\\"should not combine cache points when new messages have fewer tokens than the smallest combined gap\\\", () => {\\n982 | \\t\\t\\t\\t// This test verifies that when new messages have fewer tokens than the smallest combined gap,\\n983 | \\t\\t\\t\\t// the algorithm keeps all existing cache points and doesn't add a new one\\n----\\n1004 | \\t\\t\\t\\t\\t\\t),\\n1005 | \\t\\t\\t\\t\\t\\t// Small addition (only 50 tokens total)\\n1006 | \\t\\t\\t\\t\\t\\tcreateMessage(\\\"user\\\", \\\"Thanks for the explanation.\\\", 20),\\n----\\n1009 | \\n1010 | \\t\\t\\t\\t\\t// Previous cache point placements with significant token coverage\\n1011 | \\t\\t\\t\\t\\tconst previousCachePointPlacements: CachePointPlacement[] = [\\n----\\n1014 | \\t\\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n1015 | \\t\\t\\t\\t\\t\\t\\ttokensCovered: 400, // Significant token coverage\\n1016 | \\t\\t\\t\\t\\t\\t},\\n----\\n1019 | \\t\\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n1020 | \\t\\t\\t\\t\\t\\t\\ttokensCovered: 300, // Significant token coverage\\n1021 | \\t\\t\\t\\t\\t\\t},\\n----\\n1024 | \\t\\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n1025 | \\t\\t\\t\\t\\t\\t\\ttokensCovered: 300, // Significant token coverage\\n1026 | \\t\\t\\t\\t\\t\\t},\\n----\\n1030 | \\t\\t\\t\\t\\t\\tmodelInfo: multiPointModelInfo,\\n1031 | \\t\\t\\t\\t\\t\\tsystemPrompt: \\\"You are a helpful assistant.\\\", // ~10 tokens\\n1032 | \\t\\t\\t\\t\\t\\tmessages,\\n----\\n1057 | \\n1058 | \\t\\t\\tit(\\\"should make correct decisions based on token counts\\\", () => {\\n1059 | \\t\\t\\t\\t// This test verifies that the algorithm correctly compares token counts\\n1060 | \\t\\t\\t\\t// and makes the right decision about combining cache points\\n1061 | \\n1062 | \\t\\t\\t\\t// Create messages with a variety of token counts\\n1063 | \\t\\t\\t\\tconst messages = [\\n----\\n1081 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n1082 | \\t\\t\\t\\t\\t\\ttokensCovered: 400,\\n1083 | \\t\\t\\t\\t\\t},\\n----\\n1086 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n1087 | \\t\\t\\t\\t\\t\\ttokensCovered: 150,\\n1088 | \\t\\t\\t\\t\\t},\\n----\\n1091 | \\t\\t\\t\\t\\t\\ttype: \\\"message\\\",\\n1092 | \\t\\t\\t\\t\\t\\ttokensCovered: 150,\\n1093 | \\t\\t\\t\\t\\t},\\n----\\n\\n# src/api/providers/__tests__/openai-usage-tracking.spec.ts\\n 28 | \\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 29 | \\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 30 | \\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 31 | \\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 32 | \\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 47 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 48 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 49 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 2,\\n 50 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 12,\\n 51 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 62 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 63 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 64 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 4,\\n 65 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 14,\\n 66 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 77 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 78 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 79 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 80 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 81 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n155 | \\t\\t\\t\\t\\t\\tchoices: [{ message: { role: \\\"assistant\\\", content: \\\"Test response\\\" } }],\\n156 | \\t\\t\\t\\t\\t\\tusage: { prompt_tokens: 10, completion_tokens: 5, total_tokens: 15 },\\n157 | \\t\\t\\t\\t\\t}\\n----\\n177 | \\t\\t\\t\\t\\t\\t\\tusage: {\\n178 | \\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n179 | \\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n180 | \\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n181 | \\t\\t\\t\\t\\t\\t\\t},\\n----\\n\\n# src/api/providers/__tests__/unbound.spec.ts\\n 82 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 83 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 10,\\n 84 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 5,\\n 85 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 15,\\n 86 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 91 | \\t\\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 92 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tprompt_tokens: 8,\\n 93 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcompletion_tokens: 4,\\n 94 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\ttotal_tokens: 12,\\n 95 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcache_creation_input_tokens: 3,\\n 96 | \\t\\t\\t\\t\\t\\t\\t\\t\\t\\tcache_read_input_tokens: 2,\\n 97 | \\t\\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n232 | \\t\\t\\t\\t\\ttemperature: 0,\\n233 | \\t\\t\\t\\t\\tmax_tokens: 8192,\\n234 | \\t\\t\\t\\t}),\\n----\\n253 | \\n254 | \\t\\tit(\\\"should not set max_tokens for non-Anthropic models\\\", async () => {\\n255 | \\t\\t\\tmockCreate.mockClear()\\n----\\n277 | \\n278 | \\t\\t\\texpect(mockCreate.mock.calls[0][0]).not.toHaveProperty(\\\"max_tokens\\\")\\n279 | \\t\\t})\\n----\\n\\n# src/api/providers/__tests__/anthropic-vertex.spec.ts\\n 21 | \\t\\t\\t\\t\\t\\tusage: {\\n 22 | \\t\\t\\t\\t\\t\\t\\tinput_tokens: 10,\\n 23 | \\t\\t\\t\\t\\t\\t\\toutput_tokens: 5,\\n 24 | \\t\\t\\t\\t\\t\\t},\\n----\\n 32 | \\t\\t\\t\\t\\t\\t\\t\\tusage: {\\n 33 | \\t\\t\\t\\t\\t\\t\\t\\t\\tinput_tokens: 10,\\n 34 | \\t\\t\\t\\t\\t\\t\\t\\t\\toutput_tokens: 5,\\n 35 | \\t\\t\\t\\t\\t\\t\\t\\t},\\n----\\n 95 | \\t\\t\\t\\t\\t\\tusage: {\\n 96 | \\t\\t\\t\\t\\t\\t\\tinput_tokens: 10,\\n 97 | \\t\\t\\t\\t\\t\\t\\toutput_tokens: 0,\\n 98 | \\t\\t\\t\\t\\t\\t},\\n----\\n118 | \\t\\t\\t\\t\\tusage: {\\n119 | \\t\\t\\t\\t\\t\\toutput_tokens: 5,\\n120 | \\t\\t\\t\\t\\t},\\n----\\n164 | \\t\\t\\t\\tmodel: \\\"claude-3-5-sonnet-v2@20241022\\\",\\n165 | \\t\\t\\t\\tmax_tokens: 8192,\\n166 | \\t\\t\\t\\ttemperature: 0,\\n----\\n284 | \\t\\t\\t\\t\\t\\tusage: {\\n285 | \\t\\t\\t\\t\\t\\t\\tinput_tokens: 10,\\n286 | \\t\\t\\t\\t\\t\\t\\toutput_tokens: 0,\\n287 | \\t\\t\\t\\t\\t\\t\\tcache_creation_input_tokens: 3,\\n288 | \\t\\t\\t\\t\\t\\t\\tcache_read_input_tokens: 2,\\n289 | \\t\\t\\t\\t\\t\\t},\\n----\\n309 | \\t\\t\\t\\t\\tusage: {\\n310 | \\t\\t\\t\\t\\t\\toutput_tokens: 5,\\n311 | \\t\\t\\t\\t\\t},\\n----\\n419 | \\t\\t\\t\\t\\t\\tusage: {\\n420 | \\t\\t\\t\\t\\t\\t\\tinput_tokens: 10,\\n421 | \\t\\t\\t\\t\\t\\t\\toutput_tokens: 0,\\n422 | \\t\\t\\t\\t\\t\\t\\tcache_creation_input_tokens: 5,\\n423 | \\t\\t\\t\\t\\t\\t\\tcache_read_input_tokens: 3,\\n424 | \\t\\t\\t\\t\\t\\t},\\n----\\n484 | \\t\\t\\t\\t\\t\\tusage: {\\n485 | \\t\\t\\t\\t\\t\\t\\tinput_tokens: 10,\\n486 | \\t\\t\\t\\t\\t\\t\\toutput_tokens: 0,\\n487 | \\t\\t\\t\\t\\t\\t},\\n----\\n618 | \\t\\t\\t\\tmodel: \\\"claude-3-5-sonnet-v2@20241022\\\",\\n619 | \\t\\t\\t\\tmax_tokens: 8192,\\n620 | \\t\\t\\t\\ttemperature: 0,\\n----\\n752 | \\n753 | \\t\\t\\t// Test with default thinking budget (80% of max tokens)\\n754 | \\t\\t\\tconst handlerWithDefaultBudget = new AnthropicVertexHandler({\\n----\\n767 | \\t\\t\\t\\tvertexRegion: \\\"us-central1\\\",\\n768 | \\t\\t\\t\\tmodelMaxTokens: 1000, // This would result in 800 tokens for thinking, but minimum is 1024\\n769 | \\t\\t\\t})\\n----\\n789 | \\t\\t\\t\\t\\t\\tmodel: options.model,\\n790 | \\t\\t\\t\\t\\t\\tusage: { input_tokens: 10, output_tokens: 5 },\\n791 | \\t\\t\\t\\t\\t}\\n----\\n794 | \\t\\t\\t\\t\\tasync *[Symbol.asyncIterator]() {\\n795 | \\t\\t\\t\\t\\t\\tyield { type: \\\"message_start\\\", message: { usage: { input_tokens: 10, output_tokens: 5 } } }\\n796 | \\t\\t\\t\\t\\t},\\n----\\n806 | \\t\\t\\t\\texpect.objectContaining({\\n807 | \\t\\t\\t\\t\\tthinking: { type: \\\"enabled\\\", budget_tokens: 4096 },\\n808 | \\t\\t\\t\\t\\ttemperature: 1.0, // Thinking requires temperature 1.0\\n----\\n\\n# src/core/prompts/tools/codebase-search.ts\\n 12 | \\n 13 | Example: Searching for functions related to user authentication\\n 14 | <codebase_search>\\n----\\n\\n# src/core/task/__tests__/Task.test.ts\\n185 | \\t\\t\\t\\t\\t\\t\\t\\ttask: \\\"historical task\\\",\\n186 | \\t\\t\\t\\t\\t\\t\\t\\ttokensIn: 100,\\n187 | \\t\\t\\t\\t\\t\\t\\t\\ttokensOut: 200,\\n188 | \\t\\t\\t\\t\\t\\t\\t\\tcacheWrites: 0,\\n----\\n253 | \\t\\t\\t\\ttask: \\\"historical task\\\",\\n254 | \\t\\t\\t\\ttokensIn: 100,\\n255 | \\t\\t\\t\\ttokensOut: 200,\\n256 | \\t\\t\\t\\tcacheWrites: 0,\\n----\\n615 | \\t\\t\\t\\t\\t\\ttext: JSON.stringify({\\n616 | \\t\\t\\t\\t\\t\\t\\ttokensIn: 100,\\n617 | \\t\\t\\t\\t\\t\\t\\ttokensOut: 50,\\n618 | \\t\\t\\t\\t\\t\\t\\tcacheWrites: 0,\\n----\\n740 | \\t\\t\\t\\t\\t\\ttext: JSON.stringify({\\n741 | \\t\\t\\t\\t\\t\\t\\ttokensIn: 100,\\n742 | \\t\\t\\t\\t\\t\\t\\ttokensOut: 50,\\n743 | \\t\\t\\t\\t\\t\\t\\tcacheWrites: 0,\\n----\\n\\n# src/core/prompts/instructions/create-mcp-server.ts\\n 11 | \\n 12 | When creating MCP servers, it's important to understand that they operate in a non-interactive environment. The server cannot initiate OAuth flows, open browser windows, or prompt for user input during runtime. All credentials and authentication tokens must be provided upfront through environment variables in the MCP settings configuration. For example, Spotify's API uses OAuth to get a refresh token for the user, but the MCP server cannot initiate this flow. While you can walk the user through  [truncated...]\\n 13 | \\n----\\n 76 | \\t\\t\\t\\t...\\n 77 | \\t\\t\\t\\t\\\"type\\\": \\\"module\\\", // added by default, uses ES module syntax (import/export) rather than CommonJS (require/module.exports) (Important to know if you create additional scripts in this server repository like a get-refresh-token.js script)\\n 78 | \\t\\t\\t\\t\\\"scripts\\\": {\\n----\\n\\n# src/core/task/Task.ts\\n 24 | import { TelemetryService } from \\\"@roo-code/telemetry\\\"\\n 25 | import { CloudService } from \\\"@roo-code/cloud\\\"\\n 26 | \\n----\\n 96 | \\ttaskSpawned: [taskId: string]\\n 97 | \\ttaskCompleted: [taskId: string, tokenUsage: TokenUsage, toolUsage: ToolUsage]\\n 98 | \\ttaskTokenUsageUpdated: [taskId: string, tokenUsage: TokenUsage]\\n 99 | \\ttaskToolFailed: [taskId: string, tool: ToolName, error: string]\\n----\\n346 | \\n347 | \\t\\tconst shouldCaptureMessage = message.partial !== true && CloudService.isEnabled()\\n348 | \\n349 | \\t\\tif (shouldCaptureMessage) {\\n350 | \\t\\t\\tCloudService.instance.captureEvent({\\n351 | \\t\\t\\t\\tevent: TelemetryEventName.TASK_MESSAGE,\\n----\\n366 | \\n367 | \\t\\tconst shouldCaptureMessage = message.partial !== true && CloudService.isEnabled()\\n368 | \\n369 | \\t\\tif (shouldCaptureMessage) {\\n370 | \\t\\t\\tCloudService.instance.captureEvent({\\n371 | \\t\\t\\t\\tevent: TelemetryEventName.TASK_MESSAGE,\\n----\\n384 | \\n385 | \\t\\t\\tconst { historyItem, tokenUsage } = await taskMetadata({\\n386 | \\t\\t\\t\\tmessages: this.clineMessages,\\n----\\n392 | \\n393 | \\t\\t\\tthis.emit(\\\"taskTokenUsageUpdated\\\", this.taskId, tokenUsage)\\n394 | \\n----\\n1246 | \\t\\t\\t\\t\\t...JSON.parse(this.clineMessages[lastApiReqIndex].text || \\\"{}\\\"),\\n1247 | \\t\\t\\t\\t\\ttokensIn: inputTokens,\\n1248 | \\t\\t\\t\\t\\ttokensOut: outputTokens,\\n1249 | \\t\\t\\t\\t\\tcacheWrites: cacheWriteTokens,\\n----\\n1678 | \\t\\tif (contextTokens) {\\n1679 | \\t\\t\\t// Default max tokens value for thinking models when no specific\\n1680 | \\t\\t\\t// value is set.\\n----\\n\\n# src/api/providers/glama.ts\\n 51 | \\n 52 | \\t\\t// Required by Anthropic; other providers default to max tokens allowed.\\n 53 | \\t\\tlet maxTokens: number | undefined\\n----\\n 60 | \\t\\t\\tmodel: modelId,\\n 61 | \\t\\t\\tmax_tokens: maxTokens,\\n 62 | \\t\\t\\tmessages: openAiMessages,\\n----\\n 90 | \\t\\t\\t\\t// In case of an interrupted request, we need to wait for the upstream API to finish processing the request\\n 91 | \\t\\t\\t\\t// before we can fetch information about the token usage and cost.\\n 92 | \\t\\t\\t\\tconst response = await axios.get(\\n----\\n 98 | \\n 99 | \\t\\t\\t\\tif (completionRequest.tokenUsage && completionRequest.totalCostUsd) {\\n100 | \\t\\t\\t\\t\\tyield {\\n101 | \\t\\t\\t\\t\\t\\ttype: \\\"usage\\\",\\n102 | \\t\\t\\t\\t\\t\\tcacheWriteTokens: completionRequest.tokenUsage.cacheCreationInputTokens,\\n103 | \\t\\t\\t\\t\\t\\tcacheReadTokens: completionRequest.tokenUsage.cacheReadInputTokens,\\n104 | \\t\\t\\t\\t\\t\\tinputTokens: completionRequest.tokenUsage.promptTokens,\\n105 | \\t\\t\\t\\t\\t\\toutputTokens: completionRequest.tokenUsage.completionTokens,\\n106 | \\t\\t\\t\\t\\t\\ttotalCost: parseFloat(completionRequest.totalCostUsd),\\n----\\n132 | \\t\\t\\tif (modelId.startsWith(\\\"anthropic/\\\")) {\\n133 | \\t\\t\\t\\trequestOptions.max_tokens = info.maxTokens\\n134 | \\t\\t\\t}\\n----\\n\\n# src/api/transform/reasoning.ts\\n 12 | \\teffort?: ReasoningEffort\\n 13 | \\tmax_tokens?: number\\n 14 | \\texclude?: boolean\\n----\\n 36 | \\tshouldUseReasoningBudget({ model, settings })\\n 37 | \\t\\t? { max_tokens: reasoningBudget }\\n 38 | \\t\\t: shouldUseReasoningEffort({ model, settings })\\n----\\n 46 | }: GetModelReasoningOptions): AnthropicReasoningParams | undefined =>\\n 47 | \\tshouldUseReasoningBudget({ model, settings }) ? { type: \\\"enabled\\\", budget_tokens: reasoningBudget! } : undefined\\n 48 | \\n----\\n\\n# src/api/providers/openrouter.ts\\n 32 | \\tinclude_reasoning?: boolean\\n 33 | \\t// https://openrouter.ai/docs/use-cases/reasoning-tokens\\n 34 | \\treasoning?: OpenRouterReasoningParams\\n----\\n 40 | interface CompletionUsage {\\n 41 | \\tcompletion_tokens?: number\\n 42 | \\tcompletion_tokens_details?: {\\n 43 | \\t\\treasoning_tokens?: number\\n 44 | \\t}\\n 45 | \\tprompt_tokens?: number\\n 46 | \\tprompt_tokens_details?: {\\n 47 | \\t\\tcached_tokens?: number\\n 48 | \\t}\\n 49 | \\ttotal_tokens?: number\\n 50 | \\tcost?: number\\n----\\n 81 | \\n 82 | \\t\\t// OpenRouter sends reasoning tokens by default for Gemini 2.5 Pro\\n 83 | \\t\\t// Preview even if you don't request them. This is not the default for\\n----\\n116 | \\t\\t\\tmodel: modelId,\\n117 | \\t\\t\\t...(maxTokens && maxTokens > 0 && { max_tokens: maxTokens }),\\n118 | \\t\\t\\ttemperature,\\n----\\n165 | \\t\\t\\t\\ttype: \\\"usage\\\",\\n166 | \\t\\t\\t\\tinputTokens: lastUsage.prompt_tokens || 0,\\n167 | \\t\\t\\t\\toutputTokens: lastUsage.completion_tokens || 0,\\n168 | \\t\\t\\t\\t// Waiting on OpenRouter to figure out what this represents in the Gemini case\\n169 | \\t\\t\\t\\t// and how to best support it.\\n170 | \\t\\t\\t\\t// cacheReadTokens: lastUsage.prompt_tokens_details?.cached_tokens,\\n171 | \\t\\t\\t\\treasoningTokens: lastUsage.completion_tokens_details?.reasoning_tokens,\\n172 | \\t\\t\\t\\ttotalCost: (lastUsage.is_byok ? BYOK_COST_MULTIPLIER : 1) * (lastUsage.cost || 0),\\n----\\n219 | \\t\\t\\tmodel: modelId,\\n220 | \\t\\t\\tmax_tokens: maxTokens,\\n221 | \\t\\t\\ttemperature,\\n----\\n\\n# src/utils/__tests__/cost.spec.ts\\n 13 | \\t\\t\\tsupportsPromptCache: true,\\n 14 | \\t\\t\\tinputPrice: 3.0, // $3 per million tokens\\n 15 | \\t\\t\\toutputPrice: 15.0, // $15 per million tokens\\n 16 | \\t\\t\\tcacheWritesPrice: 3.75, // $3.75 per million tokens\\n 17 | \\t\\t\\tcacheReadsPrice: 0.3, // $0.30 per million tokens\\n 18 | \\t\\t}\\n----\\n 70 | \\n 71 | \\t\\tit(\\\"should handle zero tokens\\\", () => {\\n 72 | \\t\\t\\tconst cost = calculateApiCostAnthropic(mockModelInfo, 0, 0, 0, 0)\\n----\\n106 | \\t\\t\\tsupportsPromptCache: true,\\n107 | \\t\\t\\tinputPrice: 3.0, // $3 per million tokens\\n108 | \\t\\t\\toutputPrice: 15.0, // $15 per million tokens\\n109 | \\t\\t\\tcacheWritesPrice: 3.75, // $3.75 per million tokens\\n110 | \\t\\t\\tcacheReadsPrice: 0.3, // $0.30 per million tokens\\n111 | \\t\\t}\\n----\\n163 | \\n164 | \\t\\tit(\\\"should handle zero tokens\\\", () => {\\n165 | \\t\\t\\tconst cost = calculateApiCostOpenAI(mockModelInfo, 0, 0, 0, 0)\\n----\\n\\n# src/utils/__tests__/git.spec.ts\\n 98 | \\t\\t\\t\\tsubject: \\\"fix: test commit\\\",\\n 99 | \\t\\t\\t\\tauthor: \\\"John Doe\\\",\\n100 | \\t\\t\\t\\tdate: \\\"2024-01-06\\\",\\n----\\n162 | \\t\\t\\t\\t[\\n163 | \\t\\t\\t\\t\\t'git log -n 10 --format=\\\"%H%n%h%n%s%n%an%n%ad\\\" --date=short --author-date-order abc123',\\n164 | \\t\\t\\t\\t\\t{ stdout: mockCommitData, stderr: \\\"\\\" },\\n----\\n185 | \\t\\t\\t\\tsubject: \\\"fix: test commit\\\",\\n186 | \\t\\t\\t\\tauthor: \\\"John Doe\\\",\\n187 | \\t\\t\\t\\tdate: \\\"2024-01-06\\\",\\n----\\n\\n# src/api/providers/gemini.ts\\n  7 | } from \\\"@google/genai\\\"\\n  8 | import type { JWTInput } from \\\"google-auth-library\\\"\\n  9 | \\n----\\n179 | \\t\\t\\tif (response.totalTokens === undefined) {\\n180 | \\t\\t\\t\\tconsole.warn(\\\"Gemini token counting returned undefined, using fallback\\\")\\n181 | \\t\\t\\t\\treturn super.countTokens(content)\\n----\\n185 | \\t\\t} catch (error) {\\n186 | \\t\\t\\tconsole.warn(\\\"Gemini token counting failed, using fallback\\\", error)\\n187 | \\t\\t\\treturn super.countTokens(content)\\n----\\n209 | \\n210 | \\t\\t// If there's tiered pricing then adjust the input and output token prices\\n211 | \\t\\t// based on the input tokens used.\\n212 | \\t\\tif (info.tiers) {\\n----\\n221 | \\n222 | \\t\\t// Subtract the cached input tokens from the total input tokens.\\n223 | \\t\\tconst uncachedInputTokens = inputTokens - cacheReadTokens\\n----\\n230 | \\n231 | \\t\\tconst trace: Record<string, { price: number; tokens: number; cost: number }> = {\\n232 | \\t\\t\\tinput: { price: inputPrice, tokens: uncachedInputTokens, cost: inputTokensCost },\\n233 | \\t\\t\\toutput: { price: outputPrice, tokens: outputTokens, cost: outputTokensCost },\\n234 | \\t\\t}\\n----\\n236 | \\t\\tif (cacheReadTokens > 0) {\\n237 | \\t\\t\\ttrace.cacheRead = { price: cacheReadsPrice, tokens: cacheReadTokens, cost: cacheReadCost }\\n238 | \\t\\t}\\n----\\n\\n# src/api/providers/lite-llm.ts\\n 46 | \\n 47 | \\t\\t// Required by some providers; others default to max tokens allowed\\n 48 | \\t\\tlet maxTokens: number | undefined = info.maxTokens ?? undefined\\n----\\n 51 | \\t\\t\\tmodel: modelId,\\n 52 | \\t\\t\\tmax_tokens: maxTokens,\\n 53 | \\t\\t\\tmessages: openAiMessages,\\n----\\n 84 | \\t\\t\\t\\t\\ttype: \\\"usage\\\",\\n 85 | \\t\\t\\t\\t\\tinputTokens: lastUsage.prompt_tokens || 0,\\n 86 | \\t\\t\\t\\t\\toutputTokens: lastUsage.completion_tokens || 0,\\n 87 | \\t\\t\\t\\t\\tcacheWriteTokens: lastUsage.cache_creation_input_tokens || 0,\\n 88 | \\t\\t\\t\\t\\tcacheReadTokens: lastUsage.prompt_tokens_details?.cached_tokens || 0,\\n 89 | \\t\\t\\t\\t}\\n----\\n121 | \\n122 | \\t\\t\\trequestOptions.max_tokens = info.maxTokens\\n123 | \\n----\\n136 | interface LiteLLMUsage extends OpenAI.CompletionUsage {\\n137 | \\tcache_creation_input_tokens?: number\\n138 | }\\n----\\n\\n# src/api/providers/requesty.ts\\n 19 | // Requesty usage includes an extra field for Anthropic use cases.\\n 20 | // Safely cast the prompt token details section to the appropriate structure.\\n 21 | interface RequestyUsage extends OpenAI.CompletionUsage {\\n 22 | \\tprompt_tokens_details?: {\\n 23 | \\t\\tcaching_tokens?: number\\n 24 | \\t\\tcached_tokens?: number\\n 25 | \\t}\\n----\\n 76 | \\t\\tconst requestyUsage = usage as RequestyUsage\\n 77 | \\t\\tconst inputTokens = requestyUsage?.prompt_tokens || 0\\n 78 | \\t\\tconst outputTokens = requestyUsage?.completion_tokens || 0\\n 79 | \\t\\tconst cacheWriteTokens = requestyUsage?.prompt_tokens_details?.caching_tokens || 0\\n 80 | \\t\\tconst cacheReadTokens = requestyUsage?.prompt_tokens_details?.cached_tokens || 0\\n 81 | \\t\\tconst totalCost = modelInfo\\n----\\n102 | \\t\\t\\tinfo,\\n103 | \\t\\t\\tmaxTokens: max_tokens,\\n104 | \\t\\t\\ttemperature,\\n----\\n116 | \\t\\t\\tmodel,\\n117 | \\t\\t\\tmax_tokens,\\n118 | \\t\\t\\ttemperature,\\n----\\n150 | \\tasync completePrompt(prompt: string): Promise<string> {\\n151 | \\t\\tconst { id: model, maxTokens: max_tokens, temperature } = await this.fetchModel()\\n152 | \\n----\\n156 | \\t\\t\\tmodel,\\n157 | \\t\\t\\tmax_tokens,\\n158 | \\t\\t\\tmessages: openAiMessages,\\n----\\n\\n# src/api/providers/anthropic.ts\\n 29 | \\t\\tconst apiKeyFieldName =\\n 30 | \\t\\t\\tthis.options.anthropicBaseUrl && this.options.anthropicUseAuthToken ? \\\"authToken\\\" : \\\"apiKey\\\"\\n 31 | \\n----\\n 75 | \\t\\t\\t\\t\\t\\tmodel: modelId,\\n 76 | \\t\\t\\t\\t\\t\\tmax_tokens: maxTokens ?? ANTHROPIC_DEFAULT_MAX_TOKENS,\\n 77 | \\t\\t\\t\\t\\t\\ttemperature,\\n----\\n124 | \\t\\t\\t\\t\\tmodel: modelId,\\n125 | \\t\\t\\t\\t\\tmax_tokens: maxTokens ?? ANTHROPIC_DEFAULT_MAX_TOKENS,\\n126 | \\t\\t\\t\\t\\ttemperature,\\n----\\n142 | \\t\\t\\t\\t\\t\\ttype: \\\"usage\\\",\\n143 | \\t\\t\\t\\t\\t\\tinputTokens: usage.input_tokens || 0,\\n144 | \\t\\t\\t\\t\\t\\toutputTokens: usage.output_tokens || 0,\\n145 | \\t\\t\\t\\t\\t\\tcacheWriteTokens: usage.cache_creation_input_tokens || undefined,\\n146 | \\t\\t\\t\\t\\t\\tcacheReadTokens: usage.cache_read_input_tokens || undefined,\\n147 | \\t\\t\\t\\t\\t}\\n----\\n151 | \\t\\t\\t\\tcase \\\"message_delta\\\":\\n152 | \\t\\t\\t\\t\\t// Tells us stop_reason, stop_sequence, and output tokens\\n153 | \\t\\t\\t\\t\\t// along the way and at the end of the message.\\n----\\n156 | \\t\\t\\t\\t\\t\\tinputTokens: 0,\\n157 | \\t\\t\\t\\t\\t\\toutputTokens: chunk.usage.output_tokens || 0,\\n158 | \\t\\t\\t\\t\\t}\\n----\\n231 | \\t\\t\\tmodel,\\n232 | \\t\\t\\tmax_tokens: ANTHROPIC_DEFAULT_MAX_TOKENS,\\n233 | \\t\\t\\tthinking: undefined,\\n----\\n243 | \\t/**\\n244 | \\t * Counts tokens for the given content using Anthropic's API\\n245 | \\t *\\n246 | \\t * @param content The content blocks to count tokens for\\n247 | \\t * @returns A promise resolving to the token count\\n248 | \\t */\\n----\\n258 | \\n259 | \\t\\t\\treturn response.input_tokens\\n260 | \\t\\t} catch (error) {\\n261 | \\t\\t\\t// Log error but fallback to tiktoken estimation\\n262 | \\t\\t\\tconsole.warn(\\\"Anthropic token counting failed, using fallback\\\", error)\\n263 | \\n----\\n\\n# src/api/providers/deepseek.ts\\n 32 | \\t\\t\\ttype: \\\"usage\\\",\\n 33 | \\t\\t\\tinputTokens: usage?.prompt_tokens || 0,\\n 34 | \\t\\t\\toutputTokens: usage?.completion_tokens || 0,\\n 35 | \\t\\t\\tcacheWriteTokens: usage?.prompt_tokens_details?.cache_miss_tokens,\\n 36 | \\t\\t\\tcacheReadTokens: usage?.prompt_tokens_details?.cached_tokens,\\n 37 | \\t\\t}\\n----\\n\\n# src/api/providers/openai.ts\\n162 | \\t\\t\\tif (this.options.includeMaxTokens) {\\n163 | \\t\\t\\t\\trequestOptions.max_tokens = modelInfo.maxTokens\\n164 | \\t\\t\\t}\\n----\\n241 | \\t\\t\\ttype: \\\"usage\\\",\\n242 | \\t\\t\\tinputTokens: usage?.prompt_tokens || 0,\\n243 | \\t\\t\\toutputTokens: usage?.completion_tokens || 0,\\n244 | \\t\\t\\tcacheWriteTokens: usage?.cache_creation_input_tokens || undefined,\\n245 | \\t\\t\\tcacheReadTokens: usage?.cache_read_input_tokens || undefined,\\n246 | \\t\\t}\\n----\\n347 | \\t\\t\\t\\t\\ttype: \\\"usage\\\",\\n348 | \\t\\t\\t\\t\\tinputTokens: chunk.usage.prompt_tokens || 0,\\n349 | \\t\\t\\t\\t\\toutputTokens: chunk.usage.completion_tokens || 0,\\n350 | \\t\\t\\t\\t}\\n----\\n\\n# src/api/providers/xai.ts\\n 52 | \\t\\t\\tmodel: modelId,\\n 53 | \\t\\t\\tmax_tokens: modelInfo.maxTokens,\\n 54 | \\t\\t\\ttemperature: this.options.modelTemperature ?? XAI_DEFAULT_TEMPERATURE,\\n----\\n 78 | \\t\\t\\tif (chunk.usage) {\\n 79 | \\t\\t\\t\\t// Extract detailed token information if available\\n 80 | \\t\\t\\t\\t// First check for prompt_tokens_details structure (real API response)\\n 81 | \\t\\t\\t\\tconst promptDetails = \\\"prompt_tokens_details\\\" in chunk.usage ? chunk.usage.prompt_tokens_details : null;\\n 82 | \\t\\t\\t\\tconst cachedTokens = promptDetails && \\\"cached_tokens\\\" in promptDetails ? promptDetails.cached_tokens : 0;\\n 83 | \\n 84 | \\t\\t\\t\\t// Fall back to direct fields in usage (used in test mocks)\\n 85 | \\t\\t\\t\\tconst readTokens = cachedTokens || (\\\"cache_read_input_tokens\\\" in chunk.usage ? (chunk.usage as any).cache_read_input_tokens : 0);\\n 86 | \\t\\t\\t\\tconst writeTokens = \\\"cache_creation_input_tokens\\\" in chunk.usage ? (chunk.usage as any).cache_creation_input_tokens : 0;\\n 87 | \\n----\\n 89 | \\t\\t\\t\\t\\ttype: \\\"usage\\\",\\n 90 | \\t\\t\\t\\t\\tinputTokens: chunk.usage.prompt_tokens || 0,\\n 91 | \\t\\t\\t\\t\\toutputTokens: chunk.usage.completion_tokens || 0,\\n 92 | \\t\\t\\t\\t\\tcacheReadTokens: readTokens,\\n----\\n\\nLoading...\"}"
}
[2025-06-13T05:00:03.473Z] {
  "ts": 1749790803442,
  "type": "ask",
  "ask": "resume_task"
}
[2025-06-13T05:00:14.579Z] {
  "ts": 1749790814550,
  "type": "say",
  "say": "text",
  "text": "Fix the following GitHub issue:\n\nRepository: RooCodeInc/Roo-Code\nIssue #123: Fix authentication bug in login flow\n\nDescription:\nUsers are experiencing issues logging in with OAuth providers. The token refresh mechanism seems to be failing intermittently, causing users to be logged out unexpectedly.\n\nSteps to reproduce:\n1. Login with GitHub OAuth\n2. Wait for token to expire\n3. Try to make an authenticated request\n4. User gets logged out instead of token being refreshed\n\nLabels: bug, authentication, oauth\n\nPlease analyze the issue, understand what needs to be fixed, and implement a solution."
}
[2025-06-13T05:00:14.629Z] taskStarted -> 895eb50e-d7d0-4aad-9b96-f37f457e15ad
[2025-06-13T05:00:14.659Z] {
  "ts": 1749790814630,
  "type": "say",
  "say": "api_req_started",
  "text": "{\"request\":\"<task>\\nFix the following GitHub issue:\\n\\nRepository: RooCodeInc/Roo-Code\\nIssue #123: Fix authentication bug in login flow\\n\\nDescription:\\nUsers are experiencing issues logging in with OAuth providers. The token refresh mechanism seems to be failing intermittently, causing users to be logged out unexpectedly.\\n\\nSteps to reproduce:\\n1. Login with GitHub OAuth\\n2. Wait for token to expire\\n3. Try to make an authenticated request\\n4. User gets logged out instead of token being refreshed\\n\\nLabels: bug, authentication, oauth\\n\\nPlease analyze the issue, understand what needs to be fixed, and implement a solution.\\n</task>\\n\\nLoading...\"}"
}
