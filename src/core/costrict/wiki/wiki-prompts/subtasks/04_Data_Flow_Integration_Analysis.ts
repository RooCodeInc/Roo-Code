import { WIKI_OUTPUT_DIR, SUBTASK_OUTPUT_FILENAMES } from "./constants"

export const DATA_FLOW_INTEGRATION_ANALYSIS_TEMPLATE = (workspace: string) => `# 数据流和集成深度分析

## 使用场景
从代码仓库中分析数据在系统中的流动路径和集成方式，生成详细的数据流文档，包括数据流转、集成模式、数据一致性等。

## 输入要求
- **完整代码仓库**: 项目的完整源代码
- **数据模型**: 数据库模型和数据结构定义
- **API接口**: 数据传输接口定义
- **消息配置**: 消息队列和数据流配置

# 数据流和集成深度分析任务

## 任务描述
请深度分析项目中的数据流动和集成方式，从数据流转路径、集成模式、数据一致性、数据安全等维度生成完整的数据流技术文档。

## 分析维度

### 1. 数据流路径分析
#### 数据流转模式
- **同步数据流**: 实时数据传输和处理
- **异步数据流**: 基于消息队列的数据流
- **批量数据流**: 批量数据处理和传输
- **实时数据流**: 流式数据处理和分析

### 2. 数据集成模式分析
#### API集成模式
- **REST API**: HTTP/JSON数据交换
- **gRPC**: 高性能RPC数据传输
- **WebSocket**: 实时双向数据传输

#### 消息集成模式
- **发布订阅**: 异步消息传递
- **点对点**: 直接消息传输
- **事件驱动**: 基于事件的数据流

#### 数据库集成模式
- **主从复制**: 主数据库到从数据库的数据同步
- **读写分离**: 读操作和写操作的数据分流
- **分库分表**: 数据水平拆分和集成
- **多数据源**: 多个数据源的数据集成

### 3. 数据格式和协议分析
#### 数据格式
- **JSON**: REST API数据交换格式
- **Protobuf**: gRPC服务数据格式
- **Avro**: 消息队列数据格式

#### 传输协议
- **HTTP/HTTPS**: Web服务数据传输
- **gRPC**: 高性能RPC数据传输
- **WebSocket**: 实时双向数据传输

### 4. 数据一致性分析
#### 事务一致性
- **ACID特性**: 原子性、一致性、隔离性、持久性
- **分布式事务**: 跨服务事务处理

#### 最终一致性
- **消息队列**: 异步消息的最终一致性
- **事件溯源**: 基于事件的数据一致性
- **补偿事务**: 失败操作的补偿机制

### 5. 数据安全分析
#### 数据加密
- **传输加密**: SSL/TLS数据传输加密
- **存储加密**: 数据库字段加密
- **密钥管理**: 加密密钥的生命周期管理

#### 数据脱敏
- **敏感数据**: 个人信息、财务数据脱敏
- **日志数据**: 日志中的敏感信息过滤
- **监控数据**: 监控指标中的敏感信息处理

### 6. 数据监控和分析
#### 数据流监控
- **流量监控**: 数据传输流量和频率监控
- **延迟监控**: 数据传输延迟和性能监控
- **错误监控**: 数据传输错误和异常监控

#### 数据分析
- **实时分析**: 流式数据的实时分析
- **批量分析**: 历史数据的批量分析
- **预测分析**: 基于历史数据的预测分析

## 输出格式要求

生成完整的数据流和集成分析文档：

### 文档结构
\`\`\`\`markdown
# {项目名称} 数据流和集成分析

## 数据流概览

### 数据流分类
| 数据流类型 | 描述 | 涉及服务 | 传输方式 |
|-----------|------|----------|----------|
| 业务数据流 | 核心业务数据流转 | Management, IDM | HTTP/REST |
| 事件数据流 | 系统事件和通知 | 所有服务 | Message Queue |
| 日志数据流 | 系统日志和监控 | 所有服务 | File/Stream |
| 配置数据流 | 配置信息同步 | 所有服务 | Config Center |

### 数据流架构图
\`\`\`mermaid
graph TB
    subgraph "数据源"
        A[用户输入]
        B[外部系统]
        C[IoT设备]
    end
    
    subgraph "接入层"
        D[API网关]
        E[WebSocket网关]
        F[消息网关]
    end
    
    subgraph "处理层"
        G[Management服务]
        H[Collector服务]
        I[IDM服务]
    end
    
    subgraph "存储层"
        J[(PostgreSQL)]
        K[(Redis)]
        L[(Pulsar)]
        M[(Elasticsearch)]
    end
    
    A --> D
    B --> D
    C --> F
    D --> G
    D --> H
    D --> I
    G --> J
    G --> K
    H --> L
    I --> J
\`\`\`

## 业务数据流分析

### 核心数据流
#### 数据流转路径
\`\`\`mermaid
sequenceDiagram
    participant U as 用户
    participant API as API网关
    participant S as 业务服务
    participant DB as 数据库
    participant MQ as 消息队列
    
    U->>API: 请求数据
    API->>S: 转发请求
    S->>DB: 查询/存储数据
    DB->>S: 返回结果
    S->>MQ: 发送事件
    S->>API: 返回响应
    API->>U: 返回结果
\`\`\`

#### 数据格式定义
**请求数据格式**:
\`\`\`json
{
  "id": "string",
  "type": "string",
  "data": "object",
  "timestamp": "timestamp"
}
\`\`\`

**响应数据格式**:
\`\`\`json
{
  "status": "string",
  "data": "object",
  "message": "string",
  "timestamp": "timestamp"
}
\`\`\`

## 集成模式分析

### API集成模式
| API端点 | HTTP方法 | 数据格式 | 用途 | 集成服务 |
|---------|----------|----------|------|----------|
| /api/v1/users | POST/GET | JSON | 用户管理 | Management, IDM |
| /api/v1/data | POST | JSON | 数据提交 | Management, Collector |
| /api/v1/auth | POST | JSON | 身份验证 | Management, IDM |

### 消息集成模式
| 主题(Topic) | 发布者 | 订阅者 | 消息格式 | 用途 |
|------------|--------|--------|----------|------|
| user-events | Management, IDM | 所有服务 | JSON | 用户事件通知 |
| data-events | Collector | 数据分析服务 | JSON | 数据事件处理 |
| system-events | 所有服务 | 监控服务 | JSON | 系统事件监控 |

### 数据库集成模式
| 数据库 | 角色 | 复制方式 | 延迟 | 用途 |
|--------|------|----------|------|------|
| PostgreSQL主库 | 主库 | 同步复制 | <1s | 写操作 |
| PostgreSQL从库 | 从库 | 异步复制 | <5s | 读操作 |
| Redis主库 | 主库 | 同步复制 | <1s | 缓存写入 |
| Redis从库 | 从库 | 异步复制 | <3s | 缓存读取 |

## 数据一致性分析

### 事务一致性
#### ACID特性保证
- **原子性**: 事务中的操作要么全部成功，要么全部失败
- **一致性**: 事务执行前后数据保持一致状态
- **隔离性**: 并发事务之间相互隔离
- **持久性**: 事务提交后数据持久化存储

### 最终一致性
#### 事件溯源一致性
\`\`\`mermaid
graph TB
    A[业务操作] --> B[生成事件]
    B --> C[存储事件]
    C --> D[发布事件]
    D --> E[事件处理]
    E --> F[更新状态]
\`\`\`

#### 补偿事务机制
- **正向操作**: 正常的业务操作流程
- **补偿操作**: 失败时的补偿和回滚操作
- **重试策略**: 失败操作的重试机制
- **超时处理**: 操作超时的处理机制

## 数据安全分析

### 数据加密
| 加密方式 | 加密算法 | 密钥长度 | 应用场景 |
|----------|----------|----------|----------|
| HTTPS | TLS 1.3 | 256位 | API通信 |
| 存储加密 | AES-256 | 256位 | 敏感数据 |
| 密码哈希 | bcrypt | - | 用户密码 |

### 数据脱敏
| 数据类型 | 脱敏方式 | 脱敏规则 | 应用场景 |
|----------|----------|----------|----------|
| 手机号 | 部分遮蔽 | 138****1234 | 日志、显示 |
| 邮箱 | 部分遮蔽 | user***@domain.com | 日志、显示 |
| 身份证 | 部分遮蔽 | 110****1234 | 日志、显示 |

## 数据监控和分析

### 数据流监控
| 指标类型 | 指标名称 | 阈值 | 告警级别 |
|----------|----------|------|----------|
| 流量监控 | API请求量 | >10000 QPS | 警告 |
| 延迟监控 | API响应时间 | >1000ms | 警告 |
| 错误监控 | API错误率 | >5% | 警告 |
| 消息监控 | 消息处理量 | >5000 msg/s | 警告 |

### 数据分析
- **实时分析**: 流式数据的实时分析和处理
- **批量分析**: 历史数据的批量分析和统计
- **预测分析**: 基于历史数据的预测和建模
- **可视化分析**: 数据可视化展示和分析


\`\`\`\`

## 特别注意事项
1. 必须基于实际的代码和配置进行分析，不能虚构数据流
2. 重点分析关键数据路径和集成模式
3. 关注数据一致性和安全性保证

## 输出文件命名
\`${workspace}${WIKI_OUTPUT_DIR}${SUBTASK_OUTPUT_FILENAMES.DATA_FLOW_INTEGRATION_TASK_FILE}\`
注意：如果${workspace}${WIKI_OUTPUT_DIR}目录不存在，则创建。

## 示例输出特征
基于项目的数据流分析特征：
- 详细的数据流转路径和时序图
- 完整的集成模式和应用场景分析
- 全面的数据一致性保证机制
- 实用的数据安全和加密策略
- 具体的性能优化和故障处理方案
`
